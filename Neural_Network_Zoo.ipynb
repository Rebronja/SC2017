{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kreacija dataseta\n",
    "\n",
    "%run ./make_images.py\n",
    "\n",
    "%run ./prep_dataset.py\n",
    "\n",
    "dset = dataset.HiraSet('dset', 22500)\n",
    "dset.pull()\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new(25,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_data = train_data.reshape(train_data.shape[0], 1, 150, 150).astype('float32')\n",
    "test_data = test_data.reshape(test_data.shape[0], 1, 150, 150).astype('float32')\n",
    "\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, border_mode='valid', input_shape=(1, 150, 150), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(74, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(train_data, train_labels, nb_epoch=300, batch_size=50, verbose=0)\n",
    "\n",
    "scores = model.evaluate(test_data, test_labels, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Accuracy : 98.74%\n",
    "# Time of Execution : ~25m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbours\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nsamples, nx, ny, nz = train_data.shape\n",
    "d2_train_data = train_data.reshape((nsamples,nx*ny*nz))\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 1)\n",
    "model.fit(d2_train_data, train_labels)\n",
    "\n",
    "#printing out the score for test sample\n",
    "\n",
    "nsamples, nx, ny, nz = test_data.shape\n",
    "d2_test_data = test_data.reshape((nsamples,nx*ny*nz))\n",
    "prediction = model.predict(d2_test_data)\n",
    "acc_knn = accuracy_score(test_labels, prediction)\n",
    "print('Nearest neighbours accuracy: ',acc_knn)\n",
    "\n",
    "# Neighbour count = 1 Accuracy = 49.26%\n",
    "# Neighbour count = 2 Accuracy = 23.65%\n",
    "# Neighbour count = 3 Accuracy = 31.96%\n",
    "# Neighbour count = 4 Accuracy = 17.97%\n",
    "# Neighbour count = 5 Accuracy = 21.08%\n",
    "# Neighbour count = 6 Accuracy = 11.82%\n",
    "# Time of Execution : ~93s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short Term Memory Recurrent Neural Network\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_data = train_data.reshape(train.shape[0], 1, 150, 150).astype('float32')\n",
    "test_data = test_data.reshape(test.shape[0], 1, 150, 150).astype('float32')\n",
    "\n",
    "nsamples, nx, ny, nz = train_data.shape\n",
    "d3_train_data = train_data.reshape((nsamples,nx,ny*nz))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(2, batch_input_shape=(10, 1, 22500), input_dim=22500, stateful=True, return_sequences=True))\n",
    "model.add(LSTM(2, batch_input_shape=(10, 1, 22500), input_dim=22500, stateful=True))\n",
    "model.add(Dense(74))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(d3_train_data, train_labels, nb_epoch=300, batch_size=10, verbose=0)\n",
    "\n",
    "# Time of Execution : ~45m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "nsamples, nx, ny, nz = test_data.shape\n",
    "d3_test_data = test_data.reshape((nsamples,nx,ny*nz))\n",
    "\n",
    "testPredict = model.predict(d3_test_data, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Score LSTM\n",
    "\n",
    "scores = model.evaluate(d3_test, test_labels, verbose=0, batch_size=10)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores*100))\n",
    "\n",
    "# Accuracy : 1.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new(25,20)\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_data, train_labels)\n",
    "prediction = model.predict(test_data)\n",
    "acc_rf = accuracy_score(test_labels, prediction)\n",
    "print (\"Random forest accuracy: \",acc_rf)\n",
    "\n",
    "# Accuracy : 9.26%\n",
    "# Time of Execution : ~15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new_RNN(25,20)\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "\n",
    "model = SGDClassifier()\n",
    "model.fit(train_data, train_labels)\n",
    "prediction = model.predict(test_data)\n",
    "acc_sgd = accuracy_score(test_labels, prediction)\n",
    "print (\"Stochastic gradient descent accuracy: \",acc_sgd)\n",
    "\n",
    "# Accuracy : 8.45%\n",
    "# Time of Execution : ~30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Machine\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new_RNN(25,20)\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(train_data, train_labels)\n",
    "prediction = model.predict(test_data)\n",
    "acc_svm = accuracy_score(test_labels, prediction)\n",
    "print (\"Linear SVM accuracy: \",acc_svm)\n",
    "\n",
    "# Accuracy : 35.47%\n",
    "# Time of Execution : ~23m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Attempt Number 1\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# prepare model\n",
    "model = Sequential()\n",
    "model.add(Dense(70, input_dim=22500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(140))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(74))\n",
    "model.add(Activation('relu'))\n",
    "# compile model with optimizer\n",
    "sgd = SGD(lr=0.1, decay=0.001, momentum=0.7)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "# training\n",
    "train_data=np.array(train_data)\n",
    "train_labels=np.array(train_labels)\n",
    "nsamples, nx, ny, nz = train_data.shape\n",
    "d2_train_data = train_data.reshape((nsamples,nx*ny*nz))\n",
    "training = model.fit(d2_train_data, train_labels, nb_epoch=300, batch_size=222, verbose=0)\n",
    "\n",
    "scores = model.evaluate(d2_train, train_labels, verbose=0)\n",
    "\n",
    "# Accuracy : 1.93%\n",
    "# Time of Execution : ~15s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Layer Long Short Term Memory through usage of lasagne\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import sklearn.metrics\n",
    "import lasagne.layers as L\n",
    "\n",
    "from lasagne.layers import InputLayer, LSTMLayer, ReshapeLayer, DenseLayer, GaussianNoiseLayer\n",
    "\n",
    "\n",
    "# Number of Units in hidden layers\n",
    "L1_UNITS = 50\n",
    "L2_UNITS = 100\n",
    "\n",
    "# Training Params \n",
    "LEARNING_RATE = 0.001\n",
    "N_BATCH = 10\n",
    "NUM_EPOCHS = 1500\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "(train,test,train_labels,test_labels) = dset.require_new_RNN(8,2)\n",
    "\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 150, 150).astype('float32')\n",
    "\n",
    "num_feat    = train.shape[1]\n",
    "num_classes = np.unique(test).size\n",
    "\n",
    "# Generate sequence masks (redundant here)\n",
    "mask_train = np.ones((train.shape[0], train.shape[1]))\n",
    "mask_test  = np.ones((test.shape[0], test.shape[1]))\n",
    "\n",
    "\n",
    "# Model\n",
    "tanh = lasagne.nonlinearities.tanh\n",
    "relu = lasagne.nonlinearities.rectify\n",
    "soft = lasagne.nonlinearities.softmax\n",
    "\n",
    "# Network Architecture\n",
    "l_in = InputLayer(shape=(None, None, num_feat))\n",
    "batchsize, seqlen, _ = l_in.input_var.shape\n",
    "\n",
    "l_noise = GaussianNoiseLayer(l_in, sigma=0.6) \n",
    "l_mask  = InputLayer(shape=(batchsize, seqlen))\n",
    "\n",
    "l_rnn_1 = LSTMLayer(l_noise, num_units=L1_UNITS, mask_input=l_mask)\n",
    "l_in_drop = lasagne.layers.DropoutLayer(l_rnn_1, p=0.25)\n",
    "l_rnn_2 = LSTMLayer(l_in_drop, num_units=L2_UNITS)\n",
    "l_in_drop2 = lasagne.layers.DropoutLayer(l_rnn_2, p=0.1)\n",
    "l_shp   = ReshapeLayer(l_in_drop2,(-1, L2_UNITS))\n",
    "l_dense = DenseLayer(l_shp, num_units=num_classes, nonlinearity=soft)\n",
    "l_out   = ReshapeLayer(l_dense, (batchsize, seqlen, num_classes)) \n",
    "\n",
    "\n",
    "# Symbols and Cost Function\n",
    "target_values = T.ivector('target_output')\n",
    "\n",
    "network_output   = L.get_output(l_out)\n",
    "predicted_values = network_output[:, -1]\n",
    "prediction = T.argmax(predicted_values, axis=1)\n",
    "all_params = L.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = lasagne.objectives.categorical_crossentropy(predicted_values, target_values)\n",
    "cost = cost.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Compute SGD updates for training\n",
    "print(\"Computing updates ...\")\n",
    "updates = lasagne.updates.rmsprop(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "# Theano functions for training and computing cost\n",
    "print(\"Compiling functions ...\")\n",
    "training   = theano.function(\n",
    "    [l_in.input_var, target_values, l_mask.input_var], cost, updates=updates,allow_input_downcast=True)\n",
    "predict = theano.function([l_in.input_var, l_mask.input_var], prediction,allow_input_downcast=True)\n",
    "compute_cost = theano.function([l_in.input_var, target_values, l_mask.input_var], cost,allow_input_downcast=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "print(\"Training ...\")\n",
    "num_batches_train = int(np.ceil(len(train) / N_BATCH))\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    now = time.time\n",
    "    losses = []        \n",
    "\n",
    "    batch_shuffle = np.random.choice(train.shape[0], train.shape[0], False)\n",
    "    sequences   = train[batch_shuffle]\n",
    "    labels      = train_labels[batch_shuffle]\n",
    "    train_masks = mask_train[batch_shuffle]\n",
    "\n",
    "    for batch in range(num_batches_train):\n",
    "        batch_slice = slice(N_BATCH * batch,\n",
    "                            N_BATCH * (batch + 1))\n",
    "        \n",
    "        X_batch = sequences[batch_slice]\n",
    "        y_batch = labels[batch_slice]\n",
    "        m_batch = train_masks[batch_slice]\n",
    "\n",
    "        loss = training(X_batch, y_batch, m_batch)\n",
    "        losses.append(loss)\n",
    "                \n",
    "    train_loss = np.mean(losses)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss = compute_cost(test, test_labels, mask_test)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    test_pred   = predict(test, mask_test)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, test_pred)\n",
    "\n",
    "\n",
    "    print('Current epoch:', epoch+1,'|', 'Number of Epochs:', NUM_EPOCHS,'|','Train loss:', train_loss,'|','Validation loss:', valid_loss,'|','Accuracy:', accuracy)y)\n",
    "    \n",
    "    \n",
    "# Accuracy : peaks at ~70%, starts at 1%\n",
    "# Time of Execution : ~5h\n",
    "# Time per Epoch : ~21s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN attempt number 2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "folder='images50x50'\n",
    "\n",
    "rel_path = 'images50x50'\n",
    "size = 2500 # 50x50 slicica\n",
    "\n",
    "folders = os.listdir(rel_path)\n",
    "\n",
    "kana_dict = dataset.characters()\n",
    "hiragana_dataset = dataset.HiraSet('dset50x50', size)\n",
    "\n",
    "for folder in folders:\n",
    "    entry = dataset.HiraEntry(folder, kana_dict[folder])\n",
    "\n",
    "    files = os.listdir(rel_path + '/' + folder)\n",
    "    for file in files:\n",
    "        img = imread(rel_path + '/' + folder + '/' + file)\n",
    "        re_img = np.reshape(img, size)\n",
    "        flt_img = re_img / 65535.0\n",
    "        print(flt_img)\n",
    "\n",
    "        entry.add(flt_img)\n",
    "\n",
    "    hiragana_dataset.add(entry)\n",
    "dset50x50 = dataset.HiraSet('dset50x50', 2500)    \n",
    "dset50x50.pull()\n",
    "\n",
    "\n",
    "(train,test,train_labels,test_labels) = dset50x50.require_new(25,20)\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2500, input_dim=2500, init='normal', activation='relu'))\n",
    "model.add(Dense(74, init='normal', activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# training\n",
    "training = model.fit(train, train_labels, nb_epoch=300, batch_size=100, verbose=0)\n",
    "scores = model.evaluate(test, test_labels, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "# Accuracy : ~52.16%\n",
    "# Time of Execution : ~6m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 neural network with pretrained weights. Not Tested as it works with coloured images.\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
    "model_vgg16_conv.summary()\n",
    "\n",
    "#Create your own input format (here 3x200x200)\n",
    "input = Input(shape=(3,150,150),name = 'image_input')\n",
    "\n",
    "#Use the generated model\n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "#Add the fully-connected layers\n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(74, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model\n",
    "my_model = Model(input=input, output=x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "#Then training with your data !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}