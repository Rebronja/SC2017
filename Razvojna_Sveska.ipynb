{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load1_1.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page1_1.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [850, 1000, 100, 250], 10), 'a', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 320, 470], 10), 'i', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 530, 680], 10), 'u', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 750, 900], 10), 'e', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 980, 1130], 10), 'o', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1190, 1340], 10), 'ka', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1400, 1550], 10), 'ki', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1630, 1780], 10), 'ku', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1840, 1990], 10), 'ke', 1)\n",
    "dataset.save(dataset.crop(img, [840, 990, 2060, 2210], 10), 'ko', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 100, 250], 10), 'sa', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 330, 480], 10), 'shi', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 540, 690], 10), 'su', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 760, 910], 10), 'se', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 970, 1120], 10), 'so', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1190, 1340], 10), 'ta', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1420, 1570], 10), 'chi', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1630, 1780], 10), 'tsu', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1850, 2000], 10), 'te', 1)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 2040, 2190], 10), 'to', 1)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 100, 250], 10), 'na', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 320, 470], 10), 'ni', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 540, 690], 10), 'nu', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 760, 910], 10), 'ne', 1)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 960, 1110], 10), 'no', 1)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1200, 1350], 10), 'ha', 1)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1410, 1560], 10), 'hi', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1630, 1780], 10), 'fu', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1850, 2000], 10), 'he', 1)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 2070, 2220], 10), 'ho', 1)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 110, 260], 10), 'ma', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 330, 480], 10), 'mi', 1)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 550, 700], 10), 'mu', 1)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 750, 900], 10), 'me', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 970, 1120], 10), 'mo', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1190, 1340], 10), 'ra', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1390, 1540], 10), 'ri', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1630, 1780], 10), 'ru', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1850, 2000], 10), 're', 1)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 2060, 2210], 10), 'ro', 1)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 120, 270], 10), 'ya', 1)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 330, 480], 10), 'yu', 1)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 550, 700], 10), 'yo', 1)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 770, 920], 10), 'wa', 1)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 980, 1130], 10), 'wo', 1)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1190, 1340], 10), 'n', 1)\n",
    "dataset.save(dataset.crop(img, [1630, 1780, 1400, 1550], 10), 'lowerCaseYa', 1)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1630, 1780], 10), 'lowerCaseYu', 1)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1840, 1990], 10), 'lowerCaseYo', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 120, 270], 10), 'ga', 1)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 340, 490], 10), 'gi', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 560, 710], 10), 'gu', 1)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 770, 920], 10), 'ge', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 990, 1140], 10), 'go', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1200, 1350], 10), 'za', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1420, 1570], 10), 'ji', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1620, 1770], 10), 'zu', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1840, 1990], 10), 'ze', 1)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 2060, 2210], 10), 'zo', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 110, 260], 10), 'da', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 340, 490], 10), 'di', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 550, 700], 10), 'du', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 780, 930], 10), 'de', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 990, 1140], 10), 'do', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1200, 1350], 10), 'ba', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1410, 1560], 10), 'bi', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1630, 1780], 10), 'bu', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1840, 1990], 10), 'be', 1)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 2060, 2210], 10), 'bo', 1)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 120, 270], 10), 'pa', 1)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 340, 490], 10), 'pi', 1)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 550, 700], 10), 'pu', 1)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 770, 920], 10), 'pe', 1)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 980, 1130], 10), 'po', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load2_1.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page2_1.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [900, 1050, 150, 300], 10), 'a', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 360, 510], 10), 'i', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 580, 730], 10), 'u', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 790, 940], 10), 'e', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 1010, 1160], 10), 'o', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 1220, 1370], 10), 'ka', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 1450, 1600], 10), 'ki', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 1660, 1810], 10), 'ku', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 1880, 2030], 10), 'ke', 2)\n",
    "dataset.save(dataset.crop(img, [900, 1050, 2100, 2250], 10), 'ko', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 150, 300], 10), 'sa', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 360, 510], 10), 'shi', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 580, 730], 10), 'su', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 800, 950], 10), 'se', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 1015, 1165], 10), 'so', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 1220, 1370], 10), 'ta', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 1440, 1590], 10), 'chi', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 1660, 1810], 10), 'tsu', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 1880, 2030], 10), 'te', 2)\n",
    "dataset.save(dataset.crop(img, [1090, 1240, 2095, 2245], 10), 'to', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 150, 300], 10), 'na', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 360, 510], 10), 'ni', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 580, 730], 10), 'nu', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 795, 945], 10), 'ne', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 1010, 1160], 10), 'no', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 1225, 1375], 10), 'ha', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 1440, 1590], 10), 'hi', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 1655, 1805], 10), 'fu', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 1875, 2025], 10), 'he', 2)\n",
    "dataset.save(dataset.crop(img, [1275, 1425, 2095, 2245], 10), 'ho', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 150, 300], 10), 'ma', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 365, 515], 10), 'mi', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 580, 730], 10), 'mu', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 795, 945], 10), 'me', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 1015, 1165], 10), 'mo', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 1225, 1375], 10), 'ra', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 1445, 1595], 10), 'ri', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 1660, 1810], 10), 'ru', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 1870, 2020], 10), 're', 2)\n",
    "dataset.save(dataset.crop(img, [1465, 1615, 2090, 2240], 10), 'ro', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 150, 300], 10), 'ya', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 365, 515], 10), 'yu', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 585, 735], 10), 'yo', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 795, 945], 10), 'wa', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 1015, 1165], 10), 'wo', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 1225, 1375], 10), 'n', 2)\n",
    "dataset.save(dataset.crop(img, [1650, 1800, 1445, 1595], 10), 'lowerCaseYa', 2)\n",
    "dataset.save(dataset.crop(img, [1660, 1810, 1650, 1800], 10), 'lowerCaseYu', 2)\n",
    "dataset.save(dataset.crop(img, [1660, 1810, 1870, 2020], 10), 'lowerCaseYo', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 150, 300], 10), 'ga', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 375, 525], 10), 'gi', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 595, 745], 10), 'gu', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 800, 950], 10), 'ge', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 1015, 1165], 10), 'go', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 1230, 1380], 10), 'za', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 1445, 1595], 10), 'ji', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 1655, 1805], 10), 'zu', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 1870, 2020], 10), 'ze', 2)\n",
    "dataset.save(dataset.crop(img, [2020, 2170, 2090, 2240], 10), 'zo', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 150, 300], 10), 'da', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 375, 525], 10), 'di', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 590, 740], 10), 'du', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 805, 955], 10), 'de', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 1015, 1165], 10), 'do', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 1230, 1380], 10), 'ba', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 1445, 1595], 10), 'bi', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 1655, 1805], 10), 'bu', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 1870, 2020], 10), 'be', 2)\n",
    "dataset.save(dataset.crop(img, [2210, 2360, 2085, 2235], 10), 'bo', 2)\n",
    "dataset.save(dataset.crop(img, [2395, 2545, 155, 305], 10), 'pa', 2)\n",
    "dataset.save(dataset.crop(img, [2395, 2545, 370, 520], 10), 'pi', 2)\n",
    "dataset.save(dataset.crop(img, [2395, 2545, 585, 735], 10), 'pu', 2)\n",
    "dataset.save(dataset.crop(img, [2395, 2545, 800, 950], 10), 'pe', 2)\n",
    "dataset.save(dataset.crop(img, [2395, 2545, 1015, 1165], 10), 'po', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load3_2.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page3_2.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [780, 930, 100, 250], 10), 'a', 3)\n",
    "dataset.save(dataset.crop(img, [780, 930, 330, 480], 10), 'i', 3)\n",
    "dataset.save(dataset.crop(img, [770, 920, 540, 690], 10), 'u', 3)\n",
    "dataset.save(dataset.crop(img, [770, 920, 770, 920], 10), 'e', 3)\n",
    "dataset.save(dataset.crop(img, [760, 910, 980, 1130], 10), 'o', 3)\n",
    "dataset.save(dataset.crop(img, [760, 910, 1200, 1350], 10), 'ka', 3)\n",
    "dataset.save(dataset.crop(img, [750, 900, 1420, 1570], 10), 'ki', 3)\n",
    "dataset.save(dataset.crop(img, [740, 890, 1640, 1790], 10), 'ku', 3)\n",
    "dataset.save(dataset.crop(img, [730, 880, 1860, 2010], 10), 'ke', 3)\n",
    "dataset.save(dataset.crop(img, [740, 890, 2080, 2230], 10), 'ko', 3)\n",
    "dataset.save(dataset.crop(img, [1050, 1200, 110, 260], 10), 'sa', 3)\n",
    "dataset.save(dataset.crop(img, [1050, 1200, 340, 490], 10), 'shi', 3)\n",
    "dataset.save(dataset.crop(img, [1050, 1200, 550, 700], 10), 'su', 3)\n",
    "dataset.save(dataset.crop(img, [1050, 1200, 770, 920], 10), 'se', 3)\n",
    "dataset.save(dataset.crop(img, [1050, 1200, 980, 1130], 10), 'so', 3)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1200, 1350], 10), 'ta', 3)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1430, 1580], 10), 'chi', 3)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1630, 1780], 10), 'tsu', 3)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1860, 2010], 10), 'te', 3)\n",
    "dataset.save(dataset.crop(img, [1010, 1160, 2080, 2230], 10), 'to', 3)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 130, 280], 10), 'na', 3)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 350, 500], 10), 'ni', 3)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 560, 710], 10), 'nu', 3)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 780, 930], 10), 'ne', 3)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 990, 1140], 10), 'no', 3)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1210, 1360], 10), 'ha', 3)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1420, 1570], 10), 'hi', 3)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1640, 1790], 10), 'fu', 3)\n",
    "dataset.save(dataset.crop(img, [1280, 1430, 1860, 2010], 10), 'he', 3)\n",
    "dataset.save(dataset.crop(img, [1270, 1420, 2080, 2230], 10), 'ho', 3)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 140, 290], 10), 'ma', 3)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 350, 500], 10), 'mi', 3)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 570, 720], 10), 'mu', 3)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 780, 930], 10), 'me', 3)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 1000, 1150], 10), 'mo', 3)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1210, 1360], 10), 'ra', 3)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1440, 1590], 10), 'ri', 3)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1640, 1790], 10), 'ru', 3)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1860, 2010], 10), 're', 3)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 2070, 2220], 10), 'ro', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 140, 290], 10), 'ya', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 350, 500], 10), 'yu', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 580, 730], 10), 'yo', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 780, 930], 10), 'wa', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 1000, 1150], 10), 'wo', 3)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1220, 1370], 10), 'n', 3)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 1430, 1580], 10), 'lowerCaseYa', 3)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1650, 1800], 10), 'lowerCaseYu', 3)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1870, 2020], 10), 'lowerCaseYo', 3)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 150, 300], 10), 'ga', 3)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 370, 520], 10), 'gi', 3)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 590, 740], 10), 'gu', 3)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 800, 950], 10), 'ge', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1020, 1170], 10), 'go', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1230, 1380], 10), 'za', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1450, 1600], 10), 'ji', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1650, 1800], 10), 'zu', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1860, 2010], 10), 'ze', 3)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 2090, 2240], 10), 'zo', 3)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 160, 310], 10), 'da', 3)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 370, 520], 10), 'di', 3)\n",
    "dataset.save(dataset.crop(img, [2610, 2760, 590, 740], 10), 'du', 3)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 810, 960], 10), 'de', 3)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1020, 1170], 10), 'do', 3)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1230, 1380], 10), 'ba', 3)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1440, 1590], 10), 'bi', 3)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1650, 1800], 10), 'bu', 3)\n",
    "dataset.save(dataset.crop(img, [2590, 2740, 1870, 2020], 10), 'be', 3)\n",
    "dataset.save(dataset.crop(img, [2590, 2740, 2090, 2240], 10), 'bo', 3)\n",
    "dataset.save(dataset.crop(img, [2890, 3040, 170, 320], 10), 'pa', 3)\n",
    "dataset.save(dataset.crop(img, [2890, 3040, 380, 530], 10), 'pi', 3)\n",
    "dataset.save(dataset.crop(img, [2880, 3030, 590, 740], 10), 'pu', 3)\n",
    "dataset.save(dataset.crop(img, [2870, 3020, 810, 960], 10), 'pe', 3)\n",
    "dataset.save(dataset.crop(img, [2870, 3020, 1030, 1180], 10), 'po', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load1_2.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page1_2.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [850, 1000, 110, 260], 10), 'a', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 330, 480], 10), 'i', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 540, 690], 10), 'u', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 760, 910], 10), 'e', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 990, 1140], 10), 'o', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1200, 1350], 10), 'ka', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1410, 1560], 10), 'ki', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1640, 1790], 10), 'ku', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1850, 2000], 10), 'ke', 4)\n",
    "dataset.save(dataset.crop(img, [840, 990, 2070, 2220], 10), 'ko', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 110, 260], 10), 'sa', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 340, 490], 10), 'shi', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 550, 700], 10), 'su', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 770, 920], 10), 'se', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 980, 1130], 10), 'so', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1200, 1350], 10), 'ta', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1430, 1580], 10), 'chi', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1640, 1790], 10), 'tsu', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1860, 2010], 10), 'te', 4)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 2060, 2210], 10), 'to', 4)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 110, 260], 10), 'na', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 330, 480], 10), 'ni', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 560, 710], 10), 'nu', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 770, 920], 10), 'ne', 4)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 980, 1130], 10), 'no', 4)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1210, 1360], 10), 'ha', 4)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1420, 1570], 10), 'hi', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1640, 1790], 10), 'fu', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1860, 2010], 10), 'he', 4)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 2080, 2230], 10), 'ho', 4)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 120, 270], 10), 'ma', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 340, 490], 10), 'mi', 4)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 560, 710], 10), 'mu', 4)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 760, 910], 10), 'me', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 980, 1130], 10), 'mo', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1200, 1350], 10), 'ra', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1400, 1550], 10), 'ri', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1640, 1790], 10), 'ru', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1860, 2010], 10), 're', 4)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 2070, 2220], 10), 'ro', 4)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 130, 280], 10), 'ya', 4)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 340, 490], 10), 'yu', 4)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 560, 710], 10), 'yo', 4)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 780, 930], 10), 'wa', 4)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 990, 1140], 10), 'wo', 4)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1200, 1350], 10), 'n', 4)\n",
    "dataset.save(dataset.crop(img, [1630, 1780, 1410, 1560], 10), 'lowerCaseYa', 4)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1640, 1790], 10), 'lowerCaseYu', 4)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1850, 2000], 10), 'lowerCaseYo', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 130, 280], 10), 'ga', 4)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 350, 500], 10), 'gi', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 570, 720], 10), 'gu', 4)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 780, 930], 10), 'ge', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1000, 1150], 10), 'go', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1210, 1360], 10), 'za', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1430, 1580], 10), 'ji', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1630, 1780], 10), 'zu', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1850, 2000], 10), 'ze', 4)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 2060, 2210], 10), 'zo', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 120, 270], 10), 'da', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 350, 500], 10), 'di', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 560, 710], 10), 'du', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 790, 940], 10), 'de', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1000, 1150], 10), 'do', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1210, 1360], 10), 'ba', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1420, 1570], 10), 'bi', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1640, 1790], 10), 'bu', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1850, 2000], 10), 'be', 4)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 2070, 2220], 10), 'bo', 4)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 130, 280], 10), 'pa', 4)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 350, 500], 10), 'pi', 4)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 560, 710], 10), 'pu', 4)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 780, 930], 10), 'pe', 4)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 990, 1140], 10), 'po', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load2_2.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page2_2.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [850, 1000, 170, 320], 10), 'a', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 380, 530], 10), 'i', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 600, 750], 10), 'u', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 810, 960], 10), 'e', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 1030, 1180], 10), 'o', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 1240, 1390], 10), 'ka', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 1470, 1620], 10), 'ki', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 1680, 1830], 10), 'ku', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 1900, 2050], 10), 'ke', 5)\n",
    "dataset.save(dataset.crop(img, [850, 1000, 2120, 2270], 10), 'ko', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 170, 320], 10), 'sa', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 380, 530], 10), 'shi', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 600, 750], 10), 'su', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 820, 970], 10), 'se', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 1035, 1185], 10), 'so', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 1240, 1390], 10), 'ta', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 1460, 1610], 10), 'chi', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 1680, 1830], 10), 'tsu', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 1900, 2050], 10), 'te', 5)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 2115, 2265], 10), 'to', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 170, 320], 10), 'na', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 380, 530], 10), 'ni', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 600, 750], 10), 'nu', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 815, 965], 10), 'ne', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 1030, 1180], 10), 'no', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 1245, 1395], 10), 'ha', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 1460, 1610], 10), 'hi', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 1675, 1825], 10), 'fu', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 1895, 2045], 10), 'he', 5)\n",
    "dataset.save(dataset.crop(img, [1225, 1375, 2115, 2265], 10), 'ho', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 170, 320], 10), 'ma', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 385, 535], 10), 'mi', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 600, 750], 10), 'mu', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 815, 965], 10), 'me', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 1035, 1185], 10), 'mo', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 1245, 1395], 10), 'ra', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 1465, 1615], 10), 'ri', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 1680, 1830], 10), 'ru', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 1890, 2040], 10), 're', 5)\n",
    "dataset.save(dataset.crop(img, [1415, 1565, 2110, 2260], 10), 'ro', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 170, 320], 10), 'ya', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 385, 535], 10), 'yu', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 605, 755], 10), 'yo', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 815, 965], 10), 'wa', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1035, 1185], 10), 'wo', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1245, 1395], 10), 'n', 5)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1465, 1615], 10), 'lowerCaseYa', 5)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1670, 1820], 10), 'lowerCaseYu', 5)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1890, 2040], 10), 'lowerCaseYo', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 170, 320], 10), 'ga', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 395, 545], 10), 'gi', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 615, 765], 10), 'gu', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 820, 970], 10), 'ge', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1035, 1185], 10), 'go', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1250, 1400], 10), 'za', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1465, 1615], 10), 'ji', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1675, 1825], 10), 'zu', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1890, 2040], 10), 'ze', 5)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 2110, 2260], 10), 'zo', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 170, 320], 10), 'da', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 395, 545], 10), 'di', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 610, 760], 10), 'du', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 825, 975], 10), 'de', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1035, 1185], 10), 'do', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1250, 1400], 10), 'ba', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1465, 1615], 10), 'bi', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1675, 1825], 10), 'bu', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1890, 2040], 10), 'be', 5)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 2105, 2255], 10), 'bo', 5)\n",
    "dataset.save(dataset.crop(img, [2345, 2495, 175, 325], 10), 'pa', 5)\n",
    "dataset.save(dataset.crop(img, [2345, 2495, 390, 540], 10), 'pi', 5)\n",
    "dataset.save(dataset.crop(img, [2345, 2495, 605, 755], 10), 'pu', 5)\n",
    "dataset.save(dataset.crop(img, [2345, 2495, 820, 970], 10), 'pe', 5)\n",
    "dataset.save(dataset.crop(img, [2345, 2495, 1035, 1185], 10), 'po', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load3_1.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page3_1.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [810, 960, 80, 230], 10), 'a', 6)\n",
    "dataset.save(dataset.crop(img, [810, 960, 310, 460], 10), 'i', 6)\n",
    "dataset.save(dataset.crop(img, [800, 950, 520, 670], 10), 'u', 6)\n",
    "dataset.save(dataset.crop(img, [800, 950, 750, 900], 10), 'e', 6)\n",
    "dataset.save(dataset.crop(img, [790, 940, 960, 1110], 10), 'o', 6)\n",
    "dataset.save(dataset.crop(img, [790, 940, 1180, 1330], 10), 'ka', 6)\n",
    "dataset.save(dataset.crop(img, [780, 930, 1400, 1550], 10), 'ki', 6)\n",
    "dataset.save(dataset.crop(img, [770, 920, 1620, 1770], 10), 'ku', 6)\n",
    "dataset.save(dataset.crop(img, [760, 910, 1840, 1990], 10), 'ke', 6)\n",
    "dataset.save(dataset.crop(img, [770, 920, 2060, 2210], 10), 'ko', 6)\n",
    "dataset.save(dataset.crop(img, [1080, 1230, 90, 240], 10), 'sa', 6)\n",
    "dataset.save(dataset.crop(img, [1080, 1230, 320, 470], 10), 'shi', 6)\n",
    "dataset.save(dataset.crop(img, [1080, 1230, 530, 680], 10), 'su', 6)\n",
    "dataset.save(dataset.crop(img, [1080, 1230, 750, 900], 10), 'se', 6)\n",
    "dataset.save(dataset.crop(img, [1080, 1230, 960, 1110], 10), 'so', 6)\n",
    "dataset.save(dataset.crop(img, [1060, 1210, 1180, 1330], 10), 'ta', 6)\n",
    "dataset.save(dataset.crop(img, [1060, 1210, 1410, 1560], 10), 'chi', 6)\n",
    "dataset.save(dataset.crop(img, [1060, 1210, 1610, 1760], 10), 'tsu', 6)\n",
    "dataset.save(dataset.crop(img, [1060, 1210, 1840, 1990], 10), 'te', 6)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 2060, 2210], 10), 'to', 6)\n",
    "dataset.save(dataset.crop(img, [1350, 1500, 110, 260], 10), 'na', 6)\n",
    "dataset.save(dataset.crop(img, [1350, 1500, 330, 480], 10), 'ni', 6)\n",
    "dataset.save(dataset.crop(img, [1340, 1490, 540, 690], 10), 'nu', 6)\n",
    "dataset.save(dataset.crop(img, [1340, 1490, 760, 910], 10), 'ne', 6)\n",
    "dataset.save(dataset.crop(img, [1340, 1490, 970, 1120], 10), 'no', 6)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 1190, 1340], 10), 'ha', 6)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 1400, 1550], 10), 'hi', 6)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 1620, 1770], 10), 'fu', 6)\n",
    "dataset.save(dataset.crop(img, [1300, 1450, 1840, 1990], 10), 'he', 6)\n",
    "dataset.save(dataset.crop(img, [1300, 1450, 2060, 2210], 10), 'ho', 6)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 120, 270], 10), 'ma', 6)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 330, 480], 10), 'mi', 6)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 550, 700], 10), 'mu', 6)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 760, 910], 10), 'me', 6)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 980, 1130], 10), 'mo', 6)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1190, 1340], 10), 'ra', 6)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1420, 1570], 10), 'ri', 6)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1620, 1770], 10), 'ru', 6)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1840, 1990], 10), 're', 6)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 2050, 2200], 10), 'ro', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 120, 270], 10), 'ya', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 330, 480], 10), 'yu', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 560, 710], 10), 'yo', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 760, 910], 10), 'wa', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 980, 1130], 10), 'wo', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 1200, 1350], 10), 'n', 6)\n",
    "dataset.save(dataset.crop(img, [1860, 2010, 1410, 1560], 10), 'lowerCaseYa', 6)\n",
    "dataset.save(dataset.crop(img, [1850, 2000, 1630, 1780], 10), 'lowerCaseYu', 6)\n",
    "dataset.save(dataset.crop(img, [1850, 2000, 1850, 2000], 10), 'lowerCaseYo', 6)\n",
    "dataset.save(dataset.crop(img, [2390, 2540, 130, 280], 10), 'ga', 6)\n",
    "dataset.save(dataset.crop(img, [2380, 2530, 350, 500], 10), 'gi', 6)\n",
    "dataset.save(dataset.crop(img, [2390, 2540, 570, 720], 10), 'gu', 6)\n",
    "dataset.save(dataset.crop(img, [2380, 2530, 780, 930], 10), 'ge', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 1000, 1150], 10), 'go', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 1210, 1360], 10), 'za', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 1430, 1580], 10), 'ji', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 1630, 1780], 10), 'zu', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 1840, 1990], 10), 'ze', 6)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 2070, 2220], 10), 'zo', 6)\n",
    "dataset.save(dataset.crop(img, [2650, 2800, 140, 290], 10), 'da', 6)\n",
    "dataset.save(dataset.crop(img, [2650, 2800, 350, 500], 10), 'di', 6)\n",
    "dataset.save(dataset.crop(img, [2640, 2790, 570, 720], 10), 'du', 6)\n",
    "dataset.save(dataset.crop(img, [2650, 2800, 790, 940], 10), 'de', 6)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 1000, 1150], 10), 'do', 6)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 1210, 1360], 10), 'ba', 6)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 1420, 1570], 10), 'bi', 6)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 1630, 1780], 10), 'bu', 6)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 1850, 2000], 10), 'be', 6)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 2070, 2220], 10), 'bo', 6)\n",
    "dataset.save(dataset.crop(img, [2920, 3070, 130, 280], 10), 'pa', 6)\n",
    "dataset.save(dataset.crop(img, [2920, 3070, 360, 510], 10), 'pi', 6)\n",
    "dataset.save(dataset.crop(img, [2910, 3060, 570, 720], 10), 'pu', 6)\n",
    "dataset.save(dataset.crop(img, [2900, 3050, 790, 940], 10), 'pe', 6)\n",
    "dataset.save(dataset.crop(img, [2900, 3050, 1010, 1160], 10), 'po', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load1_3.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page1_3.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [850, 1000, 100, 250], 10), 'a', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 320, 470], 10), 'i', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 530, 680], 10), 'u', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 750, 900], 10), 'e', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 980, 1130], 10), 'o', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1190, 1340], 10), 'ka', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1400, 1550], 10), 'ki', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1630, 1780], 10), 'ku', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 1840, 1990], 10), 'ke', 7)\n",
    "dataset.save(dataset.crop(img, [840, 990, 2060, 2210], 10), 'ko', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 100, 250], 10), 'sa', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 330, 480], 10), 'shi', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 540, 690], 10), 'su', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 760, 910], 10), 'se', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 970, 1120], 10), 'so', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1190, 1340], 10), 'ta', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1420, 1570], 10), 'chi', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1630, 1780], 10), 'tsu', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1850, 2000], 10), 'te', 7)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 2040, 2190], 10), 'to', 7)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 100, 250], 10), 'na', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 320, 470], 10), 'ni', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 540, 690], 10), 'nu', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 760, 910], 10), 'ne', 7)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 960, 1110], 10), 'no', 7)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1200, 1350], 10), 'ha', 7)\n",
    "dataset.save(dataset.crop(img, [1230, 1380, 1410, 1560], 10), 'hi', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1630, 1780], 10), 'fu', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 1850, 2000], 10), 'he', 7)\n",
    "dataset.save(dataset.crop(img, [1220, 1370, 2070, 2220], 10), 'ho', 7)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 110, 260], 10), 'ma', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 330, 480], 10), 'mi', 7)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 550, 700], 10), 'mu', 7)\n",
    "dataset.save(dataset.crop(img, [1410, 1560, 750, 900], 10), 'me', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 970, 1120], 10), 'mo', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1190, 1340], 10), 'ra', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1390, 1540], 10), 'ri', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1630, 1780], 10), 'ru', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 1850, 2000], 10), 're', 7)\n",
    "dataset.save(dataset.crop(img, [1420, 1570, 2060, 2210], 10), 'ro', 7)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 120, 270], 10), 'ya', 7)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 330, 480], 10), 'yu', 7)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 550, 700], 10), 'yo', 7)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 770, 920], 10), 'wa', 7)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 980, 1130], 10), 'wo', 7)\n",
    "dataset.save(dataset.crop(img, [1600, 1750, 1190, 1340], 10), 'n', 7)\n",
    "dataset.save(dataset.crop(img, [1630, 1780, 1400, 1550], 10), 'lowerCaseYa', 7)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1630, 1780], 10), 'lowerCaseYu', 7)\n",
    "dataset.save(dataset.crop(img, [1610, 1760, 1840, 1990], 10), 'lowerCaseYo', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 120, 270], 10), 'ga', 7)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 340, 490], 10), 'gi', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 560, 710], 10), 'gu', 7)\n",
    "dataset.save(dataset.crop(img, [1980, 2130, 770, 920], 10), 'ge', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 990, 1140], 10), 'go', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1200, 1350], 10), 'za', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1420, 1570], 10), 'ji', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1620, 1770], 10), 'zu', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 1840, 1990], 10), 'ze', 7)\n",
    "dataset.save(dataset.crop(img, [1970, 2120, 2060, 2210], 10), 'zo', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 110, 260], 10), 'da', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 340, 490], 10), 'di', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 550, 700], 10), 'du', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 780, 930], 10), 'de', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 990, 1140], 10), 'do', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1200, 1350], 10), 'ba', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1410, 1560], 10), 'bi', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1630, 1780], 10), 'bu', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 1840, 1990], 10), 'be', 7)\n",
    "dataset.save(dataset.crop(img, [2160, 2310, 2060, 2210], 10), 'bo', 7)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 120, 270], 10), 'pa', 7)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 340, 490], 10), 'pi', 7)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 550, 700], 10), 'pu', 7)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 770, 920], 10), 'pe', 7)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 980, 1130], 10), 'po', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load2_3.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page2_3.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [830, 980, 130, 280], 10), 'a', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 360, 510], 10), 'i', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 580, 730], 10), 'u', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 790, 940], 10), 'e', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 1010, 1160], 10), 'o', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 1220, 1370], 10), 'ka', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 1450, 1600], 10), 'ki', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 1660, 1810], 10), 'ku', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 1880, 2030], 10), 'ke', 8)\n",
    "dataset.save(dataset.crop(img, [830, 980, 2100, 2250], 10), 'ko', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 140, 290], 10), 'sa', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 350, 500], 10), 'shi', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 570, 720], 10), 'su', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 800, 950], 10), 'se', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 1015, 1165], 10), 'so', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 1220, 1370], 10), 'ta', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 1440, 1590], 10), 'chi', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 1660, 1810], 10), 'tsu', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 1880, 2030], 10), 'te', 8)\n",
    "dataset.save(dataset.crop(img, [1020, 1170, 2095, 2245], 10), 'to', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 120, 270], 10), 'na', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 350, 500], 10), 'ni', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 570, 720], 10), 'nu', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 785, 935], 10), 'ne', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 1010, 1160], 10), 'no', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 1225, 1375], 10), 'ha', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 1440, 1590], 10), 'hi', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 1655, 1805], 10), 'fu', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 1875, 2025], 10), 'he', 8)\n",
    "dataset.save(dataset.crop(img, [1205, 1355, 2095, 2245], 10), 'ho', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 150, 300], 10), 'ma', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 355, 505], 10), 'mi', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 580, 730], 10), 'mu', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 795, 945], 10), 'me', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 1015, 1165], 10), 'mo', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 1225, 1375], 10), 'ra', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 1445, 1595], 10), 'ri', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 1660, 1810], 10), 'ru', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 1870, 2020], 10), 're', 8)\n",
    "dataset.save(dataset.crop(img, [1395, 1545, 2090, 2240], 10), 'ro', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 150, 300], 10), 'ya', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 365, 515], 10), 'yu', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 585, 735], 10), 'yo', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 795, 945], 10), 'wa', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1015, 1165], 10), 'wo', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1225, 1375], 10), 'n', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1445, 1595], 10), 'lowerCaseYa', 8)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 1650, 1800], 10), 'lowerCaseYu', 8)\n",
    "dataset.save(dataset.crop(img, [1590, 1740, 1870, 2020], 10), 'lowerCaseYo', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 150, 300], 10), 'ga', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 375, 525], 10), 'gi', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 595, 745], 10), 'gu', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 800, 950], 10), 'ge', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 1015, 1165], 10), 'go', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 1230, 1380], 10), 'za', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 1445, 1595], 10), 'ji', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 1655, 1805], 10), 'zu', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 1870, 2020], 10), 'ze', 8)\n",
    "dataset.save(dataset.crop(img, [1950, 2100, 2090, 2240], 10), 'zo', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 150, 300], 10), 'da', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 375, 525], 10), 'di', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 590, 740], 10), 'du', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 805, 955], 10), 'de', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 1015, 1165], 10), 'do', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 1230, 1380], 10), 'ba', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 1445, 1595], 10), 'bi', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 1655, 1805], 10), 'bu', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 1870, 2020], 10), 'be', 8)\n",
    "dataset.save(dataset.crop(img, [2140, 2290, 2085, 2235], 10), 'bo', 8)\n",
    "dataset.save(dataset.crop(img, [2325, 2475, 155, 305], 10), 'pa', 8)\n",
    "dataset.save(dataset.crop(img, [2325, 2475, 370, 520], 10), 'pi', 8)\n",
    "dataset.save(dataset.crop(img, [2325, 2475, 585, 735], 10), 'pu', 8)\n",
    "dataset.save(dataset.crop(img, [2325, 2475, 800, 950], 10), 'pe', 8)\n",
    "dataset.save(dataset.crop(img, [2325, 2475, 1015, 1165], 10), 'po', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint16\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# %load load3_3.py\n",
    "from skimage.io import imread\n",
    "import dataset\n",
    "\n",
    "img = imread('dataset_raw/page3_3.jpg')\n",
    "\n",
    "dataset.save(dataset.crop(img, [760, 910, 80, 230], 10), 'a', 9)\n",
    "dataset.save(dataset.crop(img, [760, 910, 300, 450], 10), 'i', 9)\n",
    "dataset.save(dataset.crop(img, [750, 900, 520, 670], 10), 'u', 9)\n",
    "dataset.save(dataset.crop(img, [750, 900, 750, 900], 10), 'e', 9)\n",
    "dataset.save(dataset.crop(img, [750, 900, 960, 1110], 10), 'o', 9)\n",
    "dataset.save(dataset.crop(img, [740, 890, 1180, 1330], 10), 'ka', 9)\n",
    "dataset.save(dataset.crop(img, [750, 900, 1420, 1570], 10), 'ki', 9)\n",
    "dataset.save(dataset.crop(img, [740, 890, 1640, 1790], 10), 'ku', 9)\n",
    "dataset.save(dataset.crop(img, [730, 880, 1860, 2010], 10), 'ke', 9)\n",
    "dataset.save(dataset.crop(img, [740, 890, 2080, 2230], 10), 'ko', 9)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 90, 240], 10), 'sa', 9)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 320, 470], 10), 'shi', 9)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 530, 680], 10), 'su', 9)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 750, 900], 10), 'se', 9)\n",
    "dataset.save(dataset.crop(img, [1040, 1190, 960, 1110], 10), 'so', 9)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1200, 1350], 10), 'ta', 9)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1430, 1580], 10), 'chi', 9)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1630, 1780], 10), 'tsu', 9)\n",
    "dataset.save(dataset.crop(img, [1030, 1180, 1860, 2010], 10), 'te', 9)\n",
    "dataset.save(dataset.crop(img, [1010, 1160, 2080, 2230], 10), 'to', 9)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 100, 250], 10), 'na', 9)\n",
    "dataset.save(dataset.crop(img, [1320, 1470, 320, 470], 10), 'ni', 9)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 530, 680], 10), 'nu', 9)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 750, 900], 10), 'ne', 9)\n",
    "dataset.save(dataset.crop(img, [1310, 1460, 960, 1110], 10), 'no', 9)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1200, 1350], 10), 'ha', 9)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1420, 1570], 10), 'hi', 9)\n",
    "dataset.save(dataset.crop(img, [1290, 1440, 1640, 1790], 10), 'fu', 9)\n",
    "dataset.save(dataset.crop(img, [1280, 1430, 1860, 2010], 10), 'he', 9)\n",
    "dataset.save(dataset.crop(img, [1270, 1420, 2080, 2230], 10), 'ho', 9)\n",
    "dataset.save(dataset.crop(img, [1580, 1730, 110, 260], 10), 'ma', 9)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 320, 470], 10), 'mi', 9)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 540, 690], 10), 'mu', 9)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 750, 900], 10), 'me', 9)\n",
    "dataset.save(dataset.crop(img, [1570, 1720, 970, 1120], 10), 'mo', 9)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1210, 1360], 10), 'ra', 9)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1440, 1590], 10), 'ri', 9)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1640, 1790], 10), 'ru', 9)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 1860, 2010], 10), 're', 9)\n",
    "dataset.save(dataset.crop(img, [1550, 1700, 2070, 2220], 10), 'ro', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 100, 250], 10), 'ya', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 330, 480], 10), 'yu', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 560, 710], 10), 'yo', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 770, 920], 10), 'wa', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 990, 1140], 10), 'wo', 9)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1220, 1370], 10), 'n', 9)\n",
    "dataset.save(dataset.crop(img, [1830, 1980, 1430, 1580], 10), 'lowerCaseYa', 9)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1650, 1800], 10), 'lowerCaseYu', 9)\n",
    "dataset.save(dataset.crop(img, [1820, 1970, 1870, 2020], 10), 'lowerCaseYo', 9)\n",
    "dataset.save(dataset.crop(img, [2370, 2520, 130, 280], 10), 'ga', 9)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 350, 500], 10), 'gi', 9)\n",
    "dataset.save(dataset.crop(img, [2370, 2520, 570, 720], 10), 'gu', 9)\n",
    "dataset.save(dataset.crop(img, [2360, 2510, 780, 930], 10), 'ge', 9)\n",
    "dataset.save(dataset.crop(img, [2350, 2500, 1000, 1150], 10), 'go', 9)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1230, 1380], 10), 'za', 9)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1450, 1600], 10), 'ji', 9)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1650, 1800], 10), 'zu', 9)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 1860, 2010], 10), 'ze', 9)\n",
    "dataset.save(dataset.crop(img, [2340, 2490, 2090, 2240], 10), 'zo', 9)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 150, 300], 10), 'da', 9)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 360, 510], 10), 'di', 9)\n",
    "dataset.save(dataset.crop(img, [2620, 2770, 580, 730], 10), 'du', 9)\n",
    "dataset.save(dataset.crop(img, [2630, 2780, 800, 950], 10), 'de', 9)\n",
    "dataset.save(dataset.crop(img, [2610, 2760, 1010, 1160], 10), 'do', 9)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1230, 1380], 10), 'ba', 9)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1440, 1590], 10), 'bi', 9)\n",
    "dataset.save(dataset.crop(img, [2600, 2750, 1650, 1800], 10), 'bu', 9)\n",
    "dataset.save(dataset.crop(img, [2590, 2740, 1870, 2020], 10), 'be', 9)\n",
    "dataset.save(dataset.crop(img, [2590, 2740, 2090, 2240], 10), 'bo', 9)\n",
    "dataset.save(dataset.crop(img, [2890, 3040, 150, 300], 10), 'pa', 9)\n",
    "dataset.save(dataset.crop(img, [2890, 3040, 370, 520], 10), 'pi', 9)\n",
    "dataset.save(dataset.crop(img, [2880, 3030, 580, 730], 10), 'pu', 9)\n",
    "dataset.save(dataset.crop(img, [2870, 3020, 800, 950], 10), 'pe', 9)\n",
    "dataset.save(dataset.crop(img, [2870, 3020, 1020, 1170], 10), 'po', 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37611963  0.37611963  0.37611963 ...,  0.41647974  0.41255818\n",
      "  0.40863661]\n",
      "[ 0.37947662  0.38339818  0.38339818 ...,  0.42767987  0.4237583\n",
      "  0.41983673]\n",
      "[ 0.36043336  0.36827649  0.3800412  ...,  0.40471504  0.40471504\n",
      "  0.40079347]\n",
      "[ 0.34474708  0.36435492  0.37219806 ...,  0.41255818  0.40863661\n",
      "  0.40863661]\n",
      "[ 0.36435492  0.36043336  0.36435492 ...,  0.40471504  0.40079347\n",
      "  0.40863661]\n",
      "[ 0.36435492  0.36827649  0.36827649 ...,  0.40471504  0.40471504\n",
      "  0.40471504]\n",
      "[ 0.35259022  0.36043336  0.36435492 ...,  0.41255818  0.40471504\n",
      "  0.40079347]\n",
      "[ 0.36942092  0.37334249  0.36157778 ...,  0.41171893  0.40779736\n",
      "  0.40779736]\n",
      "[ 0.38118563  0.37726406  0.37726406 ...,  0.41171893  0.4156405\n",
      "  0.41956207]\n",
      "[ 0.36942092  0.38902876  0.37726406 ...,  0.41171893  0.4156405\n",
      "  0.40779736]\n",
      "[ 0.38118563  0.37726406  0.36942092 ...,  0.4156405   0.4156405\n",
      "  0.41171893]\n",
      "[ 0.37219806  0.36827649  0.36827649 ...,  0.41647974  0.41647974\n",
      "  0.40863661]\n",
      "[ 0.39295033  0.3968719   0.38510719 ...,  0.41171893  0.40387579\n",
      "  0.40779736]\n",
      "[ 0.37611963  0.3800412   0.3800412  ...,  0.42824445  0.42824445\n",
      "  0.42432288]\n",
      "[ 0.38788434  0.38788434  0.38788434 ...,  0.41647974  0.42040131\n",
      "  0.42040131]\n",
      "[ 0.39180591  0.39180591  0.38396277 ...,  0.42824445  0.42432288\n",
      "  0.42432288]\n",
      "[ 0.38396277  0.38396277  0.38396277 ...,  0.41647974  0.40079347\n",
      "  0.40471504]\n",
      "[ 0.38396277  0.38396277  0.38396277 ...,  0.42432288  0.42432288\n",
      "  0.42432288]\n",
      "[ 0.35202564  0.3559472   0.36771191 ...,  0.40415045  0.40807202\n",
      "  0.41199359]\n",
      "[ 0.36379034  0.36379034  0.35986877 ...,  0.41255818  0.41255818\n",
      "  0.40863661]\n",
      "[ 0.37555505  0.37163348  0.36771191 ...,  0.39630732  0.39630732\n",
      "  0.40022889]\n",
      "[ 0.3559472   0.35986877  0.36771191 ...,  0.41199359  0.41591516\n",
      "  0.41591516]\n",
      "[ 0.36435492  0.36435492  0.36827649 ...,  0.40863661  0.40471504\n",
      "  0.41255818]\n",
      "[ 0.36771191  0.37163348  0.36771191 ...,  0.41199359  0.41199359\n",
      "  0.40415045]\n",
      "[ 0.36942092  0.37334249  0.38118563 ...,  0.40079347  0.41255818\n",
      "  0.41647974]\n",
      "[ 0.36549935  0.36549935  0.36942092 ...,  0.41255818  0.41255818\n",
      "  0.41647974]\n",
      "[ 0.37334249  0.37334249  0.36942092 ...,  0.41647974  0.41255818\n",
      "  0.42432288]\n",
      "[ 0.37726406  0.37726406  0.37726406 ...,  0.40863661  0.40471504\n",
      "  0.40863661]\n",
      "[ 0.36942092  0.36549935  0.36942092 ...,  0.42824445  0.42040131\n",
      "  0.40863661]\n",
      "[ 0.37726406  0.37726406  0.37334249 ...,  0.41647974  0.41647974\n",
      "  0.41647974]\n",
      "[ 0.37726406  0.37334249  0.37726406 ...,  0.42432288  0.42040131\n",
      "  0.41647974]\n",
      "[ 0.37726406  0.37726406  0.36942092 ...,  0.41255818  0.41647974\n",
      "  0.41647974]\n",
      "[ 0.38118563  0.38118563  0.38118563 ...,  0.40863661  0.40471504\n",
      "  0.40863661]\n",
      "[ 0.37611963  0.3800412   0.3800412  ...,  0.40471504  0.40471504\n",
      "  0.40863661]\n",
      "[ 0.38118563  0.37726406  0.37726406 ...,  0.42824445  0.42824445\n",
      "  0.42824445]\n",
      "[ 0.38339818  0.37163348  0.36771191 ...,  0.41199359  0.41199359\n",
      "  0.41199359]\n",
      "[ 0.36771191  0.36771191  0.36771191 ...,  0.41199359  0.41591516\n",
      "  0.41591516]\n",
      "[ 0.36379034  0.36379034  0.35986877 ...,  0.41199359  0.41199359\n",
      "  0.41591516]\n",
      "[ 0.37555505  0.36771191  0.37163348 ...,  0.41199359  0.41199359\n",
      "  0.40807202]\n",
      "[ 0.37947662  0.37947662  0.37555505 ...,  0.40807202  0.41199359\n",
      "  0.41591516]\n",
      "[ 0.36435492  0.36435492  0.37611963 ...,  0.41255818  0.41647974\n",
      "  0.41647974]\n",
      "[ 0.39124132  0.38731975  0.37947662 ...,  0.42767987  0.4237583\n",
      "  0.41591516]\n",
      "[ 0.37555505  0.38339818  0.39516289 ...,  0.41983673  0.41983673\n",
      "  0.4237583 ]\n",
      "[ 0.38062104  0.38454261  0.39238575 ...,  0.41983673  0.41983673\n",
      "  0.41591516]\n",
      "[ 0.38339818  0.39124132  0.39908446 ...,  0.4237583   0.42767987  0.435523  ]\n",
      "[ 0.55097276  0.54705119  0.54312963 ...,  0.53694972  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.533608    0.53752956  0.54145113 ...,  0.53246357  0.528542    0.528542  ]\n",
      "[ 0.53302815  0.53302815  0.52910658 ...,  0.50893416  0.50893416\n",
      "  0.50893416]\n",
      "[ 0.52518502  0.52910658  0.52518502 ...,  0.50501259  0.50109102\n",
      "  0.50501259]\n",
      "[ 0.52518502  0.52910658  0.52126345 ...,  0.50501259  0.51285573\n",
      "  0.51285573]\n",
      "[ 0.52910658  0.52518502  0.52126345 ...,  0.51285573  0.51285573\n",
      "  0.51285573]\n",
      "[ 0.51734188  0.52126345  0.52126345 ...,  0.51285573  0.50893416\n",
      "  0.50893416]\n",
      "[ 0.55489433  0.5588159   0.55489433 ...,  0.54705119  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.55489433  0.55489433  0.55097276 ...,  0.53920806  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.54705119  0.55097276  0.55097276 ...,  0.54312963  0.54705119\n",
      "  0.55097276]\n",
      "[ 0.54705119  0.55489433  0.5588159  ...,  0.53136492  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54479286  0.54479286\n",
      "  0.54479286]\n",
      "[ 0.55097276  0.54705119  0.54705119 ...,  0.54312963  0.53528649\n",
      "  0.53920806]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.55097276  0.54705119  0.54312963 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.53920806  0.54312963 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.54312963  0.53920806  0.53528649 ...,  0.53920806  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.5588159   0.55489433  0.55097276 ...,  0.54087129  0.54087129\n",
      "  0.54087129]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.51285573  0.50893416\n",
      "  0.50109102]\n",
      "[ 0.51792172  0.51400015  0.51400015 ...,  0.50501259  0.50893416\n",
      "  0.50893416]\n",
      "[ 0.51960021  0.51567864  0.51960021 ...,  0.50893416  0.50893416\n",
      "  0.50501259]\n",
      "[ 0.52184329  0.51792172  0.51792172 ...,  0.50109102  0.50501259\n",
      "  0.50501259]\n",
      "[ 0.54312963  0.54312963  0.54705119 ...,  0.53694972  0.53302815\n",
      "  0.53302815]\n",
      "[ 0.51007858  0.51007858  0.51007858 ...,  0.50893416  0.50893416\n",
      "  0.50501259]\n",
      "[ 0.5588159   0.55097276  0.55097276 ...,  0.54087129  0.54479286  0.552636  ]\n",
      "[ 0.5588159   0.5588159   0.5588159  ...,  0.54479286  0.552636    0.55655756]\n",
      "[ 0.54312963  0.54705119  0.55097276 ...,  0.54479286  0.54479286\n",
      "  0.54479286]\n",
      "[ 0.55489433  0.55097276  0.55097276 ...,  0.54479286  0.54479286\n",
      "  0.54479286]\n",
      "[ 0.54705119  0.55097276  0.54312963 ...,  0.54479286  0.54479286\n",
      "  0.54871443]\n",
      "[ 0.54312963  0.54705119  0.55097276 ...,  0.54705119  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.5588159   0.55489433 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53920806  0.53920806  0.53528649 ...,  0.54312963  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.55097276  0.55097276  0.55097276 ...,  0.53528649  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.53694972  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.54705119  0.53920806  0.53920806 ...,  0.53528649  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.52576486  0.52576486  0.53752956 ...,  0.51285573  0.51285573\n",
      "  0.51285573]\n",
      "[ 0.52968643  0.52968643  0.53752956 ...,  0.50109102  0.50501259\n",
      "  0.50893416]\n",
      "[ 0.52126345  0.52518502  0.52910658 ...,  0.51677729  0.52069886\n",
      "  0.52069886]\n",
      "[ 0.52576486  0.52968643  0.533608   ...,  0.52069886  0.52069886\n",
      "  0.52069886]\n",
      "[ 0.51792172  0.51400015  0.51400015 ...,  0.51285573  0.51677729\n",
      "  0.51285573]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.53302815  0.53302815\n",
      "  0.53694972]\n",
      "[ 0.5453727   0.5453727   0.54145113 ...,  0.54030671  0.54030671\n",
      "  0.54030671]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.54030671  0.54030671\n",
      "  0.53638514]\n",
      "[ 0.54929427  0.5453727   0.54145113 ...,  0.54030671  0.54030671\n",
      "  0.54030671]\n",
      "[ 0.5453727   0.5453727   0.5453727  ...,  0.53246357  0.53246357\n",
      "  0.53638514]\n",
      "[ 0.50052644  0.50836957  0.51621271 ...,  0.50107576  0.48538949\n",
      "  0.47754635]\n",
      "[ 0.51088731  0.51088731  0.51088731 ...,  0.49266804  0.49266804\n",
      "  0.49266804]\n",
      "[ 0.51311513  0.50919356  0.50919356 ...,  0.4730602   0.48090333\n",
      "  0.48090333]\n",
      "[ 0.51311513  0.51311513  0.50135042 ...,  0.48090333  0.48090333\n",
      "  0.47698177]\n",
      "[ 0.50919356  0.50919356  0.50919356 ...,  0.4730602   0.4730602\n",
      "  0.47698177]\n",
      "[ 0.50919356  0.51311513  0.51311513 ...,  0.46913863  0.4730602\n",
      "  0.47698177]\n",
      "[ 0.52487984  0.52487984  0.52095827 ...,  0.4730602   0.47698177\n",
      "  0.48090333]\n",
      "[ 0.51007858  0.51792172  0.51792172 ...,  0.51229114  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.51792172  0.51792172  0.52184329 ...,  0.49660487  0.48876173\n",
      "  0.48876173]\n",
      "[ 0.51007858  0.51007858  0.51400015 ...,  0.50836957  0.504448    0.504448  ]\n",
      "[ 0.51400015  0.51792172  0.51007858 ...,  0.4926833   0.50052644\n",
      "  0.49660487]\n",
      "[ 0.51621271  0.51621271  0.51621271 ...,  0.50107576  0.50107576\n",
      "  0.49715419]\n",
      "[ 0.52576486  0.51792172  0.51007858 ...,  0.50836957  0.504448    0.50052644]\n",
      "[ 0.51229114  0.51229114  0.51229114 ...,  0.49715419  0.49715419\n",
      "  0.49715419]\n",
      "[ 0.51229114  0.50836957  0.50836957 ...,  0.48538949  0.48538949\n",
      "  0.48931106]\n",
      "[ 0.504448    0.51229114  0.51229114 ...,  0.49715419  0.49715419\n",
      "  0.49715419]\n",
      "[ 0.51621271  0.51621271  0.51621271 ...,  0.49323262  0.48874647\n",
      "  0.48874647]\n",
      "[ 0.52405585  0.51621271  0.51229114 ...,  0.49715419  0.50107576\n",
      "  0.50107576]\n",
      "[ 0.50304417  0.50304417  0.49912261 ...,  0.48090333  0.47698177\n",
      "  0.47698177]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.48090333  0.48090333\n",
      "  0.48090333]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.46913863  0.46913863\n",
      "  0.46521706]\n",
      "[ 0.51088731  0.51088731  0.51088731 ...,  0.4730602   0.4730602   0.4730602 ]\n",
      "[ 0.50836957  0.51621271  0.51621271 ...,  0.50107576  0.49715419\n",
      "  0.50107576]\n",
      "[ 0.49520104  0.49912261  0.51088731 ...,  0.46521706  0.46521706\n",
      "  0.46521706]\n",
      "[ 0.51621271  0.51229114  0.51229114 ...,  0.50304417  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.51621271  0.51229114  0.50836957 ...,  0.50304417  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.49381247  0.50557717  0.51342031 ...,  0.50052644  0.50052644  0.504448  ]\n",
      "[ 0.51621271  0.52013428  0.52013428 ...,  0.50304417  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.52518502  0.52910658  0.51734188 ...,  0.49660487  0.4926833   0.4926833 ]\n",
      "[ 0.51734188  0.51342031  0.51734188 ...,  0.51480888  0.51088731\n",
      "  0.50304417]\n",
      "[ 0.52126345  0.50949874  0.51342031 ...,  0.50696574  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.52910658  0.52910658  0.52126345 ...,  0.50696574  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.50304417  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.504448    0.504448    0.50836957 ...,  0.49715419  0.49323262\n",
      "  0.48931106]\n",
      "[ 0.51342031  0.50949874  0.51342031 ...,  0.50304417  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.50304417  0.51088731  0.50304417 ...,  0.48090333  0.4848249   0.4848249 ]\n",
      "[ 0.51088731  0.51480888  0.51088731 ...,  0.48090333  0.4848249   0.4848249 ]\n",
      "[ 0.49912261  0.49912261  0.49127947 ...,  0.48090333  0.48874647\n",
      "  0.4848249 ]\n",
      "[ 0.51088731  0.51088731  0.51088731 ...,  0.4730602   0.4730602   0.4730602 ]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.4848249   0.4848249   0.4848249 ]\n",
      "[ 0.51229114  0.51229114  0.50836957 ...,  0.50107576  0.49715419\n",
      "  0.48931106]\n",
      "[ 0.51088731  0.50304417  0.51873045 ...,  0.49266804  0.49266804\n",
      "  0.49266804]\n",
      "[ 0.49912261  0.50696574  0.51088731 ...,  0.50443275  0.50443275\n",
      "  0.50443275]\n",
      "[ 0.51480888  0.51480888  0.51873045 ...,  0.50051118  0.49658961\n",
      "  0.49658961]\n",
      "[ 0.51480888  0.51480888  0.51088731 ...,  0.50051118  0.49658961\n",
      "  0.49658961]\n",
      "[ 0.53920806  0.54312963  0.54312963 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53638514  0.54030671  0.54030671 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52069886  0.51677729  0.51677729 ...,  0.50501259  0.50893416\n",
      "  0.50893416]\n",
      "[ 0.52069886  0.51677729  0.51285573 ...,  0.50109102  0.49324788\n",
      "  0.48148318]\n",
      "[ 0.51734188  0.51734188  0.52126345 ...,  0.48932631  0.49324788\n",
      "  0.50109102]\n",
      "[ 0.51285573  0.51285573  0.51285573 ...,  0.50109102  0.49716945\n",
      "  0.49716945]\n",
      "[ 0.51734188  0.51734188  0.51342031 ...,  0.50893416  0.50109102\n",
      "  0.49716945]\n",
      "[ 0.55097276  0.54312963  0.53528649 ...,  0.53136492  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.54312963  0.54312963  0.53920806 ...,  0.54312963  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.53528649  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.54312963  0.55097276  0.54705119 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53528649  0.53528649  0.53136492 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53920806  0.53528649  0.53136492 ...,  0.53136492  0.53136492\n",
      "  0.53920806]\n",
      "[ 0.5453727   0.54929427  0.54929427 ...,  0.52576486  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.53752956  0.54145113  0.54145113 ...,  0.52576486  0.52968643  0.533608  ]\n",
      "[ 0.53752956  0.53752956  0.54145113 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.533608    0.54145113  0.5453727  ...,  0.52968643  0.52968643\n",
      "  0.53752956]\n",
      "[ 0.54312963  0.54929427  0.54929427 ...,  0.533608    0.52968643\n",
      "  0.52576486]\n",
      "[ 0.50893416  0.51285573  0.51285573 ...,  0.50052644  0.50052644  0.504448  ]\n",
      "[ 0.50893416  0.51285573  0.52069886 ...,  0.49660487  0.504448    0.504448  ]\n",
      "[ 0.51677729  0.52462043  0.52462043 ...,  0.504448    0.51621271\n",
      "  0.51229114]\n",
      "[ 0.50893416  0.51285573  0.53246357 ...,  0.50052644  0.50052644\n",
      "  0.49660487]\n",
      "[ 0.53920806  0.53528649  0.53528649 ...,  0.52462043  0.52462043\n",
      "  0.52462043]\n",
      "[ 0.49716945  0.51285573  0.52069886 ...,  0.50052644  0.50052644\n",
      "  0.49660487]\n",
      "[ 0.54312963  0.53920806  0.54312963 ...,  0.52968643  0.52968643  0.533608  ]\n",
      "[ 0.53528649  0.53136492  0.53528649 ...,  0.52968643  0.52968643  0.533608  ]\n",
      "[ 0.55489433  0.55097276  0.5588159  ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.54705119  0.55489433  0.55489433 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.54312963  0.54705119  0.55097276 ...,  0.54705119  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.55489433  0.54705119  0.53528649 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.53920806  0.53528649  0.53528649 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.52968643  0.533608    0.53752956]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.55489433  0.55097276  0.54705119 ...,  0.533608    0.533608    0.52576486]\n",
      "[ 0.51400015  0.51792172  0.52184329 ...,  0.51229114  0.50836957\n",
      "  0.51229114]\n",
      "[ 0.52184329  0.52184329  0.51792172 ...,  0.4926833   0.49660487\n",
      "  0.50052644]\n",
      "[ 0.52968643  0.52968643  0.52576486 ...,  0.50836957  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.504448    0.50052644\n",
      "  0.50052644]\n",
      "[ 0.52462043  0.52462043  0.528542   ...,  0.50836957  0.504448    0.49660487]\n",
      "[ 0.54312963  0.53920806  0.53528649 ...,  0.53638514  0.53638514\n",
      "  0.54030671]\n",
      "[ 0.54030671  0.53638514  0.53638514 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.53246357  0.53638514  0.54422827 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.53246357  0.528542    0.528542   ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.54145113  0.53752956  0.533608   ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.51088731  0.51088731  0.50696574 ...,  0.46353857  0.46746014\n",
      "  0.4713817 ]\n",
      "[ 0.49658961  0.50051118  0.49266804 ...,  0.47530327  0.47922484\n",
      "  0.48258183]\n",
      "[ 0.48090333  0.48090333  0.4848249  ...,  0.454551    0.454551    0.454551  ]\n",
      "[ 0.48090333  0.48090333  0.48090333 ...,  0.44670787  0.44670787\n",
      "  0.45062943]\n",
      "[ 0.46521706  0.46521706  0.4730602  ...,  0.47023728  0.46239414\n",
      "  0.43886473]\n",
      "[ 0.48090333  0.48090333  0.48090333 ...,  0.45062943  0.45062943\n",
      "  0.45062943]\n",
      "[ 0.4848249   0.4848249   0.48090333 ...,  0.454551    0.45847257\n",
      "  0.46239414]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.50696574  0.51088731  0.50696574 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.50304417  0.49912261  0.50304417 ...,  0.47502861  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.47502861  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.47530327  0.47530327\n",
      "  0.47530327]\n",
      "[ 0.49520104  0.49127947  0.49127947 ...,  0.47110704  0.47502861\n",
      "  0.47110704]\n",
      "[ 0.49266804  0.48874647  0.48874647 ...,  0.46353857  0.45569543\n",
      "  0.46746014]\n",
      "[ 0.49658961  0.49266804  0.48874647 ...,  0.4713817   0.4713817\n",
      "  0.47530327]\n",
      "[ 0.48874647  0.48874647  0.48874647 ...,  0.46746014  0.46353857\n",
      "  0.46746014]\n",
      "[ 0.50051118  0.49658961  0.49266804 ...,  0.4713817   0.46746014\n",
      "  0.4713817 ]\n",
      "[ 0.50443275  0.50835431  0.51619745 ...,  0.47922484  0.46746014  0.459617  ]\n",
      "[ 0.47698177  0.47698177  0.48090333 ...,  0.45847257  0.454551    0.454551  ]\n",
      "[ 0.48090333  0.4848249   0.48874647 ...,  0.45062943  0.45062943\n",
      "  0.44670787]\n",
      "[ 0.48090333  0.47698177  0.4848249  ...,  0.44393072  0.44785229\n",
      "  0.44785229]\n",
      "[ 0.4730602   0.4730602   0.4730602  ...,  0.4427863   0.43886473\n",
      "  0.43886473]\n",
      "[ 0.50107576  0.50107576  0.50499733 ...,  0.47530327  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.46521706  0.46913863  0.4730602  ...,  0.44785229  0.454551    0.45062943]\n",
      "[ 0.50107576  0.5089189   0.50499733 ...,  0.48706798  0.459617    0.47530327]\n",
      "[ 0.49715419  0.50499733  0.50499733 ...,  0.48706798  0.49491112\n",
      "  0.48706798]\n",
      "[ 0.50304417  0.50696574  0.51088731 ...,  0.47530327  0.47922484\n",
      "  0.47530327]\n",
      "[ 0.50107576  0.49715419  0.49715419 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.51480888  0.51480888  0.50304417 ...,  0.47530327  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.49323262  0.49715419  0.49715419 ...,  0.4713817   0.4713817\n",
      "  0.46746014]\n",
      "[ 0.49715419  0.50107576  0.50499733 ...,  0.47530327  0.4713817   0.4713817 ]\n",
      "[ 0.50499733  0.50499733  0.49715419 ...,  0.48314641  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.49715419  0.51227588  0.50835431 ...,  0.47922484  0.4713817\n",
      "  0.47530327]\n",
      "[ 0.49912261  0.49520104  0.49912261 ...,  0.4713817   0.4713817   0.4713817 ]\n",
      "[ 0.50107576  0.50107576  0.50499733 ...,  0.49491112  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.47698177  0.48090333  0.4848249  ...,  0.46239414  0.46239414\n",
      "  0.45847257]\n",
      "[ 0.4848249   0.4848249   0.48090333 ...,  0.46239414  0.46239414\n",
      "  0.45847257]\n",
      "[ 0.48874647  0.48874647  0.48874647 ...,  0.454551    0.454551    0.45847257]\n",
      "[ 0.47698177  0.47698177  0.4730602  ...,  0.454551    0.45847257  0.454551  ]\n",
      "[ 0.48090333  0.48874647  0.49266804 ...,  0.45847257  0.45847257\n",
      "  0.45847257]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.48706798  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.48874647  0.49266804  0.49266804 ...,  0.46689555  0.46689555\n",
      "  0.47081712]\n",
      "[ 0.49658961  0.49658961  0.49658961 ...,  0.46746014  0.46746014\n",
      "  0.46353857]\n",
      "[ 0.50051118  0.49658961  0.4848249  ...,  0.47866026  0.47473869\n",
      "  0.47473869]\n",
      "[ 0.50051118  0.50443275  0.49658961 ...,  0.47081712  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.52968643  0.52968643  0.52184329 ...,  0.50557717  0.50557717\n",
      "  0.51342031]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.51873045  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.51285573  0.51285573  0.51285573 ...,  0.48958572  0.48958572\n",
      "  0.48566415]\n",
      "[ 0.53638514  0.53638514  0.528542   ...,  0.49350729  0.49350729\n",
      "  0.49350729]\n",
      "[ 0.50501259  0.51677729  0.51677729 ...,  0.48091859  0.48091859\n",
      "  0.48484016]\n",
      "[ 0.51677729  0.51285573  0.50893416 ...,  0.49350729  0.49660487\n",
      "  0.49660487]\n",
      "[ 0.50501259  0.50893416  0.50893416 ...,  0.4926833   0.49660487\n",
      "  0.49660487]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51400015  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51792172  0.51792172\n",
      "  0.51400015]\n",
      "[ 0.51400015  0.51792172  0.52576486 ...,  0.52576486  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51400015  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.52968643  0.533608    0.52968643 ...,  0.51621271  0.50836957  0.504448  ]\n",
      "[ 0.54145113  0.54145113  0.53752956 ...,  0.52576486  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51229114  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.52184329  0.52576486  0.52184329 ...,  0.52013428  0.52013428\n",
      "  0.51621271]\n",
      "[ 0.53752956  0.54145113  0.53752956 ...,  0.51621271  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.53752956  0.533608    0.53752956 ...,  0.51621271  0.51229114\n",
      "  0.51621271]\n",
      "[ 0.52968643  0.533608    0.53752956 ...,  0.51229114  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.51229114  0.50836957  0.51229114 ...,  0.48343633  0.48343633\n",
      "  0.49520104]\n",
      "[ 0.51229114  0.504448    0.50836957 ...,  0.49127947  0.49127947\n",
      "  0.49520104]\n",
      "[ 0.50836957  0.51621271  0.51621271 ...,  0.49127947  0.49127947\n",
      "  0.49127947]\n",
      "[ 0.50836957  0.51229114  0.51229114 ...,  0.49127947  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.51229114  0.51229114  0.50836957 ...,  0.49520104  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.5453727   0.54145113  0.5453727  ...,  0.52405585  0.52013428\n",
      "  0.51229114]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.52797742  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.53752956  0.5453727   0.53752956 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51621271  0.52013428\n",
      "  0.52405585]\n",
      "[ 0.5453727   0.54145113  0.54929427 ...,  0.52518502  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.52968643  0.52968643  0.52184329 ...,  0.52797742  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.52013428  0.52013428\n",
      "  0.52405585]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.51621271  0.51621271\n",
      "  0.52013428]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.52126345  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.52518502  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.504448    0.51229114  0.52405585 ...,  0.49520104  0.49520104\n",
      "  0.50304417]\n",
      "[ 0.504448    0.50052644  0.50052644 ...,  0.50696574  0.50304417\n",
      "  0.49520104]\n",
      "[ 0.51229114  0.51621271  0.52405585 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.51621271  0.50836957  0.51229114 ...,  0.49127947  0.49520104\n",
      "  0.49912261]\n",
      "[ 0.50836957  0.50836957  0.504448   ...,  0.49912261  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.52968643  0.52968643  0.533608   ...,  0.50949874  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.52518502  0.52910658  0.52910658 ...,  0.52657359  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.53302815  0.53302815  0.53302815 ...,  0.51873045  0.51873045\n",
      "  0.52265202]\n",
      "[ 0.52910658  0.52518502  0.52910658 ...,  0.50696574  0.51088731\n",
      "  0.51480888]\n",
      "[ 0.53189899  0.53582055  0.53189899 ...,  0.52265202  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.46718547  0.46718547  0.47502861 ...,  0.49855802  0.51032273\n",
      "  0.50247959]\n",
      "[ 0.4783856   0.47446403  0.47054246 ...,  0.49071489  0.49855802\n",
      "  0.50640116]\n",
      "[ 0.49463645  0.49463645  0.49071489 ...,  0.49855802  0.49463645\n",
      "  0.48287175]\n",
      "[ 0.47895018  0.48287175  0.49071489 ...,  0.49855802  0.50247959\n",
      "  0.50640116]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.49463645  0.49463645\n",
      "  0.50247959]\n",
      "[ 0.48287175  0.48287175  0.47895018 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.47895018  0.48287175  0.48287175 ...,  0.49855802  0.50247959\n",
      "  0.49071489]\n",
      "[ 0.47617304  0.47617304  0.48009461 ...,  0.49745937  0.50138094\n",
      "  0.50530251]\n",
      "[ 0.47617304  0.48009461  0.47617304 ...,  0.50138094  0.49745937\n",
      "  0.4935378 ]\n",
      "[ 0.48401617  0.48401617  0.48793774 ...,  0.4935378   0.49745937\n",
      "  0.50138094]\n",
      "[ 0.48009461  0.47617304  0.47617304 ...,  0.49745937  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.48287175  0.48679332  0.48287175 ...,  0.49463645  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.49745937  0.49745937\n",
      "  0.4935378 ]\n",
      "[ 0.48287175  0.47895018  0.47502861 ...,  0.48679332  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.47895018  0.48287175  0.47895018 ...,  0.49855802  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.48287175  0.48287175  0.47895018 ...,  0.49855802  0.48679332\n",
      "  0.49463645]\n",
      "[ 0.47895018  0.47110704  0.48679332 ...,  0.49855802  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.47502861  0.48287175  0.48679332 ...,  0.49463645  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.49071489  0.48679332  0.48679332 ...,  0.50247959  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.49855802  0.49855802\n",
      "  0.49463645]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.49855802  0.50247959\n",
      "  0.49855802]\n",
      "[ 0.47502861  0.48679332  0.48679332 ...,  0.49071489  0.49071489\n",
      "  0.48679332]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.5142443   0.5142443   0.5142443 ]\n",
      "[ 0.47895018  0.48287175  0.48287175 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.47502861  0.47895018  0.48287175 ...,  0.51032273  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.47895018  0.47502861  0.47110704 ...,  0.49071489  0.49071489\n",
      "  0.49463645]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.48679332  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.49463645  0.49071489\n",
      "  0.48679332]\n",
      "[ 0.4730602   0.48090333  0.48874647 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.47110704  0.47502861  0.47895018 ...,  0.49463645  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.46913863  0.4730602   0.47698177 ...,  0.49071489  0.49071489\n",
      "  0.49463645]\n",
      "[ 0.48090333  0.47698177  0.47698177 ...,  0.48679332  0.49071489\n",
      "  0.49463645]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49855802  0.49855802\n",
      "  0.49071489]\n",
      "[ 0.48874647  0.48090333  0.48090333 ...,  0.49463645  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.49071489  0.49071489  0.50247959 ...,  0.51032273  0.51032273\n",
      "  0.50640116]\n",
      "[ 0.48679332  0.48287175  0.49463645 ...,  0.51032273  0.5142443   0.5142443 ]\n",
      "[ 0.49463645  0.49463645  0.49071489 ...,  0.5142443   0.51032273\n",
      "  0.51032273]\n",
      "[ 0.48679332  0.48679332  0.48287175 ...,  0.51816587  0.5142443\n",
      "  0.51032273]\n",
      "[ 0.48679332  0.48679332  0.48679332 ...,  0.50640116  0.51816587\n",
      "  0.51816587]\n",
      "[ 0.47110704  0.47502861  0.47895018 ...,  0.49071489  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.47446403  0.48230716  0.48622873 ...,  0.49071489  0.48679332\n",
      "  0.49071489]\n",
      "[ 0.48679332  0.48679332  0.49071489 ...,  0.49855802  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.47446403  0.48230716  0.4901503  ...,  0.49855802  0.49855802\n",
      "  0.49463645]\n",
      "[ 0.48622873  0.4783856   0.47446403 ...,  0.49463645  0.49463645\n",
      "  0.49071489]\n",
      "[ 0.44814221  0.43245594  0.44814221 ...,  0.45990692  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.47167163  0.47951476\n",
      "  0.47167163]\n",
      "[ 0.44029908  0.44814221  0.45206378 ...,  0.44029908  0.44029908\n",
      "  0.44029908]\n",
      "[ 0.44814221  0.44422065  0.44422065 ...,  0.45598535  0.45598535\n",
      "  0.44814221]\n",
      "[ 0.44029908  0.44029908  0.43637751 ...,  0.4246128   0.44029908\n",
      "  0.45598535]\n",
      "[ 0.45206378  0.44814221  0.44422065 ...,  0.44814221  0.44029908\n",
      "  0.44029908]\n",
      "[ 0.43245594  0.42069123  0.44814221 ...,  0.44029908  0.44422065\n",
      "  0.44814221]\n",
      "[ 0.47028305  0.45067521  0.43891051 ...,  0.46243992  0.46243992\n",
      "  0.46636149]\n",
      "[ 0.45067521  0.45067521  0.44283207 ...,  0.46636149  0.46243992\n",
      "  0.46243992]\n",
      "[ 0.44283207  0.43891051  0.44283207 ...,  0.46243992  0.46243992\n",
      "  0.46636149]\n",
      "[ 0.46243992  0.47420462  0.47028305 ...,  0.46243992  0.45851835\n",
      "  0.46243992]\n",
      "[ 0.44814221  0.44422065  0.43637751 ...,  0.47167163  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.44675364  0.44675364  0.45067521 ...,  0.47420462  0.47420462\n",
      "  0.47420462]\n",
      "[ 0.45598535  0.45598535  0.45990692 ...,  0.47812619  0.48204776\n",
      "  0.48204776]\n",
      "[ 0.45990692  0.46382849  0.46382849 ...,  0.47812619  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.45206378  0.45206378  0.45598535 ...,  0.47028305  0.46636149\n",
      "  0.46636149]\n",
      "[ 0.46775006  0.45990692  0.45990692 ...,  0.48204776  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.45990692  0.45206378  0.44814221 ...,  0.48596933  0.48596933\n",
      "  0.47812619]\n",
      "[ 0.43189136  0.43189136  0.43581292 ...,  0.42012665  0.43973449\n",
      "  0.43973449]\n",
      "[ 0.42404822  0.42796979  0.42404822 ...,  0.44757763  0.4514992\n",
      "  0.44757763]\n",
      "[ 0.42796979  0.43189136  0.43973449 ...,  0.42404822  0.43189136\n",
      "  0.42796979]\n",
      "[ 0.44365606  0.44365606  0.44757763 ...,  0.43189136  0.42404822\n",
      "  0.43189136]\n",
      "[ 0.44814221  0.44029908  0.44422065 ...,  0.46775006  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.43973449  0.42012665  0.43189136 ...,  0.43189136  0.43581292\n",
      "  0.43581292]\n",
      "[ 0.44814221  0.45206378  0.45206378 ...,  0.47392996  0.47392996\n",
      "  0.47785153]\n",
      "[ 0.45598535  0.45990692  0.45990692 ...,  0.47000839  0.47000839\n",
      "  0.47392996]\n",
      "[ 0.45206378  0.45206378  0.45990692 ...,  0.46608682  0.46608682\n",
      "  0.46608682]\n",
      "[ 0.45598535  0.45206378  0.45206378 ...,  0.48569467  0.48569467\n",
      "  0.47000839]\n",
      "[ 0.45206378  0.44814221  0.44422065 ...,  0.47951476  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.44814221  0.43637751  0.44814221 ...,  0.47028305  0.47028305\n",
      "  0.47028305]\n",
      "[ 0.45990692  0.45990692  0.45990692 ...,  0.47028305  0.47420462\n",
      "  0.47812619]\n",
      "[ 0.45990692  0.45990692  0.45598535 ...,  0.47392996  0.47392996\n",
      "  0.47392996]\n",
      "[ 0.48343633  0.47951476  0.47559319 ...,  0.47420462  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.44422065  0.44422065  0.44814221 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.46382849  0.45990692  0.45990692 ...,  0.47420462  0.46243992\n",
      "  0.46243992]\n",
      "[ 0.43973449  0.44365606  0.43973449 ...,  0.44757763  0.4514992\n",
      "  0.45542077]\n",
      "[ 0.44365606  0.44365606  0.44365606 ...,  0.43973449  0.43973449\n",
      "  0.44365606]\n",
      "[ 0.43973449  0.44365606  0.43581292 ...,  0.45542077  0.4514992\n",
      "  0.44757763]\n",
      "[ 0.44365606  0.44365606  0.44365606 ...,  0.45542077  0.45542077\n",
      "  0.4514992 ]\n",
      "[ 0.44757763  0.44757763  0.43973449 ...,  0.4632639   0.43581292\n",
      "  0.4514992 ]\n",
      "[ 0.44422065  0.43245594  0.43245594 ...,  0.46382849  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.4514992   0.45934234  0.45934234 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.45934234  0.4514992   0.4514992  ...,  0.47559319  0.47559319\n",
      "  0.46775006]\n",
      "[ 0.45934234  0.45542077  0.4514992  ...,  0.47167163  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.47559319  0.47559319\n",
      "  0.47167163]\n",
      "[ 0.52576486  0.52576486  0.52576486 ...,  0.52744335  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.533608    0.53752956  0.533608  ]\n",
      "[ 0.50223545  0.49831388  0.49831388 ...,  0.49716945  0.49716945\n",
      "  0.49324788]\n",
      "[ 0.50501259  0.50893416  0.51285573 ...,  0.49716945  0.49716945\n",
      "  0.49716945]\n",
      "[ 0.50223545  0.49831388  0.49831388 ...,  0.50109102  0.50501259\n",
      "  0.50501259]\n",
      "[ 0.50615702  0.50615702  0.51007858 ...,  0.50109102  0.50893416\n",
      "  0.50893416]\n",
      "[ 0.49831388  0.49831388  0.50223545 ...,  0.50109102  0.50109102\n",
      "  0.50109102]\n",
      "[ 0.5254902   0.5254902   0.53333333 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.5254902   0.5254902   0.53333333 ...,  0.53920806  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.52941176  0.53333333  0.52941176 ...,  0.54705119  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.52941176  0.52941176  0.52941176 ...,  0.53920806  0.54312963\n",
      "  0.54705119]\n",
      "[ 0.51792172  0.52184329  0.52576486 ...,  0.52744335  0.51175708\n",
      "  0.51567864]\n",
      "[ 0.5254902   0.5254902   0.52941176 ...,  0.53333333  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.53752956  0.54145113  0.5453727  ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.5453727   0.5453727   0.54145113 ...,  0.53920806  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.54312963  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.53752956  0.533608    0.53752956 ...,  0.53136492  0.53136492\n",
      "  0.54312963]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.49439231  0.49439231  0.50223545 ...,  0.48540475  0.48932631\n",
      "  0.48932631]\n",
      "[ 0.50615702  0.50615702  0.51007858 ...,  0.49324788  0.49716945\n",
      "  0.49716945]\n",
      "[ 0.49831388  0.49439231  0.49831388 ...,  0.50109102  0.50501259\n",
      "  0.50501259]\n",
      "[ 0.50615702  0.50615702  0.50223545 ...,  0.50109102  0.50109102\n",
      "  0.50109102]\n",
      "[ 0.52968643  0.52576486  0.52576486 ...,  0.53528649  0.53528649\n",
      "  0.54312963]\n",
      "[ 0.47870603  0.49831388  0.49047074 ...,  0.49324788  0.49716945\n",
      "  0.49324788]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.53136492  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52576486  0.52968643  0.533608   ...,  0.53136492  0.52744335\n",
      "  0.53528649]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.54705119  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.533608    0.53528649  0.52744335 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.52576486  0.52968643  0.53752956 ...,  0.54312963  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.52352178  0.52744335  0.52744335 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.533608    0.53752956  0.54145113 ...,  0.53920806  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52744335  0.52744335  0.53136492 ...,  0.55097276  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.52744335  0.53920806  0.54312963 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53752956  0.52968643  0.52968643 ...,  0.52352178  0.51960021\n",
      "  0.52352178]\n",
      "[ 0.53920806  0.53528649  0.53528649 ...,  0.53528649  0.53136492\n",
      "  0.54312963]\n",
      "[ 0.51400015  0.51400015  0.52184329 ...,  0.50109102  0.50501259\n",
      "  0.50501259]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.51285573  0.51285573\n",
      "  0.50893416]\n",
      "[ 0.51400015  0.51792172  0.51792172 ...,  0.51285573  0.51677729\n",
      "  0.51677729]\n",
      "[ 0.51400015  0.51400015  0.51400015 ...,  0.51677729  0.50893416\n",
      "  0.50501259]\n",
      "[ 0.51792172  0.51792172  0.51400015 ...,  0.50501259  0.50109102\n",
      "  0.50109102]\n",
      "[ 0.52184329  0.52184329  0.52576486 ...,  0.52352178  0.52352178\n",
      "  0.51960021]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.52968643  0.52968643  0.533608  ]\n",
      "[ 0.52576486  0.52576486  0.52968643 ...,  0.533608    0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52968643  0.52184329  0.51792172 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52968643  0.52968643  0.52576486 ...,  0.52576486  0.52968643  0.533608  ]\n",
      "[ 0.4898909   0.4898909   0.4898909  ...,  0.48091859  0.48484016\n",
      "  0.48484016]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.49773404  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.45206378  0.45206378  0.45598535 ...,  0.46523232  0.46523232\n",
      "  0.46523232]\n",
      "[ 0.46775006  0.46775006  0.46775006 ...,  0.46523232  0.46131075\n",
      "  0.45738918]\n",
      "[ 0.47167163  0.46382849  0.46382849 ...,  0.47307546  0.46915389\n",
      "  0.47307546]\n",
      "[ 0.45206378  0.45990692  0.45990692 ...,  0.47699702  0.48876173\n",
      "  0.47699702]\n",
      "[ 0.46382849  0.46382849  0.46382849 ...,  0.45738918  0.46131075\n",
      "  0.46915389]\n",
      "[ 0.4898909   0.4898909   0.49381247 ...,  0.48654917  0.48654917\n",
      "  0.48654917]\n",
      "[ 0.49381247  0.49381247  0.49773404 ...,  0.49439231  0.49831388\n",
      "  0.50223545]\n",
      "[ 0.49381247  0.49381247  0.4898909  ...,  0.5013962   0.50531777\n",
      "  0.50531777]\n",
      "[ 0.49381247  0.4898909   0.49381247 ...,  0.50223545  0.50223545\n",
      "  0.50223545]\n",
      "[ 0.47420462  0.47420462  0.47812619 ...,  0.49381247  0.4898909   0.4898909 ]\n",
      "[ 0.48204776  0.48204776  0.47812619 ...,  0.49439231  0.49439231\n",
      "  0.47870603]\n",
      "[ 0.48596933  0.48596933  0.4898909  ...,  0.5016556   0.49773404\n",
      "  0.49773404]\n",
      "[ 0.4898909   0.4898909   0.48596933 ...,  0.51342031  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.49773404  0.49381247  0.49381247 ...,  0.51342031  0.51734188\n",
      "  0.51007858]\n",
      "[ 0.49381247  0.49381247  0.49773404 ...,  0.49831388  0.49831388\n",
      "  0.49831388]\n",
      "[ 0.48204776  0.48204776  0.4898909  ...,  0.50557717  0.5016556   0.5016556 ]\n",
      "[ 0.45206378  0.45206378  0.45598535 ...,  0.46382849  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.46382849  0.45990692  0.45990692 ...,  0.44422065  0.45206378\n",
      "  0.45598535]\n",
      "[ 0.45990692  0.46382849  0.45990692 ...,  0.45990692  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.45598535  0.45990692  0.46382849 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.48204776  0.4898909   0.4898909  ...,  0.47699702  0.4926833   0.4926833 ]\n",
      "[ 0.46382849  0.45598535  0.45206378 ...,  0.46382849  0.45990692\n",
      "  0.46775006]\n",
      "[ 0.48596933  0.49381247  0.49381247 ...,  0.48596933  0.48596933\n",
      "  0.48596933]\n",
      "[ 0.4898909   0.49381247  0.4898909  ...,  0.48596933  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.49381247  0.49381247  0.4898909  ...,  0.49381247  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.48596933  0.48596933  0.48204776 ...,  0.48596933  0.48596933\n",
      "  0.49381247]\n",
      "[ 0.48596933  0.48596933  0.4898909  ...,  0.49381247  0.49773404\n",
      "  0.5016556 ]\n",
      "[ 0.51342031  0.5016556   0.49773404 ...,  0.50557717  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.48204776  0.49381247  0.49381247 ...,  0.5016556   0.49381247\n",
      "  0.49381247]\n",
      "[ 0.48596933  0.49381247  0.4898909  ...,  0.5016556   0.5016556\n",
      "  0.50557717]\n",
      "[ 0.4898909   0.4898909   0.48596933 ...,  0.50557717  0.50557717\n",
      "  0.50949874]\n",
      "[ 0.49773404  0.49381247  0.49381247 ...,  0.48204776  0.48596933\n",
      "  0.5016556 ]\n",
      "[ 0.49773404  0.4898909   0.48596933 ...,  0.50557717  0.50557717\n",
      "  0.50949874]\n",
      "[ 0.47559319  0.47559319  0.47951476 ...,  0.48343633  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.47167163  0.47167163  0.47559319 ...,  0.48343633  0.48343633\n",
      "  0.47951476]\n",
      "[ 0.47167163  0.47167163  0.46775006 ...,  0.47559319  0.47559319\n",
      "  0.47951476]\n",
      "[ 0.46775006  0.47167163  0.47559319 ...,  0.46775006  0.47167163\n",
      "  0.47559319]\n",
      "[ 0.47559319  0.47951476  0.47559319 ...,  0.46382849  0.46382849\n",
      "  0.46382849]\n",
      "[ 0.48961624  0.48961624  0.48569467 ...,  0.48484016  0.4926833   0.4926833 ]\n",
      "[ 0.4817731   0.50138094  0.49745937 ...,  0.5016556   0.49773404\n",
      "  0.49381247]\n",
      "[ 0.48961624  0.49745937  0.4935378  ...,  0.49773404  0.49381247\n",
      "  0.49381247]\n",
      "[ 0.48569467  0.4935378   0.50138094 ...,  0.49381247  0.49381247\n",
      "  0.49773404]\n",
      "[ 0.4817731   0.48961624  0.4935378  ...,  0.49773404  0.49773404\n",
      "  0.49381247]\n",
      "[ 0.53498131  0.53498131  0.53136492 ...,  0.53528649  0.53528649\n",
      "  0.53920806]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52352178  0.52352178  0.52352178 ...,  0.49716945  0.49716945\n",
      "  0.50109102]\n",
      "[ 0.51567864  0.51960021  0.51960021 ...,  0.49716945  0.49324788\n",
      "  0.50501259]\n",
      "[ 0.52352178  0.52744335  0.52352178 ...,  0.51285573  0.51677729\n",
      "  0.51285573]\n",
      "[ 0.51960021  0.52352178  0.53136492 ...,  0.50893416  0.51285573\n",
      "  0.51677729]\n",
      "[ 0.52352178  0.52744335  0.52744335 ...,  0.50501259  0.50501259\n",
      "  0.50109102]\n",
      "[ 0.56273747  0.5588159   0.55489433 ...,  0.54312963  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.53920806  0.53920806  0.53528649 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.54312963  0.54312963  0.54705119 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.53920806  0.53920806  0.54312963 ...,  0.56665904  0.55489433\n",
      "  0.5588159 ]\n",
      "[ 0.54087129  0.54087129  0.53528649 ...,  0.54705119  0.54705119\n",
      "  0.53528649]\n",
      "[ 0.54312963  0.53920806  0.53920806 ...,  0.53528649  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.55097276  0.54705119  0.54312963 ...,  0.55489433  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.54705119  0.53920806  0.53528649 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54312963  0.54312963 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.54705119  0.54705119\n",
      "  0.53920806]\n",
      "[ 0.51400015  0.51400015  0.51792172 ...,  0.51007858  0.51007858\n",
      "  0.51007858]\n",
      "[ 0.51792172  0.52184329  0.52184329 ...,  0.50893416  0.50893416\n",
      "  0.50501259]\n",
      "[ 0.51007858  0.50615702  0.50615702 ...,  0.50223545  0.50223545\n",
      "  0.49831388]\n",
      "[ 0.51007858  0.51400015  0.51792172 ...,  0.50109102  0.50501259\n",
      "  0.50893416]\n",
      "[ 0.54312963  0.53920806  0.54312963 ...,  0.53528649  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.51400015  0.51007858  0.51400015 ...,  0.49831388  0.49831388\n",
      "  0.49831388]\n",
      "[ 0.54705119  0.55097276  0.54705119 ...,  0.54312963  0.54705119\n",
      "  0.55097276]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54312963  0.54705119 ...,  0.53528649  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54705119  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53528649  0.52744335  0.52744335 ...,  0.55097276  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53528649  0.53136492  0.53528649 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.54312963  0.55097276  0.53920806 ...,  0.5588159   0.56273747\n",
      "  0.56273747]\n",
      "[ 0.53136492  0.53528649  0.53920806 ...,  0.55489433  0.5588159   0.5588159 ]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.53920806  0.54312963\n",
      "  0.53528649]\n",
      "[ 0.53528649  0.54312963  0.55097276 ...,  0.54312963  0.54312963\n",
      "  0.54705119]\n",
      "[ 0.51792172  0.51792172  0.52184329 ...,  0.51677729  0.51677729\n",
      "  0.51285573]\n",
      "[ 0.51792172  0.51792172  0.51007858 ...,  0.51285573  0.51285573\n",
      "  0.50893416]\n",
      "[ 0.52968643  0.52576486  0.52576486 ...,  0.50109102  0.49716945\n",
      "  0.49716945]\n",
      "[ 0.51792172  0.51792172  0.52184329 ...,  0.51285573  0.50501259\n",
      "  0.51285573]\n",
      "[ 0.52184329  0.52576486  0.52576486 ...,  0.48932631  0.48932631\n",
      "  0.50501259]\n",
      "[ 0.53105974  0.53105974  0.53498131 ...,  0.53920806  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.5453727   0.5453727   0.55321584 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.5453727   0.54145113  0.5453727  ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.5453727   0.55713741  0.54929427 ...,  0.54145113  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53752956  0.53752956  0.54145113 ...,  0.54145113  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.49773404  0.49773404  0.5016556  ...,  0.49831388  0.50615702\n",
      "  0.51400015]\n",
      "[ 0.50557717  0.50557717  0.50949874 ...,  0.50223545  0.51007858\n",
      "  0.51400015]\n",
      "[ 0.4898909   0.49773404  0.48596933 ...,  0.49716945  0.49324788\n",
      "  0.48540475]\n",
      "[ 0.48596933  0.48596933  0.4898909  ...,  0.49324788  0.49324788\n",
      "  0.48932631]\n",
      "[ 0.49381247  0.49773404  0.48343633 ...,  0.48932631  0.48540475\n",
      "  0.48540475]\n",
      "[ 0.4898909   0.49381247  0.49381247 ...,  0.48540475  0.48540475\n",
      "  0.48540475]\n",
      "[ 0.47812619  0.48204776  0.48596933 ...,  0.48932631  0.48540475\n",
      "  0.48148318]\n",
      "[ 0.51708248  0.51316091  0.51316091 ...,  0.52156863  0.52156863\n",
      "  0.52156863]\n",
      "[ 0.50923934  0.50923934  0.50531777 ...,  0.52156863  0.52156863\n",
      "  0.51372549]\n",
      "[ 0.51708248  0.51316091  0.51316091 ...,  0.51764706  0.52156863\n",
      "  0.52156863]\n",
      "[ 0.51708248  0.51316091  0.50923934 ...,  0.5254902   0.5254902\n",
      "  0.52941176]\n",
      "[ 0.50949874  0.5016556   0.50557717 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.50923934  0.50923934  0.50923934 ...,  0.52100404  0.52100404\n",
      "  0.52100404]\n",
      "[ 0.51400015  0.51792172  0.51792172 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50223545  0.50615702  0.51007858 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.51007858  0.51400015  0.51400015 ...,  0.52576486  0.51792172\n",
      "  0.51400015]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52968643  0.533608    0.52968643]\n",
      "[ 0.51792172  0.51792172  0.51400015 ...,  0.51400015  0.51007858\n",
      "  0.51007858]\n",
      "[ 0.48204776  0.48204776  0.48204776 ...,  0.48596933  0.48596933\n",
      "  0.48204776]\n",
      "[ 0.47420462  0.47420462  0.47420462 ...,  0.4898909   0.48596933\n",
      "  0.47812619]\n",
      "[ 0.48204776  0.48204776  0.48204776 ...,  0.47420462  0.47420462\n",
      "  0.47812619]\n",
      "[ 0.48596933  0.48596933  0.48596933 ...,  0.48204776  0.48596933\n",
      "  0.48596933]\n",
      "[ 0.5016556   0.49773404  0.5016556  ...,  0.52184329  0.50615702\n",
      "  0.52576486]\n",
      "[ 0.46636149  0.48596933  0.50557717 ...,  0.47812619  0.48204776\n",
      "  0.48596933]\n",
      "[ 0.50949874  0.50949874  0.51342031 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.50557717  0.5016556   0.50557717 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.52126345  0.52518502  0.52910658 ...,  0.52576486  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.50557717  0.50557717  0.50949874 ...,  0.51792172  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.50557717  0.50949874  0.50949874 ...,  0.51792172  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.51400015  0.51792172  0.51400015 ...,  0.52184329  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.51792172  0.52184329  0.50615702 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.50615702  0.51007858  0.51400015 ...,  0.52576486  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.50949874  0.50949874  0.50557717 ...,  0.51007858  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.52184329  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.48091859  0.48484016  0.48484016 ...,  0.49660487  0.50052644\n",
      "  0.50052644]\n",
      "[ 0.49773404  0.5016556   0.50557717 ...,  0.48876173  0.49660487\n",
      "  0.49660487]\n",
      "[ 0.49660487  0.50052644  0.49660487 ...,  0.50836957  0.504448    0.50836957]\n",
      "[ 0.49660487  0.49660487  0.48204776 ...,  0.48484016  0.48091859\n",
      "  0.47699702]\n",
      "[ 0.4926833   0.4926833   0.4926833  ...,  0.48484016  0.48876173\n",
      "  0.4926833 ]\n",
      "[ 0.50949874  0.50557717  0.5016556  ...,  0.51400015  0.51792172\n",
      "  0.51400015]\n",
      "[ 0.50949874  0.50557717  0.50557717 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52126345  0.52126345  0.51342031 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.51734188  0.51342031  0.51342031 ...,  0.51792172  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.51400015  0.51400015\n",
      "  0.51792172]\n",
      "[ 0.44393072  0.44393072  0.44000916 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.45177386  0.44785229  0.44393072 ...,  0.48679332  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.43608759  0.42824445  0.41647974 ...,  0.48287175  0.47110704\n",
      "  0.4632639 ]\n",
      "[ 0.44000916  0.43216602  0.43216602 ...,  0.45934234  0.45542077\n",
      "  0.45934234]\n",
      "[ 0.42824445  0.43216602  0.43608759 ...,  0.48679332  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.42432288  0.42040131  0.42432288 ...,  0.45934234  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.44000916  0.43608759  0.43216602 ...,  0.4632639   0.46718547\n",
      "  0.46718547]\n",
      "[ 0.44365606  0.43973449  0.43581292 ...,  0.47392996  0.47392996\n",
      "  0.47785153]\n",
      "[ 0.45542077  0.45934234  0.45934234 ...,  0.47785153  0.4817731   0.4817731 ]\n",
      "[ 0.43973449  0.44365606  0.43973449 ...,  0.47000839  0.47392996\n",
      "  0.47392996]\n",
      "[ 0.44365606  0.4514992   0.4514992  ...,  0.47000839  0.47000839\n",
      "  0.47000839]\n",
      "[ 0.44393072  0.44393072  0.45569543 ...,  0.47110704  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.45934234  0.45542077  0.45542077 ...,  0.47785153  0.47000839\n",
      "  0.47000839]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44785229  0.44785229  0.45177386 ...,  0.48287175  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.43216602  0.44000916  0.44785229 ...,  0.47110704  0.4632639\n",
      "  0.46718547]\n",
      "[ 0.44785229  0.45177386  0.45177386 ...,  0.47110704  0.4632639   0.4632639 ]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.46718547  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.44000916  0.42824445  0.43608759 ...,  0.46269932  0.46269932\n",
      "  0.46269932]\n",
      "[ 0.43216602  0.43216602  0.42432288 ...,  0.46269932  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.44000916  0.44000916  0.44000916 ...,  0.47054246  0.47054246\n",
      "  0.47054246]\n",
      "[ 0.44393072  0.44000916  0.44000916 ...,  0.47054246  0.47446403\n",
      "  0.47446403]\n",
      "[ 0.44000916  0.43608759  0.44000916 ...,  0.4632639   0.4632639\n",
      "  0.46718547]\n",
      "[ 0.42824445  0.42432288  0.42824445 ...,  0.46269932  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.43608759  0.42824445  0.43216602 ...,  0.48287175  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.43916991  0.43524834  0.44560922 ...,  0.48287175  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.44000916  0.44000916  0.44000916 ...,  0.47698177  0.50443275\n",
      "  0.51619745]\n",
      "[ 0.43608759  0.44000916  0.44785229 ...,  0.47502861  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.44785229  0.44393072  0.44000916 ...,  0.48679332  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.44168765  0.44953079  0.44560922 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44168765  0.44168765  0.43776608 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.45569543  0.45569543  0.44785229 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.46129549  0.45345235  0.45345235 ...,  0.4632639   0.4632639\n",
      "  0.46718547]\n",
      "[ 0.43608759  0.43216602  0.43216602 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.45345235  0.44560922  0.44560922 ...,  0.47502861  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.42824445  0.42824445  0.43608759 ...,  0.4783856   0.47446403\n",
      "  0.47054246]\n",
      "[ 0.44393072  0.44393072  0.43216602 ...,  0.4783856   0.48230716\n",
      "  0.48622873]\n",
      "[ 0.44000916  0.44393072  0.44000916 ...,  0.4783856   0.4783856   0.4783856 ]\n",
      "[ 0.44393072  0.44000916  0.43608759 ...,  0.48230716  0.48230716\n",
      "  0.48230716]\n",
      "[ 0.43216602  0.43216602  0.43216602 ...,  0.46662089  0.47054246\n",
      "  0.46662089]\n",
      "[ 0.45177386  0.45569543  0.45569543 ...,  0.47110704  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.44785229  0.44785229  0.45569543 ...,  0.47502861  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.46353857  0.459617    0.459617   ...,  0.47502861  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.44393072  0.44785229  0.45177386 ...,  0.47502861  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.459617    0.45177386  0.44785229 ...,  0.47502861  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.49463645  0.49463645\n",
      "  0.49071489]\n",
      "[ 0.49658961  0.49266804  0.49266804 ...,  0.51032273  0.50640116\n",
      "  0.50640116]\n",
      "[ 0.50696574  0.50304417  0.50304417 ...,  0.50640116  0.51032273\n",
      "  0.5142443 ]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.51088731  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.49520104  0.49520104  0.49912261 ...,  0.50640116  0.5142443\n",
      "  0.52208743]\n",
      "[ 0.49520104  0.49912261  0.49912261 ...,  0.50640116  0.50640116\n",
      "  0.51032273]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.51480888  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.49745937  0.49745937  0.49745937 ...,  0.50530251  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.4935378   0.4935378   0.4935378  ...,  0.50922408  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.49745937  0.49745937  0.49745937 ...,  0.51146715  0.51146715\n",
      "  0.49970245]\n",
      "[ 0.49745937  0.50138094  0.50138094 ...,  0.50754559  0.50754559\n",
      "  0.50754559]\n",
      "[ 0.49071489  0.49463645  0.49855802 ...,  0.49855802  0.50247959\n",
      "  0.50640116]\n",
      "[ 0.49745937  0.49745937  0.49745937 ...,  0.50530251  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.49463645  0.49463645  0.49463645 ...,  0.50247959  0.50247959\n",
      "  0.50640116]\n",
      "[ 0.49463645  0.49855802  0.49463645 ...,  0.49071489  0.49071489\n",
      "  0.50640116]\n",
      "[ 0.48679332  0.49071489  0.49463645 ...,  0.49463645  0.49855802\n",
      "  0.49463645]\n",
      "[ 0.50247959  0.49855802  0.49071489 ...,  0.49855802  0.49463645\n",
      "  0.49071489]\n",
      "[ 0.49071489  0.49463645  0.49855802 ...,  0.49855802  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.49912261  0.49912261  0.49520104 ...,  0.49463645  0.50247959\n",
      "  0.50640116]\n",
      "[ 0.49520104  0.50304417  0.50304417 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.51088731  0.50696574  0.50304417 ...,  0.51032273  0.52208743\n",
      "  0.51816587]\n",
      "[ 0.49912261  0.4873579   0.49520104 ...,  0.52208743  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.50247959  0.50640116\n",
      "  0.51032273]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.5142443   0.5142443   0.5142443 ]\n",
      "[ 0.50640116  0.50640116  0.49855802 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.50247959  0.49855802  0.49463645 ...,  0.51032273  0.51032273\n",
      "  0.5142443 ]\n",
      "[ 0.49463645  0.49463645  0.50247959 ...,  0.49855802  0.50640116\n",
      "  0.50640116]\n",
      "[ 0.49855802  0.50247959  0.50640116 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.50247959  0.50247959  0.50247959 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.50247959  0.49463645  0.49463645 ...,  0.50247959  0.49855802\n",
      "  0.50247959]\n",
      "[ 0.50640116  0.49855802  0.49463645 ...,  0.49463645  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.50640116  0.50640116\n",
      "  0.50640116]\n",
      "[ 0.49463645  0.49463645  0.49463645 ...,  0.49855802  0.49855802\n",
      "  0.50247959]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.49463645  0.49855802\n",
      "  0.49463645]\n",
      "[ 0.49855802  0.49855802  0.49463645 ...,  0.49463645  0.49463645\n",
      "  0.50247959]\n",
      "[ 0.50640116  0.50640116  0.50640116 ...,  0.52208743  0.52208743\n",
      "  0.51816587]\n",
      "[ 0.53385214  0.52993057  0.52208743 ...,  0.52208743  0.52208743\n",
      "  0.51816587]\n",
      "[ 0.49855802  0.50247959  0.51032273 ...,  0.52208743  0.52208743  0.526009  ]\n",
      "[ 0.50247959  0.50640116  0.51816587 ...,  0.52208743  0.52208743  0.526009  ]\n",
      "[ 0.51816587  0.51032273  0.5142443  ...,  0.51816587  0.51816587\n",
      "  0.51816587]\n",
      "[ 0.49463645  0.49855802  0.49855802 ...,  0.50640116  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.49658961  0.50443275  0.50443275 ...,  0.49855802  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.49658961  0.49266804  0.49658961 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.49658961  0.48874647  0.49266804 ...,  0.50583658  0.50583658\n",
      "  0.50975814]\n",
      "[ 0.50051118  0.50051118  0.48874647 ...,  0.50583658  0.50975814\n",
      "  0.50583658]\n",
      "[ 0.44422065  0.44814221  0.45206378 ...,  0.4873579   0.47951476\n",
      "  0.47559319]\n",
      "[ 0.44757763  0.4514992   0.45542077 ...,  0.47951476  0.4817731   0.4817731 ]\n",
      "[ 0.44422065  0.44814221  0.45206378 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.44814221  0.44814221  0.44814221 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.44422065  0.45206378  0.44814221 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.45206378  0.45598535  0.46382849 ...,  0.46775006  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.44422065  0.44422065  0.44814221 ...,  0.47559319  0.47951476\n",
      "  0.47951476]\n",
      "[ 0.44647898  0.45040055  0.45432212 ...,  0.48204776  0.48596933\n",
      "  0.48596933]\n",
      "[ 0.44255741  0.44255741  0.44647898 ...,  0.47812619  0.47812619\n",
      "  0.48204776]\n",
      "[ 0.45040055  0.45824369  0.46608682 ...,  0.47812619  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.46608682  0.46608682  0.46216526 ...,  0.47812619  0.48204776\n",
      "  0.47812619]\n",
      "[ 0.44422065  0.45206378  0.45990692 ...,  0.46775006  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.45432212  0.45432212  0.45432212 ...,  0.47028305  0.47028305\n",
      "  0.47420462]\n",
      "[ 0.45598535  0.45598535  0.45990692 ...,  0.4817731   0.48569467\n",
      "  0.48961624]\n",
      "[ 0.45990692  0.46382849  0.46382849 ...,  0.48569467  0.48569467\n",
      "  0.4817731 ]\n",
      "[ 0.46382849  0.45598535  0.45598535 ...,  0.48569467  0.48569467\n",
      "  0.48961624]\n",
      "[ 0.46775006  0.46775006  0.46382849 ...,  0.48596933  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.44422065  0.44814221  0.44814221 ...,  0.47785153  0.47785153\n",
      "  0.47392996]\n",
      "[ 0.44365606  0.44365606  0.44365606 ...,  0.45990692  0.46382849\n",
      "  0.46382849]\n",
      "[ 0.43973449  0.43973449  0.43973449 ...,  0.46775006  0.45990692\n",
      "  0.45206378]\n",
      "[ 0.44029908  0.43637751  0.44029908 ...,  0.46382849  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.44757763  0.44757763  0.44365606 ...,  0.46382849  0.45598535\n",
      "  0.46382849]\n",
      "[ 0.44422065  0.44814221  0.45206378 ...,  0.47167163  0.47559319\n",
      "  0.48343633]\n",
      "[ 0.44757763  0.44365606  0.44365606 ...,  0.46382849  0.46382849\n",
      "  0.46382849]\n",
      "[ 0.45598535  0.45598535  0.45598535 ...,  0.47167163  0.47167163\n",
      "  0.47559319]\n",
      "[ 0.45990692  0.45990692  0.45990692 ...,  0.47392996  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.44029908  0.44814221  0.44814221 ...,  0.48343633  0.47951476\n",
      "  0.47559319]\n",
      "[ 0.45206378  0.45206378  0.45206378 ...,  0.47951476  0.4873579\n",
      "  0.47559319]\n",
      "[ 0.45598535  0.45598535  0.45206378 ...,  0.46775006  0.47167163\n",
      "  0.47559319]\n",
      "[ 0.47167163  0.47559319  0.46382849 ...,  0.4817731   0.4817731\n",
      "  0.48569467]\n",
      "[ 0.45206378  0.44029908  0.45598535 ...,  0.4935378   0.48569467\n",
      "  0.4817731 ]\n",
      "[ 0.45990692  0.45990692  0.45990692 ...,  0.47785153  0.50138094\n",
      "  0.51706722]\n",
      "[ 0.46382849  0.46382849  0.46382849 ...,  0.48569467  0.4817731   0.4817731 ]\n",
      "[ 0.44814221  0.44814221  0.45206378 ...,  0.47559319  0.47559319\n",
      "  0.47951476]\n",
      "[ 0.45990692  0.45990692  0.45598535 ...,  0.47785153  0.48961624\n",
      "  0.48569467]\n",
      "[ 0.45206378  0.45206378  0.44814221 ...,  0.46775006  0.47167163\n",
      "  0.47559319]\n",
      "[ 0.45990692  0.45990692  0.45990692 ...,  0.46775006  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.45598535  0.45598535  0.45206378 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.45598535  0.45598535  0.45598535 ...,  0.45206378  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.45598535  0.45598535  0.45598535 ...,  0.47167163  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.44422065  0.44029908  0.45598535 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.4514992   0.44365606  0.45542077 ...,  0.47000839  0.47000839\n",
      "  0.47785153]\n",
      "[ 0.44757763  0.44365606  0.44757763 ...,  0.4817731   0.47785153\n",
      "  0.47785153]\n",
      "[ 0.4632639   0.46718547  0.4632639  ...,  0.4817731   0.4817731\n",
      "  0.47785153]\n",
      "[ 0.4514992   0.4514992   0.45542077 ...,  0.4817731   0.4817731   0.4817731 ]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.53136492  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.55599298  0.55207141\n",
      "  0.54422827]\n",
      "[ 0.51567864  0.52352178  0.52744335 ...,  0.51567864  0.51567864\n",
      "  0.51567864]\n",
      "[ 0.51567864  0.51960021  0.51960021 ...,  0.51960021  0.51567864\n",
      "  0.51960021]\n",
      "[ 0.52352178  0.52352178  0.52352178 ...,  0.52352178  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53136492  0.53136492  0.53528649 ...,  0.51960021  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.52352178  0.51960021  0.51567864 ...,  0.51175708  0.51175708\n",
      "  0.51567864]\n",
      "[ 0.54117647  0.54117647  0.5372549  ...,  0.55097276  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53333333  0.53333333  0.53333333 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.52941176  0.52941176  0.52941176 ...,  0.53528649  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.52941176  0.53333333  0.52941176 ...,  0.55489433  0.55097276\n",
      "  0.53920806]\n",
      "[ 0.52968643  0.533608    0.52576486 ...,  0.55489433  0.53920806\n",
      "  0.51960021]\n",
      "[ 0.5254902   0.5254902   0.5254902  ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.52576486  0.52576486  0.52576486 ...,  0.54312963  0.53528649\n",
      "  0.52744335]\n",
      "[ 0.533608    0.52968643  0.52968643 ...,  0.53920806  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.53920806  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.533608    0.533608    0.53752956 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.53528649  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.50615702  0.50615702  0.51400015 ...,  0.51792172  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.51792172  0.51792172  0.51792172 ...,  0.52184329  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.52576486  0.52184329  0.51792172 ...,  0.51792172  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.51400015  0.51007858  0.52184329 ...,  0.51792172  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.533608    0.52968643  0.52576486 ...,  0.56273747  0.55097276\n",
      "  0.54312963]\n",
      "[ 0.51007858  0.51400015  0.52184329 ...,  0.51400015  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.53920806  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.52576486  0.52576486  0.533608   ...,  0.53528649  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52184329  0.52184329  0.52576486 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.54705119  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.54312963  0.54312963\n",
      "  0.54705119]\n",
      "[ 0.52576486  0.52576486  0.52968643 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.52576486  0.52576486  0.52968643 ...,  0.53920806  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52576486  0.52576486  0.533608   ...,  0.53528649  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.53752956  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.55321584  0.5453727\n",
      "  0.54145113]\n",
      "[ 0.52576486  0.51792172  0.51792172 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.54145113  0.53752956  0.52184329 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.533608    0.52968643  0.52968643 ...,  0.53638514  0.54030671\n",
      "  0.53638514]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.54030671  0.53638514\n",
      "  0.54030671]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.53638514  0.53638514\n",
      "  0.53638514]\n",
      "[ 0.47785153  0.4817731   0.4898909  ...,  0.5016556   0.50557717\n",
      "  0.5016556 ]\n",
      "[ 0.4935378   0.48961624  0.48569467 ...,  0.49773404  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.48484016  0.47699702  0.46915389 ...,  0.49381247  0.4898909\n",
      "  0.48204776]\n",
      "[ 0.47699702  0.47699702  0.47699702 ...,  0.5016556   0.5016556   0.5016556 ]\n",
      "[ 0.48091859  0.48091859  0.48091859 ...,  0.4898909   0.48596933\n",
      "  0.48204776]\n",
      "[ 0.47307546  0.46915389  0.46915389 ...,  0.47812619  0.48654917\n",
      "  0.48654917]\n",
      "[ 0.47699702  0.47699702  0.47699702 ...,  0.49773404  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.4898909   0.4898909   0.49773404 ...,  0.49324788  0.49716945\n",
      "  0.50109102]\n",
      "[ 0.48204776  0.4898909   0.487953   ...,  0.50615702  0.51007858\n",
      "  0.51007858]\n",
      "[ 0.48204776  0.48204776  0.49381247 ...,  0.50223545  0.50615702\n",
      "  0.50615702]\n",
      "[ 0.49773404  0.49381247  0.4898909  ...,  0.49324788  0.49716945\n",
      "  0.50501259]\n",
      "[ 0.48204776  0.47812619  0.47812619 ...,  0.5016556   0.5016556   0.5016556 ]\n",
      "[ 0.48596933  0.48596933  0.48204776 ...,  0.51007858  0.50615702\n",
      "  0.50223545]\n",
      "[ 0.4898909   0.4898909   0.4898909  ...,  0.49773404  0.49773404\n",
      "  0.5016556 ]\n",
      "[ 0.49381247  0.49381247  0.49381247 ...,  0.50949874  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.48596933  0.4898909   0.4898909  ...,  0.5016556   0.5016556\n",
      "  0.50949874]\n",
      "[ 0.49773404  0.49773404  0.49773404 ...,  0.50557717  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.48204776  0.48204776  0.48596933 ...,  0.52126345  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.46775006  0.47167163  0.47000839 ...,  0.48204776  0.47812619\n",
      "  0.48204776]\n",
      "[ 0.47392996  0.46608682  0.46608682 ...,  0.47812619  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.46382849  0.46382849  0.46216526 ...,  0.48204776  0.47028305\n",
      "  0.47028305]\n",
      "[ 0.46608682  0.46608682  0.47000839 ...,  0.47420462  0.47812619\n",
      "  0.48204776]\n",
      "[ 0.4817731   0.4817731   0.48204776 ...,  0.5016556   0.4898909\n",
      "  0.49381247]\n",
      "[ 0.47559319  0.47951476  0.47951476 ...,  0.4898909   0.48204776\n",
      "  0.47812619]\n",
      "[ 0.49773404  0.49381247  0.4898909  ...,  0.5016556   0.50557717\n",
      "  0.50557717]\n",
      "[ 0.48596933  0.48204776  0.48596933 ...,  0.49773404  0.49381247\n",
      "  0.49773404]\n",
      "[ 0.48596933  0.48596933  0.48596933 ...,  0.49773404  0.5016556   0.5016556 ]\n",
      "[ 0.4898909   0.49381247  0.48204776 ...,  0.49773404  0.5016556   0.5016556 ]\n",
      "[ 0.48204776  0.48204776  0.48204776 ...,  0.5016556   0.49773404\n",
      "  0.49773404]\n",
      "[ 0.48204776  0.47812619  0.48204776 ...,  0.51007858  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.48204776  0.47812619  0.48204776 ...,  0.51007858  0.51007858\n",
      "  0.51007858]\n",
      "[ 0.49381247  0.4898909   0.49381247 ...,  0.50949874  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4898909   0.4898909   0.4898909  ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.47812619  0.47420462  0.47420462 ...,  0.49381247  0.4898909\n",
      "  0.49381247]\n",
      "[ 0.48596933  0.48596933  0.49381247 ...,  0.51007858  0.51007858\n",
      "  0.51400015]\n",
      "[ 0.48343633  0.49127947  0.48091859 ...,  0.4898909   0.4898909\n",
      "  0.48596933]\n",
      "[ 0.47392996  0.47392996  0.47000839 ...,  0.49381247  0.4898909\n",
      "  0.48596933]\n",
      "[ 0.47812619  0.47420462  0.47812619 ...,  0.49381247  0.49381247\n",
      "  0.49381247]\n",
      "[ 0.4898909   0.4898909   0.48596933 ...,  0.48596933  0.48204776\n",
      "  0.48204776]\n",
      "[ 0.47951476  0.47951476  0.47951476 ...,  0.48204776  0.48204776\n",
      "  0.48204776]\n",
      "[ 0.48569467  0.48961624  0.48961624 ...,  0.49773404  0.49773404\n",
      "  0.5016556 ]\n",
      "[ 0.48569467  0.48569467  0.48961624 ...,  0.50557717  0.50557717\n",
      "  0.5016556 ]\n",
      "[ 0.48961624  0.4935378   0.49745937 ...,  0.5016556   0.49381247\n",
      "  0.49381247]\n",
      "[ 0.4935378   0.4935378   0.48961624 ...,  0.49773404  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.48961624  0.48569467  0.48569467 ...,  0.50557717  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.54312963  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.5453727   0.5453727   0.5453727  ...,  0.55321584  0.55321584\n",
      "  0.55321584]\n",
      "[ 0.52744335  0.53136492  0.53920806 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.52744335  0.52744335  0.53136492 ...,  0.52744335  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.51175708  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.53920806  0.52744335  0.52352178 ...,  0.53528649  0.53528649\n",
      "  0.53920806]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.52352178  0.52352178\n",
      "  0.52744335]\n",
      "[ 0.54901961  0.54901961  0.54901961 ...,  0.54312963  0.54312963\n",
      "  0.55097276]\n",
      "[ 0.54117647  0.5372549   0.5372549  ...,  0.55097276  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.54117647  0.54117647  0.54509804 ...,  0.5588159   0.55489433\n",
      "  0.55097276]\n",
      "[ 0.55294118  0.55686275  0.55686275 ...,  0.54312963  0.53528649\n",
      "  0.52352178]\n",
      "[ 0.53136492  0.53136492  0.53920806 ...,  0.55097276  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.54509804  0.54509804  0.54117647 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.54312963  0.54312963  0.53920806 ...,  0.54705119  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.54312963  0.54312963 ...,  0.53528649  0.53136492\n",
      "  0.53920806]\n",
      "[ 0.53528649  0.53528649  0.53920806 ...,  0.54705119  0.54312963\n",
      "  0.55097276]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.55489433  0.55097276\n",
      "  0.54705119]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.54705119  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.53136492  0.53136492  0.53528649 ...,  0.51960021  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.53136492  0.53136492  0.52744335 ...,  0.53136492  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.52744335  0.52744335  0.52352178 ...,  0.53528649  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.52744335  0.52744335  0.52352178 ...,  0.52352178  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.54312963  0.54312963\n",
      "  0.55097276]\n",
      "[ 0.53528649  0.53920806  0.53920806 ...,  0.51175708  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.53528649  0.53528649\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.55097276  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54312963  0.55097276 ...,  0.54312963  0.54705119\n",
      "  0.55097276]\n",
      "[ 0.54705119  0.54705119  0.55097276 ...,  0.55097276  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.54312963  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53528649  0.53920806  0.53920806 ...,  0.55097276  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53920806  0.55097276  0.54705119 ...,  0.54705119  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54705119  0.54705119 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.54312963  0.54312963 ...,  0.54705119  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.53528649  0.53920806  0.54312963 ...,  0.55489433  0.54705119\n",
      "  0.53528649]\n",
      "[ 0.53136492  0.53528649  0.53920806 ...,  0.54705119  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53136492  0.53528649  0.53920806 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53920806  0.53528649  0.53136492 ...,  0.53528649  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.51960021  0.52352178\n",
      "  0.52744335]\n",
      "[ 0.53920806  0.53920806  0.54312963 ...,  0.52352178  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.54312963  0.53920806  0.53920806 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54145113  0.54145113  0.53752956 ...,  0.54145113  0.5453727   0.5453727 ]\n",
      "[ 0.54145113  0.5453727   0.5453727  ...,  0.55321584  0.55321584\n",
      "  0.55713741]\n",
      "[ 0.55713741  0.56498054  0.55713741 ...,  0.54929427  0.54929427\n",
      "  0.55713741]\n",
      "[ 0.55321584  0.54929427  0.53752956 ...,  0.55321584  0.55713741\n",
      "  0.55713741]\n",
      "[ 0.50557717  0.50557717  0.5016556  ...,  0.51792172  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.51342031  0.50949874  0.50949874 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.50223545  0.49831388  0.49831388 ...,  0.50615702  0.50223545\n",
      "  0.50223545]\n",
      "[ 0.50615702  0.50615702  0.49831388 ...,  0.50223545  0.50223545\n",
      "  0.50615702]\n",
      "[ 0.49831388  0.50223545  0.49831388 ...,  0.51400015  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.51400015  0.51007858  0.50223545 ...,  0.50615702  0.50615702\n",
      "  0.50615702]\n",
      "[ 0.49831388  0.50223545  0.50615702 ...,  0.51007858  0.51400015\n",
      "  0.51007858]\n",
      "[ 0.50531777  0.50531777  0.50923934 ...,  0.52100404  0.51708248\n",
      "  0.51708248]\n",
      "[ 0.51708248  0.52100404  0.52100404 ...,  0.52100404  0.52492561\n",
      "  0.52100404]\n",
      "[ 0.51316091  0.52100404  0.51316091 ...,  0.52884718  0.52492561\n",
      "  0.52100404]\n",
      "[ 0.50923934  0.50923934  0.51316091 ...,  0.52492561  0.52492561\n",
      "  0.52492561]\n",
      "[ 0.5016556   0.50557717  0.50949874 ...,  0.51400015  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.50923934  0.50923934  0.50923934 ...,  0.52492561  0.52492561\n",
      "  0.52492561]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.50557717  0.50949874  0.50949874 ...,  0.52968643  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.50949874  0.51342031  0.51734188 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.51734188  0.51342031  0.51342031 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.51342031  0.51342031  0.51342031 ...,  0.533608    0.52968643\n",
      "  0.52576486]\n",
      "[ 0.49773404  0.49773404  0.49381247 ...,  0.50223545  0.50223545\n",
      "  0.50615702]\n",
      "[ 0.50557717  0.50557717  0.5016556  ...,  0.48654917  0.49047074\n",
      "  0.49439231]\n",
      "[ 0.4898909   0.49381247  0.4898909  ...,  0.51007858  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.48596933  0.4898909   0.49773404 ...,  0.49831388  0.49439231\n",
      "  0.49439231]\n",
      "[ 0.5016556   0.50949874  0.50949874 ...,  0.52576486  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.5016556   0.50949874  0.50557717 ...,  0.48654917  0.48654917\n",
      "  0.49439231]\n",
      "[ 0.50557717  0.50949874  0.50949874 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.49773404  0.49773404  0.49381247 ...,  0.51792172  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.50557717  0.50949874  0.50949874 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.52576486  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.52126345  0.51342031  0.52126345 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50223545  0.51400015  0.51792172 ...,  0.52576486  0.52576486  0.533608  ]\n",
      "[ 0.51007858  0.51792172  0.52184329 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.51400015  0.51400015  0.51400015 ...,  0.52184329  0.51400015\n",
      "  0.52576486]\n",
      "[ 0.51792172  0.51400015  0.50223545 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.5016556   0.5016556   0.5016556  ...,  0.51007858  0.50223545\n",
      "  0.50223545]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.51007858  0.50615702  0.50223545 ...,  0.50615702  0.50615702\n",
      "  0.51007858]\n",
      "[ 0.51007858  0.50615702  0.49831388 ...,  0.50615702  0.51007858\n",
      "  0.51792172]\n",
      "[ 0.49773404  0.5016556   0.50949874 ...,  0.51007858  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.51400015  0.51007858  0.50615702 ...,  0.50223545  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.51734188  0.50557717  0.49773404 ...,  0.49439231  0.49439231\n",
      "  0.49439231]\n",
      "[ 0.50557717  0.50557717  0.5016556  ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51342031  0.51342031  0.51342031 ...,  0.52126345  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.52910658  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52126345  0.51342031  0.50557717 ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.49660487  0.49660487  0.49660487 ...,  0.51229114  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.50304417  0.49127947  0.49127947 ...,  0.51706722  0.51706722\n",
      "  0.51706722]\n",
      "[ 0.52013428  0.52013428  0.51621271 ...,  0.52405585  0.52405585\n",
      "  0.52797742]\n",
      "[ 0.51621271  0.52013428  0.52405585 ...,  0.52013428  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.52405585  0.52013428  0.51621271 ...,  0.52013428  0.52405585\n",
      "  0.52797742]\n",
      "[ 0.51621271  0.52013428  0.52013428 ...,  0.52405585  0.51621271\n",
      "  0.52405585]\n",
      "[ 0.504448    0.504448    0.504448   ...,  0.52797742  0.52797742\n",
      "  0.52405585]\n",
      "[ 0.50557717  0.50949874  0.51734188 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49381247  0.5016556   0.5016556  ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.4898909   0.4898909   0.4898909  ...,  0.50557717  0.50949874\n",
      "  0.50949874]\n",
      "[ 0.5016556   0.50949874  0.50949874 ...,  0.52518502  0.52126345\n",
      "  0.51734188]\n",
      "[ 0.4926833   0.50052644  0.50052644 ...,  0.52013428  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.50949874  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.50052644  0.49660487  0.4926833  ...,  0.51342031  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.50052644  0.49660487  0.4926833  ...,  0.50949874  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.49912261  0.50304417  0.50304417 ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49520104  0.49912261  0.50304417 ...,  0.50557717  0.50557717\n",
      "  0.50949874]\n",
      "[ 0.50836957  0.50052644  0.50052644 ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.52013428  0.51229114  0.51621271 ...,  0.51734188  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.52265202  0.52265202  0.53049516 ...,  0.52910658  0.53302815\n",
      "  0.53694972]\n",
      "[ 0.50836957  0.51229114  0.51621271 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52405585  0.52405585  0.52013428 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50052644  0.504448    0.504448   ...,  0.51480888  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.51229114  0.51229114  0.50836957 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.49773404  0.49773404  0.5016556  ...,  0.50949874  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.5016556   0.5016556   0.50557717 ...,  0.51734188  0.52126345\n",
      "  0.51734188]\n",
      "[ 0.5016556   0.50557717  0.50557717 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.5016556   0.50557717  0.50557717 ...,  0.51342031  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.50949874  0.50949874  0.5016556  ...,  0.52126345  0.52126345\n",
      "  0.53302815]\n",
      "[ 0.50557717  0.50557717  0.50949874 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.5016556   0.50557717  0.50557717 ...,  0.51734188  0.52518502\n",
      "  0.52910658]\n",
      "[ 0.50557717  0.5016556   0.49773404 ...,  0.50949874  0.50557717\n",
      "  0.50949874]\n",
      "[ 0.49773404  0.49381247  0.4898909  ...,  0.51342031  0.51734188\n",
      "  0.52910658]\n",
      "[ 0.48876173  0.48876173  0.48876173 ...,  0.50836957  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.50557717  0.4898909   0.49773404 ...,  0.52910658  0.52518502\n",
      "  0.51734188]\n",
      "[ 0.52797742  0.52797742  0.52405585 ...,  0.54366369  0.53582055\n",
      "  0.53189899]\n",
      "[ 0.52797742  0.52797742  0.52797742 ...,  0.53582055  0.53582055\n",
      "  0.53582055]\n",
      "[ 0.51229114  0.52013428  0.52405585 ...,  0.52405585  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.51229114  0.50836957  0.50836957 ...,  0.53189899  0.53189899\n",
      "  0.53189899]\n",
      "[ 0.53189899  0.53582055  0.52797742 ...,  0.52797742  0.52797742\n",
      "  0.52405585]\n",
      "[ 0.50052644  0.50052644  0.504448   ...,  0.51621271  0.51621271\n",
      "  0.52013428]\n",
      "[ 0.50304417  0.49912261  0.50304417 ...,  0.52491035  0.52491035\n",
      "  0.52098878]\n",
      "[ 0.49912261  0.49520104  0.49912261 ...,  0.51706722  0.51706722\n",
      "  0.51706722]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.52265202  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.51480888  0.51088731  0.50696574 ...,  0.51706722  0.51706722\n",
      "  0.51706722]\n",
      "[ 0.48314641  0.48314641  0.47922484 ...,  0.49491112  0.49491112\n",
      "  0.48706798]\n",
      "[ 0.49434653  0.49434653  0.49434653 ...,  0.48258183  0.48258183\n",
      "  0.48258183]\n",
      "[ 0.49883268  0.49491112  0.49098955 ...,  0.49883268  0.49883268\n",
      "  0.49491112]\n",
      "[ 0.48706798  0.48706798  0.48706798 ...,  0.49098955  0.50667582\n",
      "  0.50275425]\n",
      "[ 0.49491112  0.49098955  0.49491112 ...,  0.49098955  0.49098955\n",
      "  0.49883268]\n",
      "[ 0.49098955  0.49491112  0.49098955 ...,  0.49491112  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.49098955  0.49491112  0.50275425 ...,  0.49883268  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.47054246  0.47446403  0.48230716 ...,  0.48622873  0.48230716\n",
      "  0.48622873]\n",
      "[ 0.4783856   0.4901503   0.48622873 ...,  0.4901503   0.4901503   0.4901503 ]\n",
      "[ 0.48622873  0.48622873  0.48230716 ...,  0.48230716  0.4783856   0.4901503 ]\n",
      "[ 0.49407187  0.49407187  0.49407187 ...,  0.49407187  0.49407187\n",
      "  0.4901503 ]\n",
      "[ 0.47922484  0.47922484  0.48314641 ...,  0.49098955  0.48706798\n",
      "  0.48314641]\n",
      "[ 0.48622873  0.48622873  0.4901503  ...,  0.4901503   0.48622873\n",
      "  0.48622873]\n",
      "[ 0.48314641  0.47922484  0.47922484 ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.47922484  0.48314641  0.48314641 ...,  0.49434653  0.49434653\n",
      "  0.47866026]\n",
      "[ 0.4713817   0.47530327  0.4713817  ...,  0.4865034   0.49042496\n",
      "  0.4865034 ]\n",
      "[ 0.48314641  0.48314641  0.47922484 ...,  0.47473869  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.48706798  0.48314641  0.47922484 ...,  0.48258183  0.48258183\n",
      "  0.48258183]\n",
      "[ 0.49407187  0.4901503   0.48622873 ...,  0.49883268  0.50275425\n",
      "  0.50667582]\n",
      "[ 0.49407187  0.4901503   0.4901503  ...,  0.49491112  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.4901503   0.49799344  0.49799344 ...,  0.49491112  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.48230716  0.48230716  0.48230716 ...,  0.49491112  0.49491112\n",
      "  0.49491112]\n",
      "[ 0.47922484  0.47922484  0.48314641 ...,  0.49098955  0.48706798\n",
      "  0.47922484]\n",
      "[ 0.50191501  0.50191501  0.49407187 ...,  0.50275425  0.50275425\n",
      "  0.49491112]\n",
      "[ 0.48314641  0.48706798  0.49491112 ...,  0.49098955  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.48314641  0.47922484  0.47922484 ...,  0.48706798  0.48314641\n",
      "  0.47922484]\n",
      "[ 0.48706798  0.48706798  0.48706798 ...,  0.48706798  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.48706798  0.49491112  0.49098955 ...,  0.48314641  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.50275425  0.50275425  0.49883268 ...,  0.49098955  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.47922484  0.49098955  0.49098955 ...,  0.48706798  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.48314641  0.48706798  0.48706798 ...,  0.48314641  0.48314641\n",
      "  0.48706798]\n",
      "[ 0.48314641  0.48314641  0.48314641 ...,  0.47922484  0.47530327\n",
      "  0.49491112]\n",
      "[ 0.49883268  0.49098955  0.48706798 ...,  0.48314641  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.47922484  0.47922484  0.48314641 ...,  0.48706798  0.49883268\n",
      "  0.49491112]\n",
      "[ 0.48706798  0.48706798  0.47922484 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.4901503   0.49799344  0.50191501 ...,  0.49883268  0.50275425\n",
      "  0.50275425]\n",
      "[ 0.4901503   0.49407187  0.49407187 ...,  0.50275425  0.50275425\n",
      "  0.50275425]\n",
      "[ 0.49799344  0.49407187  0.49407187 ...,  0.49883268  0.49883268\n",
      "  0.50275425]\n",
      "[ 0.49799344  0.49799344  0.49799344 ...,  0.49883268  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.4783856   0.4901503   0.50191501 ...,  0.50275425  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.49491112  0.49098955  0.48706798 ...,  0.48706798  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.48258183  0.48258183  0.48258183 ...,  0.4865034   0.47473869\n",
      "  0.48258183]\n",
      "[ 0.47866026  0.47866026  0.48258183 ...,  0.4865034   0.4865034\n",
      "  0.48258183]\n",
      "[ 0.47473869  0.47866026  0.47081712 ...,  0.48258183  0.47866026\n",
      "  0.48258183]\n",
      "[ 0.48258183  0.48258183  0.4865034  ...,  0.4865034   0.49042496\n",
      "  0.49434653]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.52265202  0.51873045\n",
      "  0.51088731]\n",
      "[ 0.51088731  0.50304417  0.51088731 ...,  0.51088731  0.51088731\n",
      "  0.52265202]\n",
      "[ 0.504448    0.504448    0.50836957 ...,  0.52405585  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.51621271  0.51621271  0.50836957 ...,  0.52405585  0.52013428\n",
      "  0.51621271]\n",
      "[ 0.50052644  0.504448    0.50836957 ...,  0.51229114  0.52657359\n",
      "  0.51873045]\n",
      "[ 0.4926833   0.49660487  0.49660487 ...,  0.52013428  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.50052644  0.49660487  0.50052644 ...,  0.52797742  0.52405585\n",
      "  0.52797742]\n",
      "[ 0.50949874  0.50557717  0.50949874 ...,  0.5016556   0.49773404\n",
      "  0.50557717]\n",
      "[ 0.49773404  0.50557717  0.50949874 ...,  0.51734188  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.50949874  0.50949874  0.51342031 ...,  0.5016556   0.50949874\n",
      "  0.50557717]\n",
      "[ 0.5016556   0.5016556   0.5016556  ...,  0.50557717  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.50304417  0.50304417  0.50696574 ...,  0.51480888  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.50557717  0.50949874  0.50557717 ...,  0.49773404  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.50304417  0.49912261  0.49520104 ...,  0.51088731  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.49912261  0.49520104  0.49912261 ...,  0.50304417  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.49127947  0.49127947  0.49127947 ...,  0.49912261  0.49912261\n",
      "  0.50696574]\n",
      "[ 0.49912261  0.49912261  0.49127947 ...,  0.50696574  0.50696574\n",
      "  0.51088731]\n",
      "[ 0.50696574  0.49912261  0.49912261 ...,  0.51088731  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.50530251  0.50922408  0.50922408 ...,  0.52657359  0.52265202\n",
      "  0.52657359]\n",
      "[ 0.50530251  0.50530251  0.50138094 ...,  0.52657359  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.51873045  0.51480888  0.51480888 ...,  0.52265202  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.53441672  0.53049516\n",
      "  0.52657359]\n",
      "[ 0.49912261  0.49520104  0.49912261 ...,  0.49520104  0.49912261\n",
      "  0.50696574]\n",
      "[ 0.50922408  0.51314565  0.50922408 ...,  0.52657359  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.50530251  0.50530251\n",
      "  0.50138094]\n",
      "[ 0.51088731  0.50696574  0.50696574 ...,  0.50922408  0.50138094\n",
      "  0.49745937]\n",
      "[ 0.50304417  0.50696574  0.50696574 ...,  0.51706722  0.51314565\n",
      "  0.51314565]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.48961624  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.49127947  0.50304417  0.51088731 ...,  0.50922408  0.50922408\n",
      "  0.51314565]\n",
      "[ 0.50696574  0.50696574  0.50304417 ...,  0.51088731  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.51088731  0.50696574  0.50304417 ...,  0.50696574  0.51088731\n",
      "  0.51480888]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.51480888  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.49520104  0.50304417  0.50304417 ...,  0.50696574  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.51088731  0.51873045  0.51088731 ...,  0.50304417  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.50304417  0.50696574  0.49912261 ...,  0.51088731  0.50696574\n",
      "  0.51088731]\n",
      "[ 0.52265202  0.51873045  0.51873045 ...,  0.550103    0.54225986\n",
      "  0.53441672]\n",
      "[ 0.51873045  0.51873045  0.52265202 ...,  0.52883192  0.53049516\n",
      "  0.53049516]\n",
      "[ 0.52657359  0.52265202  0.51480888 ...,  0.52657359  0.53049516\n",
      "  0.53049516]\n",
      "[ 0.52265202  0.52657359  0.52657359 ...,  0.53441672  0.53049516\n",
      "  0.52657359]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.54225986  0.54225986\n",
      "  0.54225986]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.51873045  0.52657359\n",
      "  0.51480888]\n",
      "[ 0.50304417  0.49520104  0.49520104 ...,  0.51480888  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.51088731  0.51873045\n",
      "  0.49127947]\n",
      "[ 0.49520104  0.50304417  0.50304417 ...,  0.51032273  0.51032273\n",
      "  0.5142443 ]\n",
      "[ 0.50696574  0.51088731  0.51088731 ...,  0.51088731  0.51088731\n",
      "  0.51088731]\n",
      "[ 0.46297398  0.46297398  0.47081712 ...,  0.45006485  0.44614328\n",
      "  0.44614328]\n",
      "[ 0.47219043  0.46826886  0.46042573 ...,  0.45650416  0.46042573\n",
      "  0.46042573]\n",
      "[ 0.48535897  0.4814374   0.47359426 ...,  0.44614328  0.45006485\n",
      "  0.45398642]\n",
      "[ 0.47359426  0.46967269  0.46967269 ...,  0.45790799  0.45790799\n",
      "  0.45398642]\n",
      "[ 0.47751583  0.47751583  0.47751583 ...,  0.45398642  0.45006485\n",
      "  0.45398642]\n",
      "[ 0.45790799  0.46182956  0.46575113 ...,  0.46575113  0.47359426\n",
      "  0.46967269]\n",
      "[ 0.4814374   0.4814374   0.4814374  ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.46746014  0.46746014  0.4713817  ...,  0.459617    0.45177386\n",
      "  0.45177386]\n",
      "[ 0.46746014  0.46353857  0.45569543 ...,  0.45569543  0.45569543\n",
      "  0.45569543]\n",
      "[ 0.45877775  0.46269932  0.46662089 ...,  0.45177386  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.45177386  0.45177386  0.45569543 ...,  0.459617    0.459617    0.45569543]\n",
      "[ 0.46689555  0.46689555  0.46689555 ...,  0.45006485  0.45006485\n",
      "  0.44614328]\n",
      "[ 0.47530327  0.4713817   0.46353857 ...,  0.46353857  0.45569543\n",
      "  0.45177386]\n",
      "[ 0.45790799  0.46575113  0.45790799 ...,  0.45398642  0.45398642\n",
      "  0.44614328]\n",
      "[ 0.45006485  0.45398642  0.46182956 ...,  0.45006485  0.45006485\n",
      "  0.46182956]\n",
      "[ 0.43830014  0.43830014  0.45006485 ...,  0.44614328  0.45790799\n",
      "  0.44614328]\n",
      "[ 0.46182956  0.45790799  0.45006485 ...,  0.45006485  0.45006485\n",
      "  0.45006485]\n",
      "[ 0.45006485  0.45398642  0.45398642 ...,  0.44614328  0.45398642\n",
      "  0.45398642]\n",
      "[ 0.47081712  0.47081712  0.46689555 ...,  0.46297398  0.46182956\n",
      "  0.46182956]\n",
      "[ 0.4865034   0.48258183  0.48258183 ...,  0.47081712  0.4865034\n",
      "  0.47081712]\n",
      "[ 0.47081712  0.46689555  0.47081712 ...,  0.45905241  0.45905241\n",
      "  0.45513085]\n",
      "[ 0.47081712  0.46689555  0.47081712 ...,  0.45790799  0.45006485\n",
      "  0.44614328]\n",
      "[ 0.46689555  0.46689555  0.46297398 ...,  0.45006485  0.45006485\n",
      "  0.45006485]\n",
      "[ 0.48258183  0.47866026  0.47866026 ...,  0.47473869  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.46689555  0.46689555  0.46297398 ...,  0.45905241  0.45905241\n",
      "  0.46689555]\n",
      "[ 0.46297398  0.46297398  0.46297398 ...,  0.45905241  0.45905241\n",
      "  0.45513085]\n",
      "[ 0.45905241  0.46297398  0.46689555 ...,  0.45513085  0.45513085\n",
      "  0.45513085]\n",
      "[ 0.46689555  0.46689555  0.46689555 ...,  0.46689555  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.46297398  0.46689555  0.47081712 ...,  0.47081712  0.46297398\n",
      "  0.45905241]\n",
      "[ 0.46297398  0.46297398  0.45905241 ...,  0.45006485  0.45398642\n",
      "  0.45790799]\n",
      "[ 0.45905241  0.44336614  0.45120928 ...,  0.44614328  0.45398642\n",
      "  0.45398642]\n",
      "[ 0.45905241  0.45905241  0.45513085 ...,  0.44614328  0.45006485\n",
      "  0.45398642]\n",
      "[ 0.45120928  0.44728771  0.44728771 ...,  0.45398642  0.45006485\n",
      "  0.45006485]\n",
      "[ 0.45120928  0.44728771  0.45120928 ...,  0.45006485  0.45398642\n",
      "  0.45398642]\n",
      "[ 0.46297398  0.46297398  0.46297398 ...,  0.45006485  0.45398642\n",
      "  0.45006485]\n",
      "[ 0.4865034   0.4865034   0.4865034  ...,  0.45484092  0.45484092\n",
      "  0.45876249]\n",
      "[ 0.48258183  0.48258183  0.47866026 ...,  0.47866026  0.47473869\n",
      "  0.47081712]\n",
      "[ 0.48258183  0.48258183  0.47866026 ...,  0.46660563  0.46660563\n",
      "  0.46660563]\n",
      "[ 0.46689555  0.46297398  0.46689555 ...,  0.4705272   0.4705272   0.4705272 ]\n",
      "[ 0.48258183  0.48258183  0.4865034  ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.46297398  0.46297398  0.45905241 ...,  0.44614328  0.45006485\n",
      "  0.45006485]\n",
      "[ 0.45650416  0.46042573  0.46042573 ...,  0.46042573  0.45650416\n",
      "  0.45258259]\n",
      "[ 0.46042573  0.46042573  0.46042573 ...,  0.45650416  0.45650416\n",
      "  0.45258259]\n",
      "[ 0.45258259  0.45258259  0.45258259 ...,  0.44866102  0.45650416\n",
      "  0.46042573]\n",
      "[ 0.46042573  0.4643473   0.4643473  ...,  0.44866102  0.44866102\n",
      "  0.44866102]\n",
      "[ 0.3968719   0.3968719   0.3968719  ...,  0.44393072  0.44000916\n",
      "  0.43608759]\n",
      "[ 0.41591516  0.41591516  0.41983673 ...,  0.44309148  0.44309148\n",
      "  0.44701305]\n",
      "[ 0.40357061  0.40357061  0.40357061 ...,  0.43216602  0.43216602\n",
      "  0.43608759]\n",
      "[ 0.39964904  0.40357061  0.40357061 ...,  0.44000916  0.43608759\n",
      "  0.43216602]\n",
      "[ 0.39572747  0.40357061  0.39964904 ...,  0.43608759  0.43608759\n",
      "  0.44000916]\n",
      "[ 0.39964904  0.39964904  0.39964904 ...,  0.44000916  0.44393072\n",
      "  0.44393072]\n",
      "[ 0.38788434  0.38788434  0.39180591 ...,  0.42824445  0.43608759\n",
      "  0.44000916]\n",
      "[ 0.40471504  0.3968719   0.40079347 ...,  0.43581292  0.43189136\n",
      "  0.43189136]\n",
      "[ 0.40387579  0.40387579  0.40387579 ...,  0.44757763  0.44365606\n",
      "  0.44365606]\n",
      "[ 0.40863661  0.40863661  0.40471504 ...,  0.43581292  0.43581292\n",
      "  0.43973449]\n",
      "[ 0.40779736  0.40779736  0.41171893 ...,  0.44365606  0.44365606\n",
      "  0.43973449]\n",
      "[ 0.3968719   0.39295033  0.39295033 ...,  0.43608759  0.43608759\n",
      "  0.43216602]\n",
      "[ 0.4156405   0.4156405   0.41171893 ...,  0.43973449  0.43973449\n",
      "  0.43973449]\n",
      "[ 0.41255818  0.40863661  0.40863661 ...,  0.45485618  0.45093462\n",
      "  0.45093462]\n",
      "[ 0.40863661  0.40863661  0.41255818 ...,  0.44701305  0.44701305\n",
      "  0.44701305]\n",
      "[ 0.40471504  0.40471504  0.40471504 ...,  0.43916991  0.44309148\n",
      "  0.44309148]\n",
      "[ 0.40079347  0.40079347  0.40863661 ...,  0.44701305  0.43132677\n",
      "  0.4274052 ]\n",
      "[ 0.41255818  0.41255818  0.41255818 ...,  0.43524834  0.43916991\n",
      "  0.44701305]\n",
      "[ 0.39124132  0.39124132  0.4069276  ...,  0.43524834  0.44309148\n",
      "  0.43916991]\n",
      "[ 0.39908446  0.39908446  0.41084916 ...,  0.42824445  0.42432288\n",
      "  0.43216602]\n",
      "[ 0.39908446  0.40300603  0.39908446 ...,  0.41956207  0.41171893\n",
      "  0.4274052 ]\n",
      "[ 0.39908446  0.39908446  0.39908446 ...,  0.43916991  0.43524834\n",
      "  0.43132677]\n",
      "[ 0.40079347  0.40079347  0.40471504 ...,  0.42824445  0.42824445\n",
      "  0.43216602]\n",
      "[ 0.38339818  0.38339818  0.38339818 ...,  0.43524834  0.43132677\n",
      "  0.43132677]\n",
      "[ 0.41255818  0.42040131  0.41647974 ...,  0.44309148  0.44701305\n",
      "  0.44701305]\n",
      "[ 0.40079347  0.40079347  0.40079347 ...,  0.43132677  0.43132677\n",
      "  0.43524834]\n",
      "[ 0.40079347  0.40079347  0.40079347 ...,  0.43524834  0.43524834\n",
      "  0.4274052 ]\n",
      "[ 0.40079347  0.40471504  0.40471504 ...,  0.44309148  0.44309148\n",
      "  0.44309148]\n",
      "[ 0.3968719   0.3968719   0.40471504 ...,  0.43132677  0.43524834\n",
      "  0.43916991]\n",
      "[ 0.41255818  0.40471504  0.40079347 ...,  0.43916991  0.44309148\n",
      "  0.44309148]\n",
      "[ 0.41255818  0.40471504  0.40079347 ...,  0.45093462  0.45093462\n",
      "  0.44701305]\n",
      "[ 0.40863661  0.40079347  0.3968719  ...,  0.43608759  0.44000916\n",
      "  0.44000916]\n",
      "[ 0.40079347  0.40471504  0.40863661 ...,  0.43916991  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.40079347  0.38902876  0.38510719 ...,  0.43216602  0.43216602\n",
      "  0.43216602]\n",
      "[ 0.40863661  0.41255818  0.40471504 ...,  0.43524834  0.43132677\n",
      "  0.43132677]\n",
      "[ 0.39630732  0.40022889  0.39630732 ...,  0.43132677  0.43524834\n",
      "  0.43132677]\n",
      "[ 0.38846418  0.39238575  0.40022889 ...,  0.44000916  0.43216602\n",
      "  0.43216602]\n",
      "[ 0.40022889  0.40022889  0.40022889 ...,  0.43524834  0.43216602\n",
      "  0.42040131]\n",
      "[ 0.39295033  0.3968719   0.40471504 ...,  0.43524834  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.39630732  0.39630732  0.39630732 ...,  0.43916991  0.43916991\n",
      "  0.43524834]\n",
      "[ 0.40079347  0.39295033  0.3968719  ...,  0.43216602  0.43216602\n",
      "  0.43216602]\n",
      "[ 0.41983673  0.41591516  0.41199359 ...,  0.44701305  0.44701305\n",
      "  0.44701305]\n",
      "[ 0.41983673  0.41983673  0.41983673 ...,  0.44701305  0.44309148\n",
      "  0.44701305]\n",
      "[ 0.40807202  0.40022889  0.39630732 ...,  0.44000916  0.44000916\n",
      "  0.44393072]\n",
      "[ 0.41591516  0.4237583   0.4237583  ...,  0.44785229  0.44785229\n",
      "  0.44785229]\n",
      "[ 0.53920806  0.54312963  0.55097276 ...,  0.533608    0.53752956  0.533608  ]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.53694972  0.53694972\n",
      "  0.54479286]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.52184329  0.52184329\n",
      "  0.51792172]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.51400015  0.51792172\n",
      "  0.52184329]\n",
      "[ 0.53302815  0.53694972  0.54087129 ...,  0.52069886  0.51285573\n",
      "  0.50893416]\n",
      "[ 0.54479286  0.54479286  0.54479286 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.53694972  0.54087129  0.54479286 ...,  0.52576486  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.53528649  0.53920806  0.54312963 ...,  0.54145113  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.54312963  0.53136492  0.53528649 ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.54312963  0.53920806  0.53528649 ...,  0.5453727   0.5453727\n",
      "  0.54929427]\n",
      "[ 0.53528649  0.53920806  0.54705119 ...,  0.55321584  0.56105898\n",
      "  0.55713741]\n",
      "[ 0.54312963  0.54312963  0.53920806 ...,  0.53920806  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.54929427  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.54145113  0.54145113  0.5453727  ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.54145113  0.54929427  0.55321584 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52576486  0.52968643  0.533608   ...,  0.5453727   0.5453727\n",
      "  0.55321584]\n",
      "[ 0.53920806  0.533608    0.533608   ...,  0.53752956  0.53752956  0.533608  ]\n",
      "[ 0.54871443  0.54871443  0.53302815 ...,  0.52184329  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.51792172  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.54087129  0.54479286  0.53694972 ...,  0.51792172  0.51792172\n",
      "  0.51400015]\n",
      "[ 0.53246357  0.53246357  0.53246357 ...,  0.50223545  0.50615702\n",
      "  0.51007858]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.53694972  0.53302815  0.52910658 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.55489433  0.55489433  0.55097276 ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.55097276  0.55097276  0.54705119 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.54705119  0.54705119  0.55489433 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.53528649  0.53920806  0.53920806 ...,  0.53752956  0.5453727   0.5453727 ]\n",
      "[ 0.54705119  0.55097276  0.55489433 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.54705119  0.54705119  0.54312963 ...,  0.54929427  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.55489433  0.55489433  0.55097276 ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.55489433  0.55489433  0.5588159  ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.53920806  0.53752956  0.54145113 ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.53752956  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.54145113  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.54087129  0.54479286  0.54871443 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.54087129  0.54087129  0.54087129 ...,  0.52184329  0.52184329\n",
      "  0.52576486]\n",
      "[ 0.54087129  0.54087129  0.54087129 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.54479286  0.54479286  0.54087129 ...,  0.52576486  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.53528649  0.53136492  0.53136492 ...,  0.533608    0.52968643\n",
      "  0.52968643]\n",
      "[ 0.54145113  0.5453727   0.5453727  ...,  0.54479286  0.54087129\n",
      "  0.54087129]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.54087129  0.54087129\n",
      "  0.54087129]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.54087129  0.53694972\n",
      "  0.53302815]\n",
      "[ 0.5453727   0.5453727   0.5453727  ...,  0.53302815  0.53302815\n",
      "  0.53302815]\n",
      "[ 0.45569543  0.45569543  0.459617   ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.46746014  0.4713817   0.46353857 ...,  0.47446403  0.47446403\n",
      "  0.4783856 ]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.46521706  0.46129549\n",
      "  0.46129549]\n",
      "[ 0.44393072  0.44785229  0.44785229 ...,  0.46521706  0.46521706\n",
      "  0.46129549]\n",
      "[ 0.44000916  0.44000916  0.43608759 ...,  0.4730602   0.46913863\n",
      "  0.46521706]\n",
      "[ 0.46353857  0.459617    0.44785229 ...,  0.4730602   0.46913863\n",
      "  0.46913863]\n",
      "[ 0.44000916  0.43608759  0.44000916 ...,  0.46129549  0.46129549\n",
      "  0.46129549]\n",
      "[ 0.45542077  0.4514992   0.4514992  ...,  0.47225147  0.47225147\n",
      "  0.47225147]\n",
      "[ 0.45934234  0.45542077  0.45542077 ...,  0.47617304  0.47617304\n",
      "  0.47617304]\n",
      "[ 0.4514992   0.4514992   0.4514992  ...,  0.48401617  0.48401617\n",
      "  0.48009461]\n",
      "[ 0.45934234  0.45542077  0.45542077 ...,  0.47617304  0.47617304\n",
      "  0.48009461]\n",
      "[ 0.45177386  0.45177386  0.45177386 ...,  0.45542077  0.45542077\n",
      "  0.46718547]\n",
      "[ 0.4514992   0.45542077  0.4514992  ...,  0.48009461  0.47617304\n",
      "  0.4683299 ]\n",
      "[ 0.44785229  0.45569543  0.459617   ...,  0.47110704  0.47895018\n",
      "  0.48679332]\n",
      "[ 0.459617    0.45569543  0.45177386 ...,  0.48287175  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.45177386  0.44393072  0.44785229 ...,  0.4730602   0.4730602\n",
      "  0.46913863]\n",
      "[ 0.46353857  0.46353857  0.46353857 ...,  0.4730602   0.4730602\n",
      "  0.47698177]\n",
      "[ 0.45177386  0.45569543  0.45569543 ...,  0.47895018  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.45177386  0.45177386  0.44785229 ...,  0.47054246  0.4713817   0.4713817 ]\n",
      "[ 0.45569543  0.44785229  0.44785229 ...,  0.46269932  0.46269932\n",
      "  0.45877775]\n",
      "[ 0.44393072  0.44000916  0.44393072 ...,  0.46662089  0.4713817\n",
      "  0.47530327]\n",
      "[ 0.44000916  0.44000916  0.44393072 ...,  0.46746014  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.44000916  0.44000916  0.44393072 ...,  0.47110704  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44785229  0.44785229  0.44393072 ...,  0.46662089  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.459617    0.459617    0.459617   ...,  0.46718547  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.46353857  0.46353857  0.46353857 ...,  0.47502861  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.4713817   0.44785229  0.43608759 ...,  0.48287175  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.459617    0.45569543  0.44785229 ...,  0.48287175  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.4632639   0.45934234\n",
      "  0.44757763]\n",
      "[ 0.44953079  0.44560922  0.44560922 ...,  0.48287175  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.459617    0.45569543  0.45177386 ...,  0.4730602   0.47698177\n",
      "  0.4730602 ]\n",
      "[ 0.45569543  0.45569543  0.45177386 ...,  0.47502861  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.45569543  0.45569543  0.45177386 ...,  0.46718547  0.4632639   0.4632639 ]\n",
      "[ 0.44785229  0.45177386  0.45569543 ...,  0.47110704  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.45177386  0.44785229  0.45177386 ...,  0.47446403  0.47054246\n",
      "  0.47446403]\n",
      "[ 0.44785229  0.45177386  0.45177386 ...,  0.48622873  0.4783856\n",
      "  0.47446403]\n",
      "[ 0.43216602  0.44000916  0.44393072 ...,  0.47446403  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.44393072  0.44393072  0.45177386 ...,  0.48622873  0.49407187\n",
      "  0.4901503 ]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.46662089  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.47110704  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.459617    0.46353857  0.46746014 ...,  0.47054246  0.4783856\n",
      "  0.48230716]\n",
      "[ 0.45569543  0.46746014  0.4713817  ...,  0.48230716  0.4901503   0.4901503 ]\n",
      "[ 0.45569543  0.459617    0.46746014 ...,  0.48230716  0.48622873\n",
      "  0.48622873]\n",
      "[ 0.459617    0.459617    0.459617   ...,  0.47054246  0.47446403\n",
      "  0.4783856 ]\n",
      "[ 0.43045701  0.42261387  0.43045701 ...,  0.43830014  0.43830014\n",
      "  0.43437858]\n",
      "[ 0.44473945  0.44081788  0.43689631 ...,  0.44866102  0.44866102\n",
      "  0.45258259]\n",
      "[ 0.41477073  0.4186923   0.41477073 ...,  0.41477073  0.42261387\n",
      "  0.42261387]\n",
      "[ 0.42261387  0.42653544  0.43045701 ...,  0.43437858  0.43045701\n",
      "  0.43045701]\n",
      "[ 0.4069276   0.4069276   0.4069276  ...,  0.4186923   0.4186923\n",
      "  0.42261387]\n",
      "[ 0.41477073  0.41477073  0.41084916 ...,  0.44222171  0.43830014\n",
      "  0.42653544]\n",
      "[ 0.41084916  0.41084916  0.4069276  ...,  0.43437858  0.43045701\n",
      "  0.42653544]\n",
      "[ 0.44728771  0.44728771  0.435523   ...,  0.44336614  0.44728771\n",
      "  0.45120928]\n",
      "[ 0.43944457  0.43944457  0.435523   ...,  0.44336614  0.43944457  0.435523  ]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.44728771  0.44728771\n",
      "  0.44728771]\n",
      "[ 0.43160143  0.43944457  0.43944457 ...,  0.435523    0.44728771\n",
      "  0.44728771]\n",
      "[ 0.43045701  0.43045701  0.42653544 ...,  0.43437858  0.43830014\n",
      "  0.44222171]\n",
      "[ 0.44728771  0.43944457  0.44336614 ...,  0.435523    0.43944457\n",
      "  0.44336614]\n",
      "[ 0.42653544  0.42261387  0.4186923  ...,  0.44336614  0.44336614\n",
      "  0.44336614]\n",
      "[ 0.41477073  0.41477073  0.4186923  ...,  0.43160143  0.42767987\n",
      "  0.43160143]\n",
      "[ 0.43830014  0.43437858  0.43045701 ...,  0.43160143  0.435523    0.435523  ]\n",
      "[ 0.4186923   0.4186923   0.4186923  ...,  0.4237583   0.41983673\n",
      "  0.42767987]\n",
      "[ 0.43045701  0.42653544  0.42261387 ...,  0.43160143  0.43160143  0.435523  ]\n",
      "[ 0.41477073  0.41084916  0.41084916 ...,  0.42905318  0.43297475\n",
      "  0.43297475]\n",
      "[ 0.40300603  0.41084916  0.41477073 ...,  0.43297475  0.43297475\n",
      "  0.43297475]\n",
      "[ 0.4186923   0.4186923   0.41477073 ...,  0.42261387  0.43045701\n",
      "  0.42653544]\n",
      "[ 0.4069276   0.40300603  0.39908446 ...,  0.43297475  0.42513161\n",
      "  0.43297475]\n",
      "[ 0.43437858  0.43045701  0.43437858 ...,  0.44222171  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.43830014  0.44222171  0.4186923  ...,  0.43045701  0.43045701\n",
      "  0.42261387]\n",
      "[ 0.44222171  0.43830014  0.43437858 ...,  0.44336614  0.43944457\n",
      "  0.43944457]\n",
      "[ 0.4186923   0.4186923   0.42653544 ...,  0.43830014  0.43045701\n",
      "  0.43045701]\n",
      "[ 0.42261387  0.42261387  0.4186923  ...,  0.44728771  0.44336614\n",
      "  0.43944457]\n",
      "[ 0.41477073  0.41477073  0.42261387 ...,  0.44222171  0.44222171\n",
      "  0.44222171]\n",
      "[ 0.43045701  0.43045701  0.43045701 ...,  0.44336614  0.43944457\n",
      "  0.43944457]\n",
      "[ 0.42261387  0.42261387  0.42653544 ...,  0.44222171  0.43830014\n",
      "  0.43437858]\n",
      "[ 0.4186923   0.4069276   0.4186923  ...,  0.43437858  0.43437858\n",
      "  0.44222171]\n",
      "[ 0.41477073  0.4186923   0.42261387 ...,  0.43437858  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.41477073  0.41084916  0.41084916 ...,  0.43045701  0.43045701\n",
      "  0.43045701]\n",
      "[ 0.42653544  0.41084916  0.4186923  ...,  0.43830014  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.42653544  0.42653544  0.42261387 ...,  0.43830014  0.43830014\n",
      "  0.44222171]\n",
      "[ 0.41477073  0.41084916  0.41477073 ...,  0.44866102  0.44081788\n",
      "  0.44081788]\n",
      "[ 0.42653544  0.43045701  0.42261387 ...,  0.42513161  0.43297475\n",
      "  0.43297475]\n",
      "[ 0.41477073  0.41477073  0.4186923  ...,  0.42905318  0.43297475\n",
      "  0.43297475]\n",
      "[ 0.41084916  0.41084916  0.4186923  ...,  0.43297475  0.43689631\n",
      "  0.43689631]\n",
      "[ 0.42653544  0.42653544  0.42261387 ...,  0.42905318  0.43689631\n",
      "  0.44473945]\n",
      "[ 0.4186923   0.4186923   0.4186923  ...,  0.43830014  0.44222171\n",
      "  0.44614328]\n",
      "[ 0.43689631  0.44081788  0.44081788 ...,  0.44866102  0.45258259\n",
      "  0.45258259]\n",
      "[ 0.42905318  0.42905318  0.43297475 ...,  0.44866102  0.44473945\n",
      "  0.44081788]\n",
      "[ 0.43297475  0.44081788  0.44081788 ...,  0.46042573  0.45258259\n",
      "  0.44473945]\n",
      "[ 0.44081788  0.43689631  0.43297475 ...,  0.44473945  0.44473945\n",
      "  0.44473945]\n",
      "[ 0.45177386  0.45569543  0.45569543 ...,  0.46353857  0.459617    0.46353857]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.47922484  0.47922484\n",
      "  0.48314641]\n",
      "[ 0.44000916  0.43608759  0.42824445 ...,  0.459617    0.46353857\n",
      "  0.46353857]\n",
      "[ 0.44000916  0.44393072  0.44785229 ...,  0.46746014  0.45569543\n",
      "  0.45569543]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.45569543  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.44000916  0.44000916  0.44393072 ...,  0.46353857  0.459617    0.46353857]\n",
      "[ 0.44000916  0.44393072  0.44393072 ...,  0.45569543  0.459617    0.459617  ]\n",
      "[ 0.4514992   0.44757763  0.4514992  ...,  0.47502861  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.4514992   0.4514992   0.4514992  ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.44365606  0.44365606  0.43973449 ...,  0.46718547  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.45542077  0.44365606  0.43973449 ...,  0.47502861  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.459617    0.45569543  0.45177386 ...,  0.47054246  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.4632639   0.4632639   0.45934234 ...,  0.4632639   0.47110704\n",
      "  0.47502861]\n",
      "[ 0.45177386  0.45177386  0.45177386 ...,  0.47530327  0.4713817   0.4713817 ]\n",
      "[ 0.45177386  0.45177386  0.45569543 ...,  0.46746014  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.459617    0.44393072  0.44393072 ...,  0.46353857  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.43216602  0.44000916  0.44393072 ...,  0.46746014  0.46353857\n",
      "  0.46746014]\n",
      "[ 0.45177386  0.44785229  0.44785229 ...,  0.47530327  0.4713817   0.4713817 ]\n",
      "[ 0.44000916  0.44393072  0.44393072 ...,  0.46353857  0.46746014\n",
      "  0.47530327]\n",
      "[ 0.43608759  0.43216602  0.42824445 ...,  0.459617    0.46353857\n",
      "  0.47530327]\n",
      "[ 0.43216602  0.42824445  0.43216602 ...,  0.459617    0.46746014\n",
      "  0.46353857]\n",
      "[ 0.43216602  0.42824445  0.42824445 ...,  0.44785229  0.45177386\n",
      "  0.45569543]\n",
      "[ 0.45569543  0.45177386  0.44393072 ...,  0.46662089  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.43216602  0.43216602  0.43216602 ...,  0.45569543  0.45569543  0.459617  ]\n",
      "[ 0.45177386  0.45177386  0.45177386 ...,  0.46718547  0.46718547\n",
      "  0.47110704]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.47110704  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.47110704  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.45569543  0.459617    0.459617   ...,  0.4632639   0.46718547\n",
      "  0.47110704]\n",
      "[ 0.459617    0.45177386  0.45569543 ...,  0.47110704  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44785229  0.45569543  0.45569543 ...,  0.47922484  0.47922484\n",
      "  0.48314641]\n",
      "[ 0.45569543  0.44785229  0.44785229 ...,  0.47922484  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.44000916  0.44000916  0.44000916 ...,  0.4713817   0.4713817   0.4713817 ]\n",
      "[ 0.45569543  0.45569543  0.45569543 ...,  0.47530327  0.47530327\n",
      "  0.47530327]\n",
      "[ 0.44785229  0.45177386  0.44785229 ...,  0.46353857  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.4713817   0.47530327\n",
      "  0.47922484]\n",
      "[ 0.45177386  0.44000916  0.43608759 ...,  0.47530327  0.4713817   0.4713817 ]\n",
      "[ 0.45177386  0.45177386  0.44785229 ...,  0.46746014  0.46353857\n",
      "  0.46746014]\n",
      "[ 0.44000916  0.44393072  0.44785229 ...,  0.47922484  0.47530327\n",
      "  0.4713817 ]\n",
      "[ 0.44785229  0.44393072  0.44393072 ...,  0.47922484  0.47922484\n",
      "  0.47530327]\n",
      "[ 0.44785229  0.44393072  0.44393072 ...,  0.46746014  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.44785229  0.45569543  0.45177386 ...,  0.47530327  0.47530327\n",
      "  0.4713817 ]\n",
      "[ 0.459617    0.459617    0.45569543 ...,  0.47922484  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.459617    0.45569543  0.45569543 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.459617    0.459617    0.459617   ...,  0.47530327  0.4713817   0.4713817 ]\n",
      "[ 0.459617    0.46353857  0.46353857 ...,  0.4713817   0.47922484\n",
      "  0.47922484]\n",
      "[ 0.40888075  0.40888075  0.40495918 ...,  0.4133669   0.41728847\n",
      "  0.41728847]\n",
      "[ 0.40552377  0.40552377  0.40552377 ...,  0.42513161  0.43297475\n",
      "  0.44081788]\n",
      "[ 0.39124132  0.39124132  0.39124132 ...,  0.39319448  0.39319448\n",
      "  0.38927291]\n",
      "[ 0.39516289  0.39516289  0.39124132 ...,  0.39711604  0.39319448\n",
      "  0.39711604]\n",
      "[ 0.39124132  0.38731975  0.38339818 ...,  0.39516289  0.39516289\n",
      "  0.39908446]\n",
      "[ 0.39908446  0.39124132  0.38731975 ...,  0.38927291  0.39319448\n",
      "  0.38927291]\n",
      "[ 0.39124132  0.38731975  0.38339818 ...,  0.40747692  0.40355535\n",
      "  0.40355535]\n",
      "[ 0.4069276   0.40300603  0.39516289 ...,  0.40807202  0.40807202\n",
      "  0.40807202]\n",
      "[ 0.4069276   0.39908446  0.41477073 ...,  0.41983673  0.41591516\n",
      "  0.41983673]\n",
      "[ 0.39908446  0.4069276   0.4069276  ...,  0.40807202  0.40807202\n",
      "  0.41199359]\n",
      "[ 0.4069276   0.40300603  0.39908446 ...,  0.41591516  0.41199359\n",
      "  0.40807202]\n",
      "[ 0.39319448  0.39711604  0.40495918 ...,  0.41728847  0.4016022   0.4133669 ]\n",
      "[ 0.39908446  0.39908446  0.4069276  ...,  0.41983673  0.41591516\n",
      "  0.41199359]\n",
      "[ 0.39124132  0.39516289  0.39908446 ...,  0.39179065  0.38394751\n",
      "  0.38394751]\n",
      "[ 0.4069276   0.4069276   0.4069276  ...,  0.41139849  0.40747692\n",
      "  0.40747692]\n",
      "[ 0.40300603  0.4069276   0.39516289 ...,  0.39571221  0.39963378\n",
      "  0.40355535]\n",
      "[ 0.38339818  0.39908446  0.4069276  ...,  0.39963378  0.39963378\n",
      "  0.39963378]\n",
      "[ 0.40300603  0.39908446  0.40300603 ...,  0.40747692  0.40747692\n",
      "  0.40747692]\n",
      "[ 0.40300603  0.40300603  0.39908446 ...,  0.40355535  0.39963378\n",
      "  0.40747692]\n",
      "[ 0.39124132  0.38731975  0.38731975 ...,  0.41532006  0.40747692\n",
      "  0.41139849]\n",
      "[ 0.39516289  0.39516289  0.39516289 ...,  0.39571221  0.39963378\n",
      "  0.39963378]\n",
      "[ 0.40300603  0.4069276   0.41477073 ...,  0.39571221  0.39179065\n",
      "  0.39179065]\n",
      "[ 0.40103761  0.39711604  0.40103761 ...,  0.4133669   0.40944533\n",
      "  0.40552377]\n",
      "[ 0.39124132  0.39124132  0.39124132 ...,  0.39963378  0.40355535\n",
      "  0.40747692]\n",
      "[ 0.40300603  0.40300603  0.40300603 ...,  0.41477073  0.41477073\n",
      "  0.41084916]\n",
      "[ 0.40300603  0.40300603  0.40300603 ...,  0.41084916  0.4133669\n",
      "  0.41728847]\n",
      "[ 0.39908446  0.41084916  0.41084916 ...,  0.39908446  0.39908446\n",
      "  0.4069276 ]\n",
      "[ 0.40300603  0.39908446  0.38339818 ...,  0.4069276   0.4069276\n",
      "  0.41084916]\n",
      "[ 0.4069276   0.4069276   0.4069276  ...,  0.40300603  0.41084916\n",
      "  0.4186923 ]\n",
      "[ 0.39908446  0.39908446  0.40300603 ...,  0.4231632   0.39963378\n",
      "  0.40355535]\n",
      "[ 0.38731975  0.38731975  0.38339818 ...,  0.40552377  0.40944533\n",
      "  0.41728847]\n",
      "[ 0.40103761  0.39711604  0.39711604 ...,  0.41139849  0.40747692\n",
      "  0.40355535]\n",
      "[ 0.39908446  0.39516289  0.4069276  ...,  0.41477073  0.42261387\n",
      "  0.41477073]\n",
      "[ 0.42064546  0.40888075  0.38535134 ...,  0.4133669   0.40552377\n",
      "  0.40944533]\n",
      "[ 0.40103761  0.39711604  0.39711604 ...,  0.40944533  0.40552377\n",
      "  0.40552377]\n",
      "[ 0.39908446  0.39908446  0.39516289 ...,  0.40747692  0.40747692\n",
      "  0.40355535]\n",
      "[ 0.39124132  0.39516289  0.39908446 ...,  0.40747692  0.40747692\n",
      "  0.41139849]\n",
      "[ 0.39516289  0.39516289  0.39516289 ...,  0.40355535  0.41139849\n",
      "  0.40747692]\n",
      "[ 0.39516289  0.39124132  0.38731975 ...,  0.40747692  0.40747692\n",
      "  0.40747692]\n",
      "[ 0.38731975  0.39124132  0.39124132 ...,  0.40355535  0.40355535\n",
      "  0.39963378]\n",
      "[ 0.40103761  0.40495918  0.40103761 ...,  0.4133669   0.41728847\n",
      "  0.4133669 ]\n",
      "[ 0.40552377  0.40552377  0.40552377 ...,  0.42513161  0.4133669   0.4133669 ]\n",
      "[ 0.4133669   0.4133669   0.4133669  ...,  0.41728847  0.41728847\n",
      "  0.41728847]\n",
      "[ 0.4016022   0.4016022   0.40552377 ...,  0.40552377  0.4133669   0.4133669 ]\n",
      "[ 0.40552377  0.40552377  0.4016022  ...,  0.4133669   0.42513161\n",
      "  0.40552377]\n",
      "[ 0.45177386  0.45177386  0.44000916 ...,  0.46353857  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.46297398  0.45905241  0.44728771 ...,  0.45513085  0.45513085\n",
      "  0.45513085]\n",
      "[ 0.43886473  0.43494316  0.43102159 ...,  0.45120928  0.45120928\n",
      "  0.45120928]\n",
      "[ 0.42432288  0.42824445  0.42824445 ...,  0.44728771  0.44728771\n",
      "  0.44728771]\n",
      "[ 0.41925689  0.42317845  0.42710002 ...,  0.45120928  0.45120928\n",
      "  0.44336614]\n",
      "[ 0.43102159  0.42710002  0.42317845 ...,  0.45120928  0.45006485\n",
      "  0.44614328]\n",
      "[ 0.41533532  0.41141375  0.43494316 ...,  0.45120928  0.45120928\n",
      "  0.45120928]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.46662089  0.47054246\n",
      "  0.46662089]\n",
      "[ 0.45093462  0.45093462  0.45093462 ...,  0.45093462  0.45485618\n",
      "  0.45877775]\n",
      "[ 0.44701305  0.44701305  0.44309148 ...,  0.47054246  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.44785229  0.44000916  0.42824445 ...,  0.45093462  0.45485618\n",
      "  0.46269932]\n",
      "[ 0.44785229  0.44785229  0.44000916 ...,  0.4713817   0.46353857\n",
      "  0.46353857]\n",
      "[ 0.44000916  0.44785229  0.44393072 ...,  0.45877775  0.45877775\n",
      "  0.46269932]\n",
      "[ 0.44670787  0.4427863   0.43886473 ...,  0.45905241  0.45120928\n",
      "  0.45120928]\n",
      "[ 0.44785229  0.45177386  0.45569543 ...,  0.46297398  0.46297398\n",
      "  0.45905241]\n",
      "[ 0.44000916  0.44000916  0.44000916 ...,  0.45120928  0.44728771\n",
      "  0.45120928]\n",
      "[ 0.43886473  0.43886473  0.43886473 ...,  0.46297398  0.45905241\n",
      "  0.45905241]\n",
      "[ 0.44000916  0.44000916  0.44393072 ...,  0.45905241  0.45905241\n",
      "  0.45905241]\n",
      "[ 0.4186923   0.4186923   0.42261387 ...,  0.44336614  0.45513085\n",
      "  0.44336614]\n",
      "[ 0.435523    0.435523    0.43160143 ...,  0.44728771  0.45513085\n",
      "  0.45905241]\n",
      "[ 0.42261387  0.41477073  0.42261387 ...,  0.45513085  0.45120928\n",
      "  0.44728771]\n",
      "[ 0.42653544  0.43437858  0.43437858 ...,  0.44728771  0.44728771\n",
      "  0.44728771]\n",
      "[ 0.44393072  0.43608759  0.44000916 ...,  0.46353857  0.46353857\n",
      "  0.4713817 ]\n",
      "[ 0.42261387  0.43045701  0.43437858 ...,  0.46689555  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.46353857  0.4713817   0.4713817 ]\n",
      "[ 0.43608759  0.43608759  0.44785229 ...,  0.46746014  0.46746014\n",
      "  0.4713817 ]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.46353857  0.4713817   0.4713817 ]\n",
      "[ 0.44393072  0.44393072  0.44000916 ...,  0.46353857  0.459617    0.459617  ]\n",
      "[ 0.44393072  0.44785229  0.45569543 ...,  0.4713817   0.47530327\n",
      "  0.46353857]\n",
      "[ 0.44393072  0.44393072  0.44000916 ...,  0.46746014  0.46746014\n",
      "  0.46353857]\n",
      "[ 0.44393072  0.44000916  0.43608759 ...,  0.46353857  0.459617    0.46353857]\n",
      "[ 0.43608759  0.44000916  0.44393072 ...,  0.45569543  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.44785229  0.44785229  0.44000916 ...,  0.46746014  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.44000916  0.44000916  0.44393072 ...,  0.45569543  0.45569543\n",
      "  0.45569543]\n",
      "[ 0.44393072  0.44000916  0.43608759 ...,  0.45177386  0.45177386\n",
      "  0.45569543]\n",
      "[ 0.43216602  0.43608759  0.43608759 ...,  0.46689555  0.46689555\n",
      "  0.46297398]\n",
      "[ 0.44000916  0.43608759  0.43608759 ...,  0.46297398  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.45569543  0.44393072  0.44393072 ...,  0.45905241  0.45905241\n",
      "  0.45513085]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.46297398  0.46689555\n",
      "  0.47081712]\n",
      "[ 0.44000916  0.44000916  0.43216602 ...,  0.44728771  0.44336614  0.435523  ]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.46746014  0.459617    0.459617  ]\n",
      "[ 0.44728771  0.45513085  0.45905241 ...,  0.46297398  0.46689555\n",
      "  0.47081712]\n",
      "[ 0.44728771  0.44728771  0.44728771 ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.44336614  0.44728771  0.44728771 ...,  0.46689555  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.44728771  0.44336614  0.44336614 ...,  0.47473869  0.46689555\n",
      "  0.45905241]\n",
      "[ 0.52462043  0.52462043  0.52069886 ...,  0.53246357  0.53246357\n",
      "  0.53638514]\n",
      "[ 0.52910658  0.53302815  0.53694972 ...,  0.52405585  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.55321584  0.55321584  0.55713741 ...,  0.533608    0.52968643  0.533608  ]\n",
      "[ 0.54929427  0.55321584  0.55713741 ...,  0.52968643  0.53752956\n",
      "  0.5453727 ]\n",
      "[ 0.5453727   0.5453727   0.5453727  ...,  0.54929427  0.55321584\n",
      "  0.55713741]\n",
      "[ 0.54929427  0.54929427  0.54929427 ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.54929427  0.5453727   0.5453727  ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52576486  0.533608    0.533608   ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.53752956  0.54145113  0.53752956 ...,  0.5453727   0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.5453727   0.55321584\n",
      "  0.54929427]\n",
      "[ 0.52462043  0.528542    0.53246357 ...,  0.54030671  0.54030671\n",
      "  0.53638514]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.533608    0.52968643\n",
      "  0.52968643]\n",
      "[ 0.53302815  0.53302815  0.52910658 ...,  0.53189899  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.52910658  0.52518502  0.52518502 ...,  0.53189899  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.52910658  0.52518502  0.52518502 ...,  0.53582055  0.53582055\n",
      "  0.53582055]\n",
      "[ 0.52910658  0.53694972  0.53694972 ...,  0.53189899  0.52797742\n",
      "  0.52405585]\n",
      "[ 0.52518502  0.52910658  0.52910658 ...,  0.53189899  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.54929427  0.54929427  0.54929427 ...,  0.54145113  0.53752956  0.533608  ]\n",
      "[ 0.54929427  0.54929427  0.5453727  ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.5453727   0.5453727   0.54929427 ...,  0.5453727   0.54145113\n",
      "  0.53752956]\n",
      "[ 0.53752956  0.533608    0.52968643 ...,  0.52184329  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52069886  0.52069886  0.52462043 ...,  0.53638514  0.53246357  0.528542  ]\n",
      "[ 0.54145113  0.5453727   0.5453727  ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.5453727   0.5453727   0.5453727  ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52968643  0.52968643  0.533608   ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.55321584  0.54145113  0.533608   ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.533608    0.533608    0.53752956 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52576486  0.52576486  0.533608   ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.53752956  0.53752956  0.533608  ]\n",
      "[ 0.54814984  0.54422827  0.54030671 ...,  0.54030671  0.54030671\n",
      "  0.54030671]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.54145113  0.53752956  0.54929427 ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.5453727   0.54929427  0.55321584 ...,  0.54929427  0.54929427\n",
      "  0.55321584]\n",
      "[ 0.55207141  0.55207141  0.56383612 ...,  0.55713741  0.55321584\n",
      "  0.55321584]\n",
      "[ 0.54929427  0.55321584  0.55713741 ...,  0.55207141  0.55599298\n",
      "  0.55599298]\n",
      "[ 0.55321584  0.55321584  0.55713741 ...,  0.55713741  0.56105898\n",
      "  0.55713741]\n",
      "[ 0.54030671  0.53638514  0.528542   ...,  0.53246357  0.528542    0.52462043]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.52797742  0.52405585\n",
      "  0.52405585]\n",
      "[ 0.53302815  0.52910658  0.52518502 ...,  0.53582055  0.53582055\n",
      "  0.53189899]\n",
      "[ 0.52518502  0.52126345  0.51734188 ...,  0.52797742  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.52910658  0.52518502  0.52518502 ...,  0.53582055  0.53582055\n",
      "  0.53582055]\n",
      "[ 0.49912261  0.50304417  0.50696574 ...,  0.50443275  0.50835431\n",
      "  0.50835431]\n",
      "[ 0.51032273  0.51032273  0.51032273 ...,  0.50443275  0.50443275\n",
      "  0.50443275]\n",
      "[ 0.52265202  0.52657359  0.52265202 ...,  0.50304417  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.51480888  0.51088731  0.52265202 ...,  0.51480888  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.51088731  0.51480888  0.51480888 ...,  0.51088731  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.51873045  0.50696574  0.49912261 ...,  0.50696574  0.51088731\n",
      "  0.51873045]\n",
      "[ 0.52265202  0.52657359  0.52657359 ...,  0.51480888  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.51314565  0.51314565  0.51706722 ...,  0.51088731  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.50922408  0.50922408  0.50922408 ...,  0.51088731  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.50922408  0.50530251  0.50530251 ...,  0.51088731  0.51088731\n",
      "  0.51088731]\n",
      "[ 0.50138094  0.51314565  0.52098878 ...,  0.50304417  0.51088731\n",
      "  0.51088731]\n",
      "[ 0.50696574  0.51088731  0.51088731 ...,  0.49855802  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.51706722  0.51314565  0.51314565 ...,  0.51873045  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.50696574  0.50304417  0.50304417 ...,  0.49658961  0.49658961\n",
      "  0.49658961]\n",
      "[ 0.51480888  0.50696574  0.50696574 ...,  0.49658961  0.49658961\n",
      "  0.50443275]\n",
      "[ 0.52657359  0.52265202  0.51088731 ...,  0.49658961  0.50051118\n",
      "  0.50443275]\n",
      "[ 0.49520104  0.49520104  0.50304417 ...,  0.50443275  0.48874647\n",
      "  0.49266804]\n",
      "[ 0.51088731  0.51088731  0.51088731 ...,  0.50051118  0.50443275\n",
      "  0.50443275]\n",
      "[ 0.51873045  0.51873045  0.51873045 ...,  0.51480888  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.51873045  0.52265202  0.52657359 ...,  0.51088731  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.51873045  0.51873045  0.51480888 ...,  0.50696574  0.51088731\n",
      "  0.51480888]\n",
      "[ 0.51088731  0.51088731  0.51480888 ...,  0.50304417  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.51873045  0.51480888  0.51088731 ...,  0.50443275  0.50051118\n",
      "  0.50443275]\n",
      "[ 0.52265202  0.51873045  0.51873045 ...,  0.49912261  0.49127947\n",
      "  0.49520104]\n",
      "[ 0.51088731  0.52265202  0.52657359 ...,  0.5142443   0.50640116\n",
      "  0.50247959]\n",
      "[ 0.52265202  0.51873045  0.51480888 ...,  0.51088731  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.51480888  0.51480888  0.51873045 ...,  0.51032273  0.5142443\n",
      "  0.51032273]\n",
      "[ 0.51088731  0.51480888  0.51873045 ...,  0.51032273  0.5142443   0.5142443 ]\n",
      "[ 0.51873045  0.51873045  0.51088731 ...,  0.52265202  0.53049516\n",
      "  0.52657359]\n",
      "[ 0.51480888  0.51088731  0.50696574 ...,  0.51227588  0.51227588\n",
      "  0.51227588]\n",
      "[ 0.49912261  0.49127947  0.49127947 ...,  0.51032273  0.5142443\n",
      "  0.51032273]\n",
      "[ 0.51088731  0.51088731  0.51480888 ...,  0.50443275  0.50835431\n",
      "  0.50835431]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.50835431  0.51227588\n",
      "  0.50835431]\n",
      "[ 0.50304417  0.50696574  0.50696574 ...,  0.49658961  0.50051118\n",
      "  0.50051118]\n",
      "[ 0.50696574  0.51088731  0.51088731 ...,  0.51032273  0.51032273\n",
      "  0.51032273]\n",
      "[ 0.53049516  0.53049516  0.52657359 ...,  0.52265202  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.52265202  0.52265202  0.50696574 ...,  0.51873045  0.51873045\n",
      "  0.52657359]\n",
      "[ 0.52657359  0.53049516  0.53049516 ...,  0.52265202  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.53049516  0.53049516  0.52657359 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.52657359  0.52657359  0.52657359 ...,  0.52265202  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.51088731  0.51088731  0.50696574 ...,  0.52404059  0.52011902\n",
      "  0.51227588]\n",
      "[ 0.5142443   0.5142443   0.51816587 ...,  0.50443275  0.50835431\n",
      "  0.50443275]\n",
      "[ 0.51032273  0.51032273  0.5142443  ...,  0.50443275  0.50443275\n",
      "  0.50835431]\n",
      "[ 0.51032273  0.51032273  0.51032273 ...,  0.50051118  0.50443275\n",
      "  0.50443275]\n",
      "[ 0.51032273  0.51032273  0.5142443  ...,  0.49658961  0.50443275\n",
      "  0.50835431]\n",
      "[ 0.52405585  0.52405585  0.52405585 ...,  0.51480888  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.52657359  0.52657359  0.52657359 ...,  0.51873045  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.51621271  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.52405585  0.52797742  0.52797742 ...,  0.53189899  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.52910658  0.52518502  0.52518502 ...,  0.53189899  0.53189899\n",
      "  0.53189899]\n",
      "[ 0.54366369  0.53974212  0.53974212 ...,  0.51621271  0.52013428\n",
      "  0.52405585]\n",
      "[ 0.55150683  0.54366369  0.53582055 ...,  0.52013428  0.52405585\n",
      "  0.52797742]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.51734188  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.51734188  0.52126345  0.52126345 ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.53694972  0.53302815  0.52910658 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.51734188  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.51229114  0.51229114  0.52013428 ...,  0.51621271  0.51229114\n",
      "  0.50836957]\n",
      "[ 0.52184329  0.51792172  0.51734188 ...,  0.51342031  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.52518502  0.52518502  0.51734188 ...,  0.51229114  0.50836957\n",
      "  0.51229114]\n",
      "[ 0.52910658  0.53302815  0.52126345 ...,  0.51621271  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.52518502  0.51734188  0.51734188 ...,  0.52013428  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.52126345  0.51734188  0.51734188 ...,  0.51229114  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.51229114  0.51229114\n",
      "  0.51621271]\n",
      "[ 0.53189899  0.52797742  0.53302815 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.52518502  0.52126345  0.52910658 ...,  0.53302815  0.52518502\n",
      "  0.52126345]\n",
      "[ 0.53582055  0.53582055  0.52910658 ...,  0.52126345  0.52126345\n",
      "  0.52518502]\n",
      "[ 0.53694972  0.53694972  0.53302815 ...,  0.53302815  0.52518502\n",
      "  0.52126345]\n",
      "[ 0.51734188  0.51342031  0.504448   ...,  0.50304417  0.50304417\n",
      "  0.51088731]\n",
      "[ 0.53582055  0.53974212  0.53974212 ...,  0.52518502  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.52518502  0.52518502  0.52126345 ...,  0.51621271  0.51621271\n",
      "  0.52013428]\n",
      "[ 0.52126345  0.52126345  0.52405585 ...,  0.51621271  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.52126345  0.52126345  0.52518502 ...,  0.52126345  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52126345  0.52126345  0.51342031 ...,  0.52013428  0.52797742\n",
      "  0.52405585]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.52518502  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.52518502  0.52910658  0.52910658 ...,  0.50836957  0.51621271\n",
      "  0.52405585]\n",
      "[ 0.52910658  0.53302815  0.53694972 ...,  0.51229114  0.51621271\n",
      "  0.52405585]\n",
      "[ 0.53302815  0.53302815  0.53302815 ...,  0.52013428  0.51621271\n",
      "  0.51229114]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.51229114  0.51229114\n",
      "  0.50836957]\n",
      "[ 0.51229114  0.51621271  0.51621271 ...,  0.49520104  0.49127947\n",
      "  0.49520104]\n",
      "[ 0.52518502  0.51734188  0.51734188 ...,  0.51621271  0.52013428\n",
      "  0.52797742]\n",
      "[ 0.53974212  0.54366369  0.53974212 ...,  0.52910658  0.53302815\n",
      "  0.53302815]\n",
      "[ 0.5554284   0.5554284   0.55150683 ...,  0.53302815  0.53302815\n",
      "  0.53694972]\n",
      "[ 0.53974212  0.54366369  0.54758526 ...,  0.52797742  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.5554284   0.55150683  0.54366369 ...,  0.54087129  0.53189899\n",
      "  0.53582055]\n",
      "[ 0.53974212  0.53582055  0.53189899 ...,  0.52126345  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.52013428  0.52013428  0.51621271 ...,  0.52013428  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.53049516  0.52657359  0.52657359 ...,  0.53049516  0.53049516\n",
      "  0.53049516]\n",
      "[ 0.53049516  0.52657359  0.52657359 ...,  0.51873045  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.52657359  0.52657359  0.52657359 ...,  0.53049516  0.52265202\n",
      "  0.51480888]\n",
      "[ 0.52657359  0.52657359  0.52657359 ...,  0.52657359  0.52265202\n",
      "  0.52657359]\n",
      "[ 0.43132677  0.43132677  0.42348363 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42824445  0.42824445  0.43216602 ...,  0.46718547  0.47110704\n",
      "  0.47895018]\n",
      "[ 0.43973449  0.43973449  0.44365606 ...,  0.4873579   0.49520104\n",
      "  0.48343633]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47167163  0.47167163\n",
      "  0.47559319]\n",
      "[ 0.44757763  0.44757763  0.44757763 ...,  0.47167163  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.43973449  0.44365606  0.44757763 ...,  0.4873579   0.4873579   0.4873579 ]\n",
      "[ 0.44757763  0.44365606  0.44757763 ...,  0.47167163  0.47559319\n",
      "  0.46775006]\n",
      "[ 0.44087892  0.43695735  0.43303578 ...,  0.46608682  0.47000839\n",
      "  0.46608682]\n",
      "[ 0.43695735  0.43303578  0.43303578 ...,  0.47000839  0.47392996\n",
      "  0.47000839]\n",
      "[ 0.43695735  0.43695735  0.44087892 ...,  0.46216526  0.46216526\n",
      "  0.46216526]\n",
      "[ 0.43303578  0.43695735  0.43695735 ...,  0.46216526  0.46216526\n",
      "  0.46608682]\n",
      "[ 0.42348363  0.4274052   0.43524834 ...,  0.45934234  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.43695735  0.43695735  0.43695735 ...,  0.46216526  0.45432212\n",
      "  0.45432212]\n",
      "[ 0.44365606  0.44365606  0.44757763 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.44365606  0.44365606  0.44365606 ...,  0.47502861  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44365606  0.44365606  0.43973449 ...,  0.47110704  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.44365606  0.44365606  0.43973449 ...,  0.47110704  0.4632639   0.4632639 ]\n",
      "[ 0.44365606  0.43581292  0.43189136 ...,  0.45542077  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.43973449  0.44365606  0.44365606 ...,  0.47502861  0.47502861\n",
      "  0.47110704]\n",
      "[ 0.43973449  0.43973449  0.44365606 ...,  0.46718547  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.43189136  0.44365606  0.44757763 ...,  0.47110704  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.43973449  0.43973449  0.44365606 ...,  0.4632639   0.46718547\n",
      "  0.47502861]\n",
      "[ 0.43216602  0.43216602  0.43524834 ...,  0.4514992   0.45542077\n",
      "  0.45934234]\n",
      "[ 0.43581292  0.42404822  0.42796979 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.42796979  0.42796979  0.43581292 ...,  0.45934234  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.42796979  0.42796979  0.42796979 ...,  0.46440833  0.46440833\n",
      "  0.46048676]\n",
      "[ 0.43581292  0.43973449  0.43973449 ...,  0.45934234  0.4632639\n",
      "  0.45542077]\n",
      "[ 0.43581292  0.43973449  0.43973449 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.42404822  0.43973449  0.44757763 ...,  0.45934234  0.4632639   0.4632639 ]\n",
      "[ 0.43973449  0.43973449  0.44365606 ...,  0.46718547  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44757763  0.44365606  0.44757763 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.43973449  0.43581292  0.43973449 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.4514992   0.4514992   0.4514992  ...,  0.47895018  0.48287175\n",
      "  0.48679332]\n",
      "[ 0.43916991  0.43524834  0.43132677 ...,  0.44365606  0.44365606\n",
      "  0.4514992 ]\n",
      "[ 0.43581292  0.43581292  0.43581292 ...,  0.46718547  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.47446403  0.46269932  0.45485618 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.44309148  0.43916991  0.43916991 ...,  0.48287175  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.44309148  0.44701305  0.45485618 ...,  0.47502861  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.45093462  0.45093462  0.45093462 ...,  0.47502861  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.43916991  0.43916991  0.43916991 ...,  0.47895018  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.42348363  0.43524834  0.43916991 ...,  0.46718547  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.43608759  0.42824445  0.43608759 ...,  0.47110704  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44309148  0.44701305  0.43524834 ...,  0.46718547  0.46718547\n",
      "  0.47110704]\n",
      "[ 0.44785229  0.44393072  0.43608759 ...,  0.47110704  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.47502861  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.51734188  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.50304417  0.50304417  0.50696574 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.52126345  0.51342031  0.51342031 ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52910658  0.52910658  0.52126345 ...,  0.55321584  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52126345  0.51342031  0.52126345 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52910658  0.52910658  0.52126345 ...,  0.54145113  0.533608    0.52968643]\n",
      "[ 0.50557717  0.5016556   0.5016556  ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.51734188  0.51342031  0.51342031 ...,  0.52576486  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.52492561  0.52100404\n",
      "  0.51708248]\n",
      "[ 0.49381247  0.49773404  0.5016556  ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.49912261  0.49520104  0.50304417 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.5016556   0.5016556   0.49381247 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.504448    0.51229114  0.504448   ...,  0.51734188  0.52126345\n",
      "  0.52518502]\n",
      "[ 0.50836957  0.51621271  0.52013428 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.50836957  0.50052644  0.50836957 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.504448    0.504448    0.51229114 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.51621271  0.51229114  0.51229114 ...,  0.51342031  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.52968643  0.52968643  0.533608  ]\n",
      "[ 0.52910658  0.51342031  0.5016556  ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.5016556   0.50557717  0.51734188 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50304417  0.50304417  0.50696574 ...,  0.51734188  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.52126345  0.51734188  0.50949874 ...,  0.52576486  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.50557717  0.50557717  0.50949874 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.50949874  0.50949874  0.51342031 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.50949874  0.50557717  0.50557717 ...,  0.533608    0.54145113\n",
      "  0.54145113]\n",
      "[ 0.50557717  0.5016556   0.5016556  ...,  0.51734188  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.50557717  0.5016556   0.50557717 ...,  0.52518502  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.51734188  0.50949874  0.50557717 ...,  0.52184329  0.51400015\n",
      "  0.52184329]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.50557717  0.50557717  0.50557717 ...,  0.52184329  0.52184329\n",
      "  0.52576486]\n",
      "[ 0.49773404  0.50557717  0.51734188 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.50557717  0.5016556   0.5016556  ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52910658  0.52518502  0.52910658 ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.51734188  0.52518502  0.52126345 ...,  0.54929427  0.5453727   0.5453727 ]\n",
      "[ 0.52518502  0.52910658  0.52910658 ...,  0.53752956  0.53752956\n",
      "  0.52968643]\n",
      "[ 0.53302815  0.52910658  0.52910658 ...,  0.53752956  0.52968643  0.533608  ]\n",
      "[ 0.50304417  0.50696574  0.50696574 ...,  0.52126345  0.49773404\n",
      "  0.5016556 ]\n",
      "[ 0.52265202  0.51873045  0.51873045 ...,  0.52518502  0.52518502\n",
      "  0.53694972]\n",
      "[ 0.50304417  0.50304417  0.50696574 ...,  0.52910658  0.53694972\n",
      "  0.52518502]\n",
      "[ 0.49912261  0.50304417  0.50696574 ...,  0.52910658  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.51706722  0.51706722  0.51706722 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.48961624  0.48961624\n",
      "  0.48569467]\n",
      "[ 0.47110704  0.46718547  0.46718547 ...,  0.48961624  0.48961624\n",
      "  0.49745937]\n",
      "[ 0.47502861  0.47110704  0.47110704 ...,  0.50949874  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.47502861  0.47110704  0.47110704 ...,  0.5016556   0.5016556   0.5016556 ]\n",
      "[ 0.47110704  0.47895018  0.48287175 ...,  0.4898909   0.48204776\n",
      "  0.48596933]\n",
      "[ 0.47502861  0.47895018  0.47895018 ...,  0.50557717  0.5016556\n",
      "  0.49381247]\n",
      "[ 0.47895018  0.48287175  0.48287175 ...,  0.4898909   0.49381247\n",
      "  0.50557717]\n",
      "[ 0.4683299   0.4683299   0.4683299  ...,  0.48204776  0.49381247\n",
      "  0.49381247]\n",
      "[ 0.46048676  0.45656519  0.46048676 ...,  0.48596933  0.48596933\n",
      "  0.4898909 ]\n",
      "[ 0.47225147  0.4683299   0.46440833 ...,  0.49381247  0.49773404\n",
      "  0.49381247]\n",
      "[ 0.46440833  0.46440833  0.47617304 ...,  0.4898909   0.4898909\n",
      "  0.49381247]\n",
      "[ 0.4632639   0.4632639   0.46718547 ...,  0.4817731   0.48569467\n",
      "  0.48569467]\n",
      "[ 0.4683299   0.47225147  0.4683299  ...,  0.4898909   0.4898909   0.4898909 ]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.49745937  0.49745937\n",
      "  0.4935378 ]\n",
      "[ 0.4632639   0.4632639   0.45934234 ...,  0.49745937  0.48961624\n",
      "  0.48569467]\n",
      "[ 0.47502861  0.47895018  0.47502861 ...,  0.48961624  0.48961624\n",
      "  0.4935378 ]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.50138094  0.49745937\n",
      "  0.50138094]\n",
      "[ 0.46718547  0.4632639   0.4632639  ...,  0.4935378   0.4935378   0.4935378 ]\n",
      "[ 0.4683299   0.4683299   0.4683299  ...,  0.4935378   0.49745937\n",
      "  0.49745937]\n",
      "[ 0.46048676  0.46048676  0.46048676 ...,  0.50530251  0.50138094\n",
      "  0.49745937]\n",
      "[ 0.47502861  0.47502861  0.47502861 ...,  0.4817731   0.47392996\n",
      "  0.47000839]\n",
      "[ 0.46440833  0.4683299   0.4683299  ...,  0.50138094  0.50530251\n",
      "  0.50557717]\n",
      "[ 0.4632639   0.4632639   0.4632639  ...,  0.48569467  0.4817731   0.4817731 ]\n",
      "[ 0.47225147  0.47225147  0.47225147 ...,  0.50138094  0.50138094\n",
      "  0.49745937]\n",
      "[ 0.4632639   0.45934234  0.45934234 ...,  0.49745937  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.48961624  0.48961624\n",
      "  0.48961624]\n",
      "[ 0.4632639   0.47110704  0.47502861 ...,  0.4935378   0.4935378\n",
      "  0.48569467]\n",
      "[ 0.4632639   0.46718547  0.46718547 ...,  0.48569467  0.4935378   0.4935378 ]\n",
      "[ 0.46718547  0.4632639   0.45934234 ...,  0.50530251  0.50138094\n",
      "  0.48961624]\n",
      "[ 0.47502861  0.47895018  0.48287175 ...,  0.50138094  0.4935378\n",
      "  0.49745937]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.49745937  0.4935378   0.4935378 ]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.4935378   0.48961624\n",
      "  0.48961624]\n",
      "[ 0.46718547  0.4632639   0.45934234 ...,  0.4935378   0.49745937\n",
      "  0.50138094]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.48961624  0.4935378   0.4935378 ]\n",
      "[ 0.46718547  0.47502861  0.47502861 ...,  0.4935378   0.48961624\n",
      "  0.48961624]\n",
      "[ 0.47502861  0.47895018  0.47895018 ...,  0.51706722  0.52098878\n",
      "  0.52098878]\n",
      "[ 0.49463645  0.49071489  0.48679332 ...,  0.51706722  0.50922408\n",
      "  0.50138094]\n",
      "[ 0.47110704  0.47895018  0.47895018 ...,  0.48569467  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.50138094  0.51314565\n",
      "  0.52098878]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.52491035  0.51706722\n",
      "  0.51706722]\n",
      "[ 0.45934234  0.45934234  0.45542077 ...,  0.48343633  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.4632639   0.4632639   0.4632639  ...,  0.49745937  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49745937  0.4935378\n",
      "  0.48961624]\n",
      "[ 0.46662089  0.46269932  0.46269932 ...,  0.49745937  0.4935378\n",
      "  0.48961624]\n",
      "[ 0.45542077  0.45934234  0.4632639  ...,  0.4935378   0.4935378   0.4935378 ]\n",
      "[ 0.51342031  0.51734188  0.51734188 ...,  0.52576486  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52013428  0.52405585  0.52797742 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.52184329  0.52576486  0.52968643 ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.533608    0.52968643\n",
      "  0.51792172]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.533608    0.52968643\n",
      "  0.52576486]\n",
      "[ 0.533608    0.52968643  0.52576486 ...,  0.54929427  0.54929427\n",
      "  0.54929427]\n",
      "[ 0.52576486  0.51792172  0.51400015 ...,  0.5453727   0.54145113\n",
      "  0.54145113]\n",
      "[ 0.51400015  0.51400015  0.51792172 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.51792172  0.51400015  0.51400015 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51400015  0.51792172  0.51792172 ...,  0.53752956  0.5453727   0.5453727 ]\n",
      "[ 0.52576486  0.52184329  0.52184329 ...,  0.533608    0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51734188  0.51342031  0.51342031 ...,  0.52968643  0.52968643  0.533608  ]\n",
      "[ 0.52184329  0.51792172  0.51792172 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.53302815  0.53302815  0.52518502 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.52126345  0.52126345  0.51734188 ...,  0.52576486  0.52576486  0.533608  ]\n",
      "[ 0.51734188  0.51734188  0.51342031 ...,  0.52968643  0.52968643\n",
      "  0.52184329]\n",
      "[ 0.51342031  0.51734188  0.51734188 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.51734188  0.50949874  0.50557717 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52518502  0.52518502  0.52126345 ...,  0.51792172  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.533608    0.52968643\n",
      "  0.52576486]\n",
      "[ 0.52910658  0.53302815  0.52910658 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.51734188  0.51342031  0.50949874 ...,  0.51400015  0.533608    0.54145113]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.5453727   0.54145113\n",
      "  0.54145113]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52126345  0.52126345  0.51734188 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.52968643  0.53752956\n",
      "  0.5453727 ]\n",
      "[ 0.52518502  0.52126345  0.52126345 ...,  0.52184329  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.51342031  0.52126345  0.52126345 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.52126345  0.52126345  0.52518502 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51734188  0.51734188  0.51342031 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.50949874  0.52126345  0.51734188 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.53302815  0.53694972  0.53302815 ...,  0.54929427  0.54929427\n",
      "  0.54929427]\n",
      "[ 0.54479286  0.54087129  0.53694972 ...,  0.55713741  0.55713741\n",
      "  0.55713741]\n",
      "[ 0.53302815  0.53694972  0.54479286 ...,  0.55321584  0.55321584\n",
      "  0.55321584]\n",
      "[ 0.52910658  0.53302815  0.53302815 ...,  0.54929427  0.5453727\n",
      "  0.54929427]\n",
      "[ 0.53694972  0.53694972  0.53302815 ...,  0.54929427  0.54929427\n",
      "  0.54929427]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.51734188  0.52126345  0.52126345 ...,  0.53302815  0.53302815\n",
      "  0.53302815]\n",
      "[ 0.52126345  0.51734188  0.51734188 ...,  0.52910658  0.53302815\n",
      "  0.54087129]\n",
      "[ 0.50530251  0.51706722  0.51706722 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.53302815  0.53302815\n",
      "  0.53302815]\n",
      "[ 0.48961624  0.48961624  0.48569467 ...,  0.50836957  0.51229114\n",
      "  0.52013428]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.51314565  0.51314565\n",
      "  0.51314565]\n",
      "[ 0.49660487  0.49660487  0.49660487 ...,  0.51792172  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.49660487  0.4926833   0.4926833  ...,  0.51007858  0.51007858\n",
      "  0.51792172]\n",
      "[ 0.50052644  0.50052644  0.504448   ...,  0.51342031  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.51229114  0.50836957  0.504448   ...,  0.50615702  0.51007858\n",
      "  0.51400015]\n",
      "[ 0.50052644  0.49660487  0.50836957 ...,  0.52184329  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.4935378   0.4935378   0.48961624 ...,  0.50756085  0.50756085\n",
      "  0.51148241]\n",
      "[ 0.48961624  0.48961624  0.48961624 ...,  0.50756085  0.51148241\n",
      "  0.51932555]\n",
      "[ 0.4935378   0.48961624  0.4817731  ...,  0.51540398  0.51540398\n",
      "  0.51540398]\n",
      "[ 0.4817731   0.48961624  0.48961624 ...,  0.51148241  0.51148241\n",
      "  0.51148241]\n",
      "[ 0.48961624  0.48961624  0.48961624 ...,  0.50527199  0.50135042\n",
      "  0.50135042]\n",
      "[ 0.48961624  0.48569467  0.4935378  ...,  0.51932555  0.51932555\n",
      "  0.51148241]\n",
      "[ 0.49127947  0.50304417  0.51088731 ...,  0.51734188  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.48961624  0.4935378   0.4935378  ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49127947  0.49912261  0.49912261 ...,  0.49773404  0.50949874\n",
      "  0.51734188]\n",
      "[ 0.48343633  0.48343633  0.49127947 ...,  0.50557717  0.5016556\n",
      "  0.50557717]\n",
      "[ 0.4935378   0.48961624  0.48961624 ...,  0.50949874  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.49745937  0.49745937  0.49745937 ...,  0.51734188  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.49745937  0.4935378   0.48961624 ...,  0.51342031  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.49745937  0.49745937  0.50530251 ...,  0.50949874  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49745937  0.4935378   0.4935378  ...,  0.51734188  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.47785153  0.47785153  0.48569467 ...,  0.504448    0.504448    0.50836957]\n",
      "[ 0.4935378   0.49745937  0.50138094 ...,  0.52126345  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49520104  0.49520104  0.49520104 ...,  0.51734188  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.4873579   0.48343633  0.48343633 ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.48569467  0.48961624  0.4935378  ...,  0.51734188  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.49520104  0.49520104  0.49520104 ...,  0.51342031  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49127947  0.49127947  0.49520104 ...,  0.5016556   0.51734188\n",
      "  0.50949874]\n",
      "[ 0.49745937  0.49745937  0.4935378  ...,  0.51734188  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.49745937  0.49745937  0.50138094 ...,  0.50949874  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.48961624  0.48961624  0.48961624 ...,  0.51342031  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.4935378   0.4935378   0.50530251 ...,  0.51734188  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.50138094  0.4935378   0.4817731  ...,  0.504448    0.49660487  0.504448  ]\n",
      "[ 0.48961624  0.4935378   0.49745937 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50530251  0.50138094  0.50138094 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.50922408  0.50530251  0.50530251 ...,  0.53302815  0.52910658\n",
      "  0.53302815]\n",
      "[ 0.51314565  0.51314565  0.51314565 ...,  0.52910658  0.52910658\n",
      "  0.52126345]\n",
      "[ 0.51314565  0.51314565  0.51314565 ...,  0.52910658  0.53302815\n",
      "  0.53694972]\n",
      "[ 0.50922408  0.50530251  0.50530251 ...,  0.52910658  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.49773404  0.49660487  0.504448  ]\n",
      "[ 0.50304417  0.49912261  0.49520104 ...,  0.52098878  0.51706722\n",
      "  0.51706722]\n",
      "[ 0.50304417  0.50304417  0.49912261 ...,  0.52491035  0.53275349\n",
      "  0.52098878]\n",
      "[ 0.49127947  0.4873579   0.49127947 ...,  0.52098878  0.51314565\n",
      "  0.51314565]\n",
      "[ 0.49520104  0.49912261  0.49912261 ...,  0.51314565  0.51314565\n",
      "  0.51706722]\n",
      "[ 0.52576486  0.52576486  0.52184329 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.54145113  0.5453727\n",
      "  0.54929427]\n",
      "[ 0.552636    0.552636    0.552636   ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54087129  0.54479286  0.54871443 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54479286  0.54479286  0.54479286 ...,  0.52518502  0.53528649\n",
      "  0.53920806]\n",
      "[ 0.53694972  0.52910658  0.53694972 ...,  0.52744335  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.54705119  0.53528649  0.52744335 ...,  0.54312963  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.533608   0.533608   0.533608  ...,  0.5372549  0.5372549  0.5372549]\n",
      "[ 0.54145113  0.533608    0.52576486 ...,  0.54901961  0.54509804\n",
      "  0.54509804]\n",
      "[ 0.54145113  0.53752956  0.533608   ...,  0.54509804  0.54901961\n",
      "  0.54901961]\n",
      "[ 0.53752956  0.533608    0.52968643 ...,  0.54509804  0.54901961\n",
      "  0.54901961]\n",
      "[ 0.53246357  0.53638514  0.528542   ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52968643  0.52968643  0.52184329 ...,  0.54509804  0.54509804\n",
      "  0.54901961]\n",
      "[ 0.52968643  0.52968643  0.52576486 ...,  0.5453727   0.54145113\n",
      "  0.53752956]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.53752956  0.53752956  0.533608  ]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.54145113  0.533608    0.533608   ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.54929427  0.54145113  0.53752956 ...,  0.54145113  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.5453727   0.54145113  0.54145113 ...,  0.54929427  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.55321584  0.54929427  0.54929427 ...,  0.53752956  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52968643  0.52968643  0.533608   ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.53752956  0.53752956  0.52576486 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.533608    0.52968643  0.52968643 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.52968643  0.52968643  0.54145113 ...,  0.54145113  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.53752956  0.5453727\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.53752956 ...,  0.54929427  0.54929427\n",
      "  0.54929427]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.54145113  0.53752956  0.533608   ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52184329  0.52576486  0.52968643 ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.52576486  0.52576486  0.52968643 ...,  0.55321584  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.533608    0.533608    0.53752956 ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.5453727   0.54145113  0.54145113 ...,  0.54479286  0.55655756\n",
      "  0.56047913]\n",
      "[ 0.54929427  0.54929427  0.55321584 ...,  0.55655756  0.55655756\n",
      "  0.55655756]\n",
      "[ 0.55713741  0.56498054  0.56498054 ...,  0.5644007   0.55655756\n",
      "  0.56047913]\n",
      "[ 0.54929427  0.55713741  0.54929427 ...,  0.5588159   0.5588159\n",
      "  0.55489433]\n",
      "[ 0.55713741  0.55321584  0.5453727  ...,  0.552636    0.552636    0.552636  ]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.52968643  0.533608    0.53752956 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52126345  0.53302815  0.53694972 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53752956  0.54145113  0.54145113 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.42432288  0.41647974  0.40471504 ...,  0.4514992   0.44757763\n",
      "  0.44757763]\n",
      "[ 0.42040131  0.42040131  0.42432288 ...,  0.45485618  0.45877775\n",
      "  0.46269932]\n",
      "[ 0.43216602  0.43216602  0.42432288 ...,  0.47502861  0.46718547\n",
      "  0.45542077]\n",
      "[ 0.43608759  0.43216602  0.42824445 ...,  0.45934234  0.45542077\n",
      "  0.45542077]\n",
      "[ 0.43216602  0.43608759  0.43608759 ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.43216602  0.43216602  0.43216602 ...,  0.49463645  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.43216602  0.43216602  0.42824445 ...,  0.43973449  0.43189136\n",
      "  0.44365606]\n",
      "[ 0.43132677  0.4274052   0.4156405  ...,  0.45264363  0.45264363\n",
      "  0.44872206]\n",
      "[ 0.43132677  0.41956207  0.4156405  ...,  0.46048676  0.46440833\n",
      "  0.4683299 ]\n",
      "[ 0.41956207  0.41956207  0.40779736 ...,  0.45264363  0.45264363\n",
      "  0.44480049]\n",
      "[ 0.4156405   0.4156405   0.4156405  ...,  0.46048676  0.45656519\n",
      "  0.45264363]\n",
      "[ 0.41647974  0.41647974  0.41647974 ...,  0.43581292  0.44757763\n",
      "  0.44757763]\n",
      "[ 0.4156405   0.41171893  0.40779736 ...,  0.45264363  0.44872206\n",
      "  0.44480049]\n",
      "[ 0.43216602  0.42824445  0.43216602 ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.43216602  0.43216602  0.43216602 ...,  0.4514992   0.45542077\n",
      "  0.45542077]\n",
      "[ 0.43216602  0.42432288  0.42040131 ...,  0.45934234  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.42432288  0.42040131  0.42432288 ...,  0.45542077  0.45934234\n",
      "  0.45542077]\n",
      "[ 0.44000916  0.44000916  0.43216602 ...,  0.45934234  0.45542077\n",
      "  0.45542077]\n",
      "[ 0.41956207  0.41171893  0.41956207 ...,  0.45485618  0.45485618\n",
      "  0.45877775]\n",
      "[ 0.42348363  0.41956207  0.43132677 ...,  0.46269932  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.43132677  0.42348363  0.4274052  ...,  0.45485618  0.45877775\n",
      "  0.45877775]\n",
      "[ 0.42348363  0.42348363  0.4274052  ...,  0.47054246  0.4783856   0.4783856 ]\n",
      "[ 0.42040131  0.42040131  0.42040131 ...,  0.44757763  0.43189136\n",
      "  0.43973449]\n",
      "[ 0.4156405   0.41956207  0.42348363 ...,  0.45877775  0.45093462\n",
      "  0.45485618]\n",
      "[ 0.40779736  0.4156405   0.4156405  ...,  0.4514992   0.44757763\n",
      "  0.4514992 ]\n",
      "[ 0.4156405   0.4156405   0.41956207 ...,  0.44757763  0.4514992\n",
      "  0.43973449]\n",
      "[ 0.41171893  0.4156405   0.41171893 ...,  0.46718547  0.45542077\n",
      "  0.4514992 ]\n",
      "[ 0.41956207  0.41171893  0.41171893 ...,  0.44757763  0.44757763\n",
      "  0.44757763]\n",
      "[ 0.41171893  0.40779736  0.40779736 ...,  0.43973449  0.44365606\n",
      "  0.4514992 ]\n",
      "[ 0.41647974  0.41255818  0.41255818 ...,  0.45656519  0.46048676\n",
      "  0.46048676]\n",
      "[ 0.44000916  0.43216602  0.42432288 ...,  0.44365606  0.4514992\n",
      "  0.45542077]\n",
      "[ 0.42432288  0.42432288  0.42432288 ...,  0.4514992   0.44365606\n",
      "  0.44757763]\n",
      "[ 0.42348363  0.41956207  0.41956207 ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.41255818  0.41255818  0.41255818 ...,  0.4514992   0.45542077\n",
      "  0.45542077]\n",
      "[ 0.41647974  0.42040131  0.3968719  ...,  0.46440833  0.46440833\n",
      "  0.46048676]\n",
      "[ 0.43132677  0.43132677  0.4274052  ...,  0.47110704  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.42348363  0.43524834  0.41956207 ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.43216602  0.42824445  0.43216602 ...,  0.48287175  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.43132677  0.43524834  0.43524834 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.43216602  0.43216602  0.43216602 ...,  0.45934234  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.41255818  0.41647974  0.42040131 ...,  0.44757763  0.44757763\n",
      "  0.4514992 ]\n",
      "[ 0.42824445  0.42432288  0.42432288 ...,  0.45093462  0.45485618\n",
      "  0.45485618]\n",
      "[ 0.42432288  0.42040131  0.42824445 ...,  0.45877775  0.45485618\n",
      "  0.45485618]\n",
      "[ 0.42040131  0.42040131  0.42824445 ...,  0.45877775  0.45877775\n",
      "  0.45877775]\n",
      "[ 0.42824445  0.42432288  0.42040131 ...,  0.45877775  0.45877775\n",
      "  0.45877775]\n",
      "[ 0.48679332  0.47895018  0.47895018 ...,  0.504448    0.51229114\n",
      "  0.52013428]\n",
      "[ 0.49855802  0.49463645  0.49071489 ...,  0.51903563  0.53080034\n",
      "  0.53080034]\n",
      "[ 0.50696574  0.50304417  0.49520104 ...,  0.52126345  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.50304417  0.50696574  0.50696574 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.51342031  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.50304417  0.51088731  0.50304417 ...,  0.52518502  0.52126345\n",
      "  0.51734188]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.51734188  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.4817731   0.47785153  0.47392996 ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.48961624  0.48569467  0.4935378  ...,  0.51734188  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.4817731   0.4817731   0.4817731  ...,  0.50949874  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.4817731   0.4817731   0.47785153 ...,  0.5016556   0.50949874\n",
      "  0.50557717]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.5016556   0.5016556\n",
      "  0.50557717]\n",
      "[ 0.4817731   0.4817731   0.4817731  ...,  0.50949874  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.48343633  0.49520104  0.49127947 ...,  0.5016556   0.5016556\n",
      "  0.50949874]\n",
      "[ 0.49520104  0.49127947  0.49520104 ...,  0.51734188  0.5016556\n",
      "  0.49773404]\n",
      "[ 0.47559319  0.47951476  0.48343633 ...,  0.51734188  0.50949874\n",
      "  0.49773404]\n",
      "[ 0.49912261  0.49912261  0.49127947 ...,  0.51342031  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.4873579   0.49127947  0.49520104 ...,  0.51734188  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.49127947  0.49127947  0.49520104 ...,  0.53302815  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.50696574  0.50304417  0.49912261 ...,  0.52126345  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.52518502  0.52126345\n",
      "  0.51342031]\n",
      "[ 0.50304417  0.50304417  0.50696574 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.49660487  0.49660487\n",
      "  0.49660487]\n",
      "[ 0.49127947  0.49127947  0.49912261 ...,  0.52126345  0.52518502\n",
      "  0.53302815]\n",
      "[ 0.48961624  0.48569467  0.48569467 ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.4817731   0.4817731   0.4817731  ...,  0.51342031  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.47785153  0.47785153  0.4817731  ...,  0.50836957  0.50109102\n",
      "  0.49716945]\n",
      "[ 0.4935378   0.4935378   0.4935378  ...,  0.50557717  0.50949874\n",
      "  0.50949874]\n",
      "[ 0.4935378   0.4935378   0.48961624 ...,  0.51342031  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.49127947  0.49127947  0.49127947 ...,  0.50557717  0.50557717\n",
      "  0.50949874]\n",
      "[ 0.49127947  0.49127947  0.49127947 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.48343633  0.4873579   0.49127947 ...,  0.51342031  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.47559319  0.48343633  0.50304417 ...,  0.51342031  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.48287175  0.47895018  0.47895018 ...,  0.50949874  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4935378   0.48569467  0.48569467 ...,  0.5016556   0.5016556\n",
      "  0.50557717]\n",
      "[ 0.52265202  0.51873045  0.51706722 ...,  0.53302815  0.53694972\n",
      "  0.54871443]\n",
      "[ 0.51088731  0.50696574  0.50696574 ...,  0.53694972  0.53694972\n",
      "  0.52910658]\n",
      "[ 0.51480888  0.51480888  0.51314565 ...,  0.53302815  0.53694972\n",
      "  0.54087129]\n",
      "[ 0.52491035  0.52098878  0.51706722 ...,  0.54087129  0.54087129\n",
      "  0.53302815]\n",
      "[ 0.49912261  0.49912261  0.49520104 ...,  0.53302815  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.49660487  0.49660487  0.504448  ]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.5229572   0.52687877\n",
      "  0.52491035]\n",
      "[ 0.48679332  0.49071489  0.49071489 ...,  0.51706722  0.52098878\n",
      "  0.51706722]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.50530251  0.50530251\n",
      "  0.50922408]\n",
      "[ 0.49071489  0.49071489  0.48679332 ...,  0.50922408  0.50922408\n",
      "  0.51314565]\n",
      "[ 0.44309148  0.45485618  0.45877775 ...,  0.47617304  0.47225147\n",
      "  0.47225147]\n",
      "[ 0.45485618  0.45877775  0.45877775 ...,  0.48287175  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.47502861  0.47110704  0.4632639  ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47110704  0.45934234  0.46718547 ...,  0.4873579   0.4873579\n",
      "  0.49127947]\n",
      "[ 0.47502861  0.47895018  0.47110704 ...,  0.49127947  0.4873579\n",
      "  0.49127947]\n",
      "[ 0.4632639   0.46718547  0.47502861 ...,  0.49127947  0.4873579   0.4873579 ]\n",
      "[ 0.45542077  0.45934234  0.46718547 ...,  0.4873579   0.48343633\n",
      "  0.48343633]\n",
      "[ 0.44757763  0.44757763  0.44757763 ...,  0.4817731   0.47785153\n",
      "  0.47785153]\n",
      "[ 0.44480049  0.44087892  0.44480049 ...,  0.4817731   0.4817731   0.4817731 ]\n",
      "[ 0.44365606  0.44365606  0.44757763 ...,  0.47392996  0.4817731\n",
      "  0.48961624]\n",
      "[ 0.44872206  0.45656519  0.45656519 ...,  0.47785153  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.44309148  0.44309148  0.44309148 ...,  0.47225147  0.47617304\n",
      "  0.48009461]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47785153  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.44701305  0.44701305  0.45093462 ...,  0.4817731   0.48961624\n",
      "  0.48569467]\n",
      "[ 0.46662089  0.45877775  0.45485618 ...,  0.4935378   0.48961624\n",
      "  0.48569467]\n",
      "[ 0.45093462  0.45093462  0.45485618 ...,  0.47785153  0.4817731\n",
      "  0.47785153]\n",
      "[ 0.44757763  0.44757763  0.4514992  ...,  0.4817731   0.48569467\n",
      "  0.48961624]\n",
      "[ 0.45093462  0.45093462  0.45485618 ...,  0.4817731   0.47785153\n",
      "  0.47392996]\n",
      "[ 0.46269932  0.45877775  0.45485618 ...,  0.48679332  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.46269932  0.45877775  0.45485618 ...,  0.4817731   0.48569467\n",
      "  0.48569467]\n",
      "[ 0.47054246  0.47446403  0.4783856  ...,  0.48401617  0.48793774\n",
      "  0.49185931]\n",
      "[ 0.44309148  0.45093462  0.45877775 ...,  0.48569467  0.48569467\n",
      "  0.48569467]\n",
      "[ 0.43132677  0.43524834  0.43524834 ...,  0.47617304  0.47617304\n",
      "  0.48009461]\n",
      "[ 0.46269932  0.46269932  0.46269932 ...,  0.47895018  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.4514992   0.44757763  0.44365606 ...,  0.48287175  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.4632639   0.45934234  0.4514992  ...,  0.48287175  0.48401617\n",
      "  0.48401617]\n",
      "[ 0.44757763  0.4514992   0.44757763 ...,  0.48009461  0.48009461\n",
      "  0.48009461]\n",
      "[ 0.44757763  0.4514992   0.45542077 ...,  0.48679332  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.45877775  0.45485618  0.45485618 ...,  0.47502861  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.45093462  0.45485618  0.46662089 ...,  0.4817731   0.4817731   0.4817731 ]\n",
      "[ 0.43916991  0.43916991  0.44309148 ...,  0.48569467  0.4817731   0.4817731 ]\n",
      "[ 0.45093462  0.44309148  0.46269932 ...,  0.47000839  0.47000839\n",
      "  0.47392996]\n",
      "[ 0.45093462  0.45093462  0.45093462 ...,  0.48569467  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.44309148  0.44309148  0.44309148 ...,  0.48401617  0.48009461\n",
      "  0.48009461]\n",
      "[ 0.45093462  0.45093462  0.45093462 ...,  0.48569467  0.48569467\n",
      "  0.48569467]\n",
      "[ 0.47054246  0.46269932  0.45877775 ...,  0.50138094  0.49745937\n",
      "  0.4935378 ]\n",
      "[ 0.47446403  0.47446403  0.47446403 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.47054246  0.47446403  0.47446403 ...,  0.48961624  0.4935378\n",
      "  0.49745937]\n",
      "[ 0.48230716  0.47446403  0.47054246 ...,  0.48961624  0.48961624\n",
      "  0.48569467]\n",
      "[ 0.4783856   0.4783856   0.47054246 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.45093462  0.45485618  0.45093462 ...,  0.4683299   0.47225147\n",
      "  0.47225147]\n",
      "[ 0.45485618  0.45485618  0.45485618 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.44701305  0.45093462  0.45485618 ...,  0.48679332  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.45485618  0.45485618  0.45485618 ...,  0.48679332  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.4274052   0.43916991  0.47054246 ...,  0.48679332  0.49463645\n",
      "  0.48287175]\n",
      "[ 0.49127947  0.49127947  0.49127947 ...,  0.51342031  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49520104  0.49912261  0.50304417 ...,  0.52910658  0.53302815\n",
      "  0.52910658]\n",
      "[ 0.4926833   0.50052644  0.50052644 ...,  0.52797742  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.53189899  0.53189899\n",
      "  0.53189899]\n",
      "[ 0.50836957  0.51229114  0.51229114 ...,  0.53582055  0.53189899\n",
      "  0.52797742]\n",
      "[ 0.50836957  0.504448    0.504448   ...,  0.51621271  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.52013428  0.51229114  0.51621271 ...,  0.51621271  0.52797742\n",
      "  0.52797742]\n",
      "[ 0.49381247  0.4898909   0.4898909  ...,  0.51734188  0.52910658\n",
      "  0.54087129]\n",
      "[ 0.4898909   0.48596933  0.4898909  ...,  0.52576486  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.49773404  0.4898909   0.48596933 ...,  0.52518502  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49381247  0.49381247  0.4898909  ...,  0.51734188  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.50304417  0.49520104  0.48343633 ...,  0.50557717  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.49381247  0.49381247  0.49381247 ...,  0.52518502  0.52126345\n",
      "  0.51734188]\n",
      "[ 0.50304417  0.50304417  0.49520104 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.49660487  0.48876173  0.4926833  ...,  0.51342031  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49127947  0.49127947  0.49912261 ...,  0.51342031  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.48484016  0.48876173  0.4926833  ...,  0.51734188  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49912261  0.49520104  0.49520104 ...,  0.52126345  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.52910658  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.50557717  0.50949874  0.50949874 ...,  0.53694972  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.50557717  0.50557717  0.5016556  ...,  0.52910658  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.5016556   0.5016556   0.50557717 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.4873579   0.4873579   0.4873579  ...,  0.52910658  0.52518502\n",
      "  0.51342031]\n",
      "[ 0.51706722  0.50922408  0.50922408 ...,  0.51342031  0.51342031\n",
      "  0.52126345]\n",
      "[ 0.4926833   0.4926833   0.4926833  ...,  0.51342031  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.48596933  0.49773404  0.5016556  ...,  0.51734188  0.52126345\n",
      "  0.52518502]\n",
      "[ 0.50304417  0.49912261  0.49127947 ...,  0.51342031  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.49660487  0.49660487  0.49660487 ...,  0.52518502  0.50949874\n",
      "  0.5016556 ]\n",
      "[ 0.49773404  0.5016556   0.50949874 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.5016556   0.5016556   0.5016556  ...,  0.51734188  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.49381247  0.49381247  0.49773404 ...,  0.51734188  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.52126345  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.49381247  0.49381247  0.49381247 ...,  0.51342031  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.4873579   0.49127947  0.49520104 ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.49773404  0.49773404  0.49773404 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.51706722  0.51314565  0.51314565 ...,  0.53302815  0.53302815\n",
      "  0.53694972]\n",
      "[ 0.50922408  0.50922408  0.50922408 ...,  0.53694972  0.53694972\n",
      "  0.54087129]\n",
      "[ 0.52491035  0.52098878  0.52098878 ...,  0.53302815  0.53694972\n",
      "  0.54087129]\n",
      "[ 0.52491035  0.52883192  0.52883192 ...,  0.552636    0.552636    0.552636  ]\n",
      "[ 0.52098878  0.52098878  0.52491035 ...,  0.53694972  0.53694972\n",
      "  0.53302815]\n",
      "[ 0.49127947  0.4873579   0.49127947 ...,  0.51734188  0.51734188\n",
      "  0.50949874]\n",
      "[ 0.49520104  0.49520104  0.49912261 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.51734188  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.50304417  0.49912261  0.49520104 ...,  0.51480888  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.49912261  0.49912261  0.50304417 ...,  0.51734188  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.46718547  0.47110704  0.47502861 ...,  0.49912261  0.49520104\n",
      "  0.50696574]\n",
      "[ 0.47617304  0.47225147  0.47617304 ...,  0.50304417  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.49520104  0.49912261  0.49912261 ...,  0.50836957  0.504448    0.504448  ]\n",
      "[ 0.50138094  0.50530251  0.50530251 ...,  0.50836957  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.49912261  0.49520104  0.49127947 ...,  0.52013428  0.51621271\n",
      "  0.50052644]\n",
      "[ 0.49520104  0.49127947  0.48343633 ...,  0.50052644  0.50052644\n",
      "  0.50836957]\n",
      "[ 0.49463645  0.49463645  0.49071489 ...,  0.504448    0.51229114\n",
      "  0.51621271]\n",
      "[ 0.47392996  0.47392996  0.47392996 ...,  0.49773404  0.49773404\n",
      "  0.50557717]\n",
      "[ 0.47785153  0.47392996  0.47785153 ...,  0.50949874  0.50557717\n",
      "  0.49381247]\n",
      "[ 0.48401617  0.47617304  0.47225147 ...,  0.49773404  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.46216526  0.46608682  0.47392996 ...,  0.5016556   0.49773404\n",
      "  0.49773404]\n",
      "[ 0.47110704  0.46718547  0.46718547 ...,  0.50304417  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47392996  0.47392996  0.47785153 ...,  0.5016556   0.5016556\n",
      "  0.49773404]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.50304417  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.47110704  0.47502861  0.47502861 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.46718547  0.46718547  0.47110704 ...,  0.50696574  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.47502861  0.47110704  0.47502861 ...,  0.51480888  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.49912261  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.48401617  0.48401617  0.48401617 ...,  0.50138094  0.50922408\n",
      "  0.50530251]\n",
      "[ 0.50362402  0.49970245  0.49578088 ...,  0.4935378   0.49745937\n",
      "  0.49745937]\n",
      "[ 0.49578088  0.49185931  0.48793774 ...,  0.50696574  0.50922408\n",
      "  0.50138094]\n",
      "[ 0.48793774  0.48401617  0.48793774 ...,  0.50530251  0.50922408\n",
      "  0.50922408]\n",
      "[ 0.47502861  0.47502861  0.47110704 ...,  0.50304417  0.49912261\n",
      "  0.49127947]\n",
      "[ 0.49970245  0.51931029  0.51538872 ...,  0.50922408  0.50922408\n",
      "  0.50922408]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.4935378   0.4935378   0.4935378 ]\n",
      "[ 0.4683299   0.47225147  0.47225147 ...,  0.50138094  0.49745937\n",
      "  0.50138094]\n",
      "[ 0.4683299   0.47225147  0.47617304 ...,  0.51873045  0.50530251\n",
      "  0.50530251]\n",
      "[ 0.47225147  0.47225147  0.47225147 ...,  0.49745937  0.49745937\n",
      "  0.4935378 ]\n",
      "[ 0.47225147  0.47225147  0.47225147 ...,  0.49745937  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.47225147  0.47225147  0.47617304 ...,  0.49745937  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.48793774  0.48793774  0.48009461 ...,  0.50530251  0.50530251\n",
      "  0.50922408]\n",
      "[ 0.47617304  0.48009461  0.48009461 ...,  0.50530251  0.50530251\n",
      "  0.50922408]\n",
      "[ 0.48009461  0.48009461  0.48009461 ...,  0.50922408  0.50138094\n",
      "  0.50138094]\n",
      "[ 0.4632639   0.46718547  0.48679332 ...,  0.49127947  0.4873579\n",
      "  0.49520104]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.50138094  0.50138094\n",
      "  0.50138094]\n",
      "[ 0.49970245  0.50362402  0.49970245 ...,  0.51314565  0.51314565\n",
      "  0.50922408]\n",
      "[ 0.49578088  0.49970245  0.49970245 ...,  0.50499733  0.50499733\n",
      "  0.51284047]\n",
      "[ 0.50362402  0.50362402  0.49578088 ...,  0.51314565  0.51314565\n",
      "  0.50922408]\n",
      "[ 0.49970245  0.50362402  0.50362402 ...,  0.50530251  0.51873045\n",
      "  0.52265202]\n",
      "[ 0.49578088  0.49970245  0.49970245 ...,  0.51480888  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.46718547  0.4632639   0.45934234 ...,  0.49127947  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.48401617  0.48401617  0.48401617 ...,  0.50696574  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.48009461  0.48009461  0.47617304 ...,  0.49912261  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.51873045  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.50696574  0.50696574\n",
      "  0.51088731]\n",
      "[ 0.44000916  0.44785229  0.45569543 ...,  0.47502861  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.45569543  0.45569543  0.459617   ...,  0.47502861  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.44785229  0.44393072  0.44785229 ...,  0.46521706  0.46129549\n",
      "  0.46129549]\n",
      "[ 0.44393072  0.44393072  0.44393072 ...,  0.46521706  0.46521706\n",
      "  0.4730602 ]\n",
      "[ 0.43608759  0.44393072  0.44785229 ...,  0.46913863  0.4730602\n",
      "  0.46913863]\n",
      "[ 0.44000916  0.46353857  0.44393072 ...,  0.46913863  0.46913863\n",
      "  0.4730602 ]\n",
      "[ 0.42824445  0.42824445  0.42824445 ...,  0.45737392  0.46129549\n",
      "  0.46521706]\n",
      "[ 0.45934234  0.45934234  0.4632639  ...,  0.46608682  0.47000839\n",
      "  0.47000839]\n",
      "[ 0.4514992   0.44757763  0.4514992  ...,  0.47000839  0.46608682\n",
      "  0.46216526]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47785153  0.47392996\n",
      "  0.47000839]\n",
      "[ 0.46718547  0.47110704  0.46718547 ...,  0.47000839  0.47392996\n",
      "  0.47392996]\n",
      "[ 0.46521706  0.43776608  0.44168765 ...,  0.46718547  0.46718547\n",
      "  0.47110704]\n",
      "[ 0.4514992   0.4514992   0.45542077 ...,  0.47785153  0.47000839\n",
      "  0.46608682]\n",
      "[ 0.45345235  0.45345235  0.45737392 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.44953079  0.44953079  0.44953079 ...,  0.47110704  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.45737392  0.45737392  0.45345235 ...,  0.47110704  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.45345235  0.44953079  0.44953079 ...,  0.48287175  0.49855802\n",
      "  0.49071489]\n",
      "[ 0.46913863  0.46521706  0.46129549 ...,  0.4632639   0.4632639\n",
      "  0.47110704]\n",
      "[ 0.44785229  0.44785229  0.45177386 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.45177386  0.45177386  0.44785229 ...,  0.47502861  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.44785229  0.44393072  0.44393072 ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.44393072  0.44785229  0.44785229 ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.43608759  0.44000916  0.44000916 ...,  0.4632639   0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.45737392  0.44953079  0.44560922 ...,  0.47110704  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.45737392  0.45737392  0.46129549 ...,  0.45934234  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44953079  0.44953079  0.44953079 ...,  0.47110704  0.48679332\n",
      "  0.47502861]\n",
      "[ 0.45345235  0.45737392  0.45345235 ...,  0.46718547  0.46718547\n",
      "  0.47110704]\n",
      "[ 0.44953079  0.44168765  0.44168765 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44168765  0.44560922  0.44560922 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.45934234  0.45934234  0.45934234 ...,  0.47895018  0.47502861\n",
      "  0.47110704]\n",
      "[ 0.44560922  0.44560922  0.44560922 ...,  0.47502861  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.44953079  0.44560922  0.44168765 ...,  0.47895018  0.4632639\n",
      "  0.46718547]\n",
      "[ 0.44393072  0.44393072  0.44785229 ...,  0.47895018  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47502861  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44785229  0.44393072  0.44393072 ...,  0.47502861  0.47502861\n",
      "  0.47110704]\n",
      "[ 0.45177386  0.45569543  0.45569543 ...,  0.47110704  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.48287175  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.44785229  0.44393072  0.44393072 ...,  0.49071489  0.49071489\n",
      "  0.48287175]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.47502861  0.47502861\n",
      "  0.48287175]\n",
      "[ 0.45569543  0.45177386  0.44785229 ...,  0.4632639   0.4632639\n",
      "  0.46718547]\n",
      "[ 0.46746014  0.4713817   0.4713817  ...,  0.48287175  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.45569543  0.459617    0.46353857 ...,  0.47895018  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.45569543  0.459617    0.459617   ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.46353857  0.46353857  0.46353857 ...,  0.47895018  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.43637751  0.43245594  0.43245594 ...,  0.44814221  0.44814221\n",
      "  0.44814221]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.45542077  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.4220798   0.43384451  0.42992294 ...,  0.42600137  0.42992294\n",
      "  0.43384451]\n",
      "[ 0.41423667  0.4103151   0.4103151  ...,  0.43776608  0.44168765\n",
      "  0.43776608]\n",
      "[ 0.41815824  0.41815824  0.41423667 ...,  0.42600137  0.42600137\n",
      "  0.42992294]\n",
      "[ 0.41423667  0.42600137  0.43384451 ...,  0.42992294  0.42992294\n",
      "  0.42992294]\n",
      "[ 0.4103151   0.4103151   0.4103151  ...,  0.43384451  0.42992294\n",
      "  0.42992294]\n",
      "[ 0.43863584  0.44255741  0.44255741 ...,  0.47028305  0.46243992\n",
      "  0.45459678]\n",
      "[ 0.44647898  0.43863584  0.44255741 ...,  0.45851835  0.45851835\n",
      "  0.45459678]\n",
      "[ 0.43863584  0.44647898  0.45432212 ...,  0.45851835  0.45851835\n",
      "  0.45851835]\n",
      "[ 0.45040055  0.45040055  0.45432212 ...,  0.46243992  0.46636149\n",
      "  0.47028305]\n",
      "[ 0.43245594  0.43637751  0.44029908 ...,  0.45990692  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.44647898  0.44255741  0.43471427 ...,  0.46243992  0.46636149\n",
      "  0.46636149]\n",
      "[ 0.45598535  0.46382849  0.45598535 ...,  0.46382849  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.45206378  0.45206378  0.45206378 ...,  0.46382849  0.45990692\n",
      "  0.45598535]\n",
      "[ 0.45598535  0.45990692  0.45206378 ...,  0.46382849  0.46382849\n",
      "  0.47559319]\n",
      "[ 0.45206378  0.45206378  0.45598535 ...,  0.47559319  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.45206378  0.45206378  0.45206378 ...,  0.45598535  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.41620508  0.41620508  0.41228351 ...,  0.4103151   0.4103151   0.4220798 ]\n",
      "[ 0.41620508  0.42012665  0.41620508 ...,  0.40639353  0.40639353\n",
      "  0.41423667]\n",
      "[ 0.41228351  0.40836194  0.40836194 ...,  0.4220798   0.41815824\n",
      "  0.4103151 ]\n",
      "[ 0.41228351  0.41228351  0.40836194 ...,  0.4103151   0.4220798   0.4220798 ]\n",
      "[ 0.44422065  0.44029908  0.44029908 ...,  0.45598535  0.45598535\n",
      "  0.45206378]\n",
      "[ 0.41815824  0.40639353  0.40247196 ...,  0.41423667  0.41423667\n",
      "  0.41423667]\n",
      "[ 0.44029908  0.43637751  0.43245594 ...,  0.45990692  0.47167163\n",
      "  0.47951476]\n",
      "[ 0.44422065  0.44422065  0.44422065 ...,  0.45598535  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.44814221  0.44814221  0.44814221 ...,  0.46382849  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.44814221  0.45598535  0.45598535 ...,  0.45990692  0.45598535\n",
      "  0.45598535]\n",
      "[ 0.44422065  0.44029908  0.44029908 ...,  0.45206378  0.44814221\n",
      "  0.45206378]\n",
      "[ 0.45598535  0.46382849  0.47167163 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.45598535  0.45206378  0.45206378 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.45598535  0.45990692  0.46382849 ...,  0.46775006  0.47167163\n",
      "  0.46382849]\n",
      "[ 0.45598535  0.45598535  0.45206378 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.43637751  0.44029908  0.44029908 ...,  0.45206378  0.45206378\n",
      "  0.45206378]\n",
      "[ 0.45598535  0.45206378  0.45206378 ...,  0.45598535  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.42404822  0.43189136  0.42404822 ...,  0.43384451  0.42600137\n",
      "  0.4220798 ]\n",
      "[ 0.4220798   0.4220798   0.41815824 ...,  0.44168765  0.44560922\n",
      "  0.44168765]\n",
      "[ 0.42012665  0.42012665  0.41228351 ...,  0.42992294  0.42992294\n",
      "  0.4220798 ]\n",
      "[ 0.41815824  0.41815824  0.41815824 ...,  0.42600137  0.4220798   0.4220798 ]\n",
      "[ 0.42404822  0.42404822  0.42404822 ...,  0.4220798   0.42600137\n",
      "  0.42992294]\n",
      "[ 0.43637751  0.42853437  0.43245594 ...,  0.44422065  0.44422065\n",
      "  0.44814221]\n",
      "[ 0.44365606  0.44365606  0.44365606 ...,  0.4632639   0.45934234\n",
      "  0.4514992 ]\n",
      "[ 0.44757763  0.44757763  0.44757763 ...,  0.45934234  0.45542077\n",
      "  0.45542077]\n",
      "[ 0.44757763  0.44757763  0.4514992  ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.44757763  0.4514992   0.4514992  ...,  0.46718547  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.53136492  0.52352178\n",
      "  0.51567864]\n",
      "[ 0.50615702  0.50615702  0.50615702 ...,  0.52184329  0.52184329\n",
      "  0.52576486]\n",
      "[ 0.48091859  0.47699702  0.47699702 ...,  0.48091859  0.48091859\n",
      "  0.48484016]\n",
      "[ 0.48091859  0.48091859  0.48484016 ...,  0.47389944  0.46997787\n",
      "  0.46605631]\n",
      "[ 0.50052644  0.49660487  0.4926833  ...,  0.4926833   0.4926833\n",
      "  0.48876173]\n",
      "[ 0.47699702  0.47699702  0.47699702 ...,  0.48484016  0.47699702\n",
      "  0.47699702]\n",
      "[ 0.49660487  0.49660487  0.49660487 ...,  0.48091859  0.48484016\n",
      "  0.48484016]\n",
      "[ 0.52941176  0.52941176  0.52941176 ...,  0.52744335  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.5254902   0.52156863  0.5254902  ...,  0.52744335  0.52352178\n",
      "  0.52744335]\n",
      "[ 0.5372549   0.52941176  0.52941176 ...,  0.52744335  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.52941176  0.52941176  0.5254902  ...,  0.52352178  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.51792172  0.51792172  0.52184329 ...,  0.52352178  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.52941176  0.52941176  0.53333333 ...,  0.52744335  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.52744335  0.51960021  0.51960021 ...,  0.52744335  0.52744335\n",
      "  0.52352178]\n",
      "[ 0.52352178  0.52352178  0.52352178 ...,  0.53136492  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.51960021  0.51960021\n",
      "  0.52352178]\n",
      "[ 0.53136492  0.53136492  0.53528649 ...,  0.52352178  0.52352178\n",
      "  0.53136492]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.52352178  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.48091859  0.48091859  0.47699702 ...,  0.48343633  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.47699702  0.47307546  0.47699702 ...,  0.46775006  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.48091859  0.47699702  0.48484016 ...,  0.47559319  0.47167163\n",
      "  0.46775006]\n",
      "[ 0.47699702  0.47699702  0.48091859 ...,  0.47951476  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.51960021  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.47699702  0.47307546  0.48876173 ...,  0.48343633  0.4873579\n",
      "  0.47951476]\n",
      "[ 0.52352178  0.51960021  0.51960021 ...,  0.52744335  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52744335  0.52744335\n",
      "  0.52352178]\n",
      "[ 0.52744335  0.52352178  0.52352178 ...,  0.52744335  0.52744335\n",
      "  0.52352178]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.53528649  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.51400015  0.52184329  0.52968643 ...,  0.51960021  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53528649  0.53136492  0.52744335 ...,  0.52744335  0.52352178\n",
      "  0.51960021]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.52352178  0.52744335\n",
      "  0.53528649]\n",
      "[ 0.53136492  0.53136492  0.53528649 ...,  0.51960021  0.51960021\n",
      "  0.51567864]\n",
      "[ 0.52576486  0.52576486  0.52184329 ...,  0.51567864  0.52352178\n",
      "  0.52352178]\n",
      "[ 0.51175708  0.50783551  0.53136492 ...,  0.52744335  0.52744335\n",
      "  0.52352178]\n",
      "[ 0.48484016  0.48484016  0.48484016 ...,  0.48091859  0.48091859\n",
      "  0.48091859]\n",
      "[ 0.49660487  0.49660487  0.49660487 ...,  0.48876173  0.48484016\n",
      "  0.48091859]\n",
      "[ 0.49660487  0.4926833   0.48876173 ...,  0.48876173  0.4926833   0.4926833 ]\n",
      "[ 0.49660487  0.48876173  0.49660487 ...,  0.48091859  0.48091859\n",
      "  0.48091859]\n",
      "[ 0.48484016  0.48484016  0.48876173 ...,  0.4926833   0.48876173\n",
      "  0.48484016]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.52352178  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.52184329  0.52184329  0.52576486 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52184329  0.52184329\n",
      "  0.52576486]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.52184329  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.51400015  0.51007858  0.51007858 ...,  0.52184329  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.46915389  0.46915389  0.47307546 ...,  0.47812619  0.47812619\n",
      "  0.47812619]\n",
      "[ 0.46775006  0.47559319  0.47167163 ...,  0.48569467  0.48569467\n",
      "  0.48569467]\n",
      "[ 0.45401694  0.4461738   0.44225223 ...,  0.4461738   0.45401694\n",
      "  0.45009537]\n",
      "[ 0.4461738   0.4461738   0.4461738  ...,  0.45793851  0.45793851\n",
      "  0.45401694]\n",
      "[ 0.4461738   0.4461738   0.4461738  ...,  0.4461738   0.4461738\n",
      "  0.43833066]\n",
      "[ 0.43833066  0.44225223  0.44225223 ...,  0.45793851  0.45401694\n",
      "  0.45009537]\n",
      "[ 0.44225223  0.44225223  0.44225223 ...,  0.45401694  0.45793851\n",
      "  0.46186007]\n",
      "[ 0.47420462  0.47028305  0.47420462 ...,  0.4826276   0.4826276\n",
      "  0.49831388]\n",
      "[ 0.47812619  0.48596933  0.48204776 ...,  0.49439231  0.48654917\n",
      "  0.47870603]\n",
      "[ 0.48204776  0.48204776  0.47812619 ...,  0.49047074  0.49047074\n",
      "  0.48654917]\n",
      "[ 0.48204776  0.47812619  0.47812619 ...,  0.4708629   0.47478447\n",
      "  0.4826276 ]\n",
      "[ 0.46915389  0.46915389  0.46915389 ...,  0.47812619  0.48204776\n",
      "  0.48596933]\n",
      "[ 0.47420462  0.47028305  0.47812619 ...,  0.48654917  0.48654917\n",
      "  0.49047074]\n",
      "[ 0.47420462  0.47028305  0.48596933 ...,  0.48654917  0.49047074\n",
      "  0.49047074]\n",
      "[ 0.47812619  0.48204776  0.48204776 ...,  0.4898909   0.4898909   0.5016556 ]\n",
      "[ 0.48204776  0.48204776  0.47028305 ...,  0.49831388  0.49439231\n",
      "  0.49439231]\n",
      "[ 0.47812619  0.47812619  0.48204776 ...,  0.49439231  0.50223545\n",
      "  0.49047074]\n",
      "[ 0.47812619  0.47812619  0.47420462 ...,  0.49381247  0.49439231\n",
      "  0.49439231]\n",
      "[ 0.43776608  0.44560922  0.44560922 ...,  0.44365606  0.44365606\n",
      "  0.44365606]\n",
      "[ 0.44168765  0.43776608  0.43776608 ...,  0.44757763  0.44757763\n",
      "  0.44757763]\n",
      "[ 0.43776608  0.43384451  0.43384451 ...,  0.44365606  0.44365606\n",
      "  0.44365606]\n",
      "[ 0.44168765  0.44168765  0.44168765 ...,  0.44365606  0.43973449\n",
      "  0.43973449]\n",
      "[ 0.46915389  0.46523232  0.45738918 ...,  0.47420462  0.46636149\n",
      "  0.47420462]\n",
      "[ 0.44560922  0.43384451  0.44168765 ...,  0.44757763  0.44757763\n",
      "  0.44365606]\n",
      "[ 0.47420462  0.47420462  0.47812619 ...,  0.47812619  0.48596933\n",
      "  0.48204776]\n",
      "[ 0.47699702  0.46915389  0.46523232 ...,  0.4898909   0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.48596933  0.47812619  0.47028305 ...,  0.48204776  0.48204776\n",
      "  0.4898909 ]\n",
      "[ 0.47420462  0.48204776  0.47812619 ...,  0.49381247  0.49381247\n",
      "  0.49381247]\n",
      "[ 0.47812619  0.47420462  0.47420462 ...,  0.49773404  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.48204776  0.48596933  0.48204776 ...,  0.49047074  0.49047074\n",
      "  0.49439231]\n",
      "[ 0.49381247  0.47812619  0.47420462 ...,  0.48654917  0.48654917\n",
      "  0.48654917]\n",
      "[ 0.47420462  0.47420462  0.47812619 ...,  0.49831388  0.49439231\n",
      "  0.49439231]\n",
      "[ 0.48204776  0.48204776  0.48204776 ...,  0.49831388  0.50615702\n",
      "  0.51007858]\n",
      "[ 0.46915389  0.46915389  0.47699702 ...,  0.47812619  0.48204776\n",
      "  0.48204776]\n",
      "[ 0.49381247  0.48204776  0.47028305 ...,  0.49831388  0.49831388\n",
      "  0.49831388]\n",
      "[ 0.45737392  0.46129549  0.45345235 ...,  0.45990692  0.45598535\n",
      "  0.45206378]\n",
      "[ 0.45542077  0.45934234  0.45542077 ...,  0.45401694  0.45401694\n",
      "  0.46186007]\n",
      "[ 0.4514992   0.45934234  0.45934234 ...,  0.45990692  0.45990692\n",
      "  0.45990692]\n",
      "[ 0.44953079  0.44953079  0.44953079 ...,  0.44422065  0.45793851\n",
      "  0.45401694]\n",
      "[ 0.44168765  0.44168765  0.44168765 ...,  0.44365606  0.44365606\n",
      "  0.4514992 ]\n",
      "[ 0.45990692  0.45990692  0.46382849 ...,  0.48596933  0.4898909   0.4898909 ]\n",
      "[ 0.47167163  0.47559319  0.48343633 ...,  0.49773404  0.5016556\n",
      "  0.50557717]\n",
      "[ 0.47167163  0.47559319  0.47559319 ...,  0.48204776  0.48204776\n",
      "  0.47812619]\n",
      "[ 0.47167163  0.47559319  0.47167163 ...,  0.4898909   0.4898909\n",
      "  0.49381247]\n",
      "[ 0.47951476  0.47951476  0.47559319 ...,  0.48596933  0.4898909\n",
      "  0.49381247]\n",
      "[ 0.53136492  0.53136492  0.53136492 ...,  0.52910658  0.52518502\n",
      "  0.52910658]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.48932631  0.50109102  0.50893416 ...,  0.48876173  0.47699702\n",
      "  0.48091859]\n",
      "[ 0.50109102  0.50501259  0.50501259 ...,  0.48091859  0.48091859\n",
      "  0.48091859]\n",
      "[ 0.49716945  0.49324788  0.49324788 ...,  0.48484016  0.48091859\n",
      "  0.47699702]\n",
      "[ 0.49324788  0.49716945  0.49716945 ...,  0.48876173  0.4926833\n",
      "  0.48876173]\n",
      "[ 0.50109102  0.49716945  0.49324788 ...,  0.48484016  0.48091859\n",
      "  0.48484016]\n",
      "[ 0.53528649  0.53528649  0.52744335 ...,  0.51960021  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.53920806  0.53528649  0.56273747 ...,  0.53920806  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.53333333  0.52941176  0.53136492 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.54705119  0.53528649  0.53528649 ...,  0.52744335  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.52352178  0.52352178  0.52744335 ...,  0.52518502  0.52126345\n",
      "  0.52910658]\n",
      "[ 0.52352178  0.52744335  0.52744335 ...,  0.5254902   0.52156863\n",
      "  0.52941176]\n",
      "[ 0.52744335  0.52744335  0.53136492 ...,  0.52744335  0.52744335\n",
      "  0.53136492]\n",
      "[ 0.53528649  0.53920806  0.53528649 ...,  0.52352178  0.51960021\n",
      "  0.52744335]\n",
      "[ 0.53920806  0.53920806  0.54312963 ...,  0.53528649  0.53528649\n",
      "  0.52352178]\n",
      "[ 0.53136492  0.53136492  0.53136492 ...,  0.54312963  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.52744335  0.52744335\n",
      "  0.53136492]\n",
      "[ 0.48484016  0.48876173  0.48876173 ...,  0.48484016  0.48091859\n",
      "  0.47699702]\n",
      "[ 0.4926833   0.49660487  0.49660487 ...,  0.48091859  0.47699702\n",
      "  0.47699702]\n",
      "[ 0.4926833   0.4926833   0.49660487 ...,  0.47699702  0.46915389\n",
      "  0.46915389]\n",
      "[ 0.4926833   0.4926833   0.48091859 ...,  0.48484016  0.48484016\n",
      "  0.48484016]\n",
      "[ 0.53136492  0.52744335  0.52744335 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.48876173  0.48876173  0.48876173 ...,  0.48091859  0.48091859\n",
      "  0.48091859]\n",
      "[ 0.54312963  0.54705119  0.54312963 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53136492  0.53136492  0.53136492 ...,  0.51960021  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.53528649  0.53136492  0.53136492 ...,  0.53136492  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.53920806  0.53136492  0.53920806 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53528649  0.53920806  0.53136492 ...,  0.53136492  0.53136492\n",
      "  0.52744335]\n",
      "[ 0.52744335  0.53528649  0.53528649 ...,  0.53920806  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52744335  0.53528649  0.53528649 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53136492  0.53528649  0.53920806 ...,  0.53528649  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.52352178  0.52744335  0.53528649 ...,  0.51960021  0.51960021\n",
      "  0.52352178]\n",
      "[ 0.53920806  0.53528649  0.52744335 ...,  0.52744335  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.49716945  0.49324788  0.49324788 ...,  0.48876173  0.4926833   0.4926833 ]\n",
      "[ 0.48932631  0.48932631  0.48540475 ...,  0.48876173  0.47699702\n",
      "  0.47699702]\n",
      "[ 0.50109102  0.49324788  0.48932631 ...,  0.4926833   0.4926833   0.4926833 ]\n",
      "[ 0.49716945  0.50109102  0.50109102 ...,  0.4926833   0.49660487\n",
      "  0.4926833 ]\n",
      "[ 0.48148318  0.48932631  0.49324788 ...,  0.4926833   0.4926833   0.4926833 ]\n",
      "[ 0.53136492  0.53136492  0.52744335 ...,  0.53528649  0.53136492\n",
      "  0.52744335]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.49773404  0.49773404  0.49439231 ...,  0.52576486  0.52576486\n",
      "  0.51792172]\n",
      "[ 0.5016556   0.50949874  0.50949874 ...,  0.51342031  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.46775006  0.46382849  0.47307546 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.46775006  0.46775006  0.46915389 ...,  0.47167163  0.47559319\n",
      "  0.47951476]\n",
      "[ 0.46523232  0.46523232  0.46915389 ...,  0.47307546  0.48876173\n",
      "  0.46915389]\n",
      "[ 0.45738918  0.46131075  0.46523232 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.47559319  0.47559319  0.47167163 ...,  0.4873579   0.47559319\n",
      "  0.47559319]\n",
      "[ 0.50531777  0.50923934  0.50531777 ...,  0.50923934  0.51316091\n",
      "  0.51708248]\n",
      "[ 0.51316091  0.50531777  0.49747463 ...,  0.52492561  0.52492561\n",
      "  0.52100404]\n",
      "[ 0.5013962   0.50531777  0.50923934 ...,  0.51316091  0.51316091\n",
      "  0.50923934]\n",
      "[ 0.49747463  0.50531777  0.50923934 ...,  0.51708248  0.51708248\n",
      "  0.51316091]\n",
      "[ 0.49773404  0.49773404  0.49381247 ...,  0.50223545  0.49831388\n",
      "  0.49831388]\n",
      "[ 0.50223545  0.50223545  0.50615702 ...,  0.51708248  0.51708248\n",
      "  0.51316091]\n",
      "[ 0.51792172  0.51400015  0.51007858 ...,  0.51792172  0.51007858\n",
      "  0.51007858]\n",
      "[ 0.51400015  0.51400015  0.51400015 ...,  0.52184329  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.51007858  0.51007858  0.51007858 ...,  0.51400015  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.50223545  0.51007858  0.51007858 ...,  0.51792172  0.51007858\n",
      "  0.51400015]\n",
      "[ 0.51007858  0.51400015  0.51007858 ...,  0.51400015  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.46775006  0.45990692  0.45990692 ...,  0.46775006  0.46775006\n",
      "  0.46382849]\n",
      "[ 0.46382849  0.46382849  0.45990692 ...,  0.46775006  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.46382849  0.46775006  0.46775006 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.45990692  0.45990692  0.46382849 ...,  0.46775006  0.46382849\n",
      "  0.46382849]\n",
      "[ 0.49773404  0.49773404  0.50615702 ...,  0.49439231  0.49831388\n",
      "  0.51007858]\n",
      "[ 0.46775006  0.47167163  0.47167163 ...,  0.47167163  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.50557717  0.50557717  0.51342031 ...,  0.52184329  0.52184329\n",
      "  0.51400015]\n",
      "[ 0.51007858  0.49831388  0.50223545 ...,  0.50223545  0.50223545\n",
      "  0.50615702]\n",
      "[ 0.49773404  0.49381247  0.50557717 ...,  0.51007858  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.50615702  0.50615702  0.50223545 ...,  0.50615702  0.50223545\n",
      "  0.49439231]\n",
      "[ 0.50557717  0.50557717  0.50557717 ...,  0.50615702  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.51007858  0.51007858  0.51400015 ...,  0.51792172  0.51007858\n",
      "  0.50615702]\n",
      "[ 0.51007858  0.51400015  0.51792172 ...,  0.51792172  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.51400015  0.51007858  0.50615702 ...,  0.52576486  0.52184329\n",
      "  0.51792172]\n",
      "[ 0.51400015  0.51792172  0.52184329 ...,  0.51400015  0.51400015\n",
      "  0.51400015]\n",
      "[ 0.49439231  0.50223545  0.50615702 ...,  0.50223545  0.50223545\n",
      "  0.50223545]\n",
      "[ 0.51400015  0.51400015  0.51400015 ...,  0.52184329  0.52576486\n",
      "  0.52968643]\n",
      "[ 0.47167163  0.47559319  0.47167163 ...,  0.47559319  0.47951476\n",
      "  0.47559319]\n",
      "[ 0.47167163  0.47559319  0.47951476 ...,  0.47559319  0.47559319\n",
      "  0.47167163]\n",
      "[ 0.47167163  0.46775006  0.46775006 ...,  0.48343633  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.46775006  0.45598535  0.47167163 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.45990692  0.46382849  0.46382849 ...,  0.47951476  0.47951476\n",
      "  0.48343633]\n",
      "[ 0.50557717  0.49773404  0.49773404 ...,  0.50223545  0.50223545\n",
      "  0.50223545]\n",
      "[ 0.5016556   0.5016556   0.50557717 ...,  0.50949874  0.51342031\n",
      "  0.52126345]\n",
      "[ 0.49773404  0.50557717  0.51342031 ...,  0.51342031  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.5016556   0.5016556   0.50557717 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.51734188  0.51342031\n",
      "  0.52518502]\n",
      "[ 0.52013428  0.52013428  0.52013428 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52518502  0.53189899  0.52797742 ...,  0.53694972  0.53302815\n",
      "  0.52910658]\n",
      "[ 0.528542    0.528542    0.53246357 ...,  0.54814984  0.54814984\n",
      "  0.54814984]\n",
      "[ 0.52462043  0.52462043  0.52462043 ...,  0.54422827  0.54422827\n",
      "  0.54814984]\n",
      "[ 0.53246357  0.53246357  0.53246357 ...,  0.54030671  0.54422827\n",
      "  0.54422827]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.5453727   0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53638514  0.53246357  0.53246357 ...,  0.54030671  0.54030671\n",
      "  0.54030671]\n",
      "[ 0.52576486  0.52576486  0.52184329 ...,  0.52884718  0.52884718\n",
      "  0.52492561]\n",
      "[ 0.52576486  0.52576486  0.52184329 ...,  0.53276875  0.53276875\n",
      "  0.53669032]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52884718  0.52884718\n",
      "  0.52884718]\n",
      "[ 0.52576486  0.52576486  0.52184329 ...,  0.52884718  0.53276875\n",
      "  0.53276875]\n",
      "[ 0.52797742  0.52797742  0.52405585 ...,  0.54929427  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.52576486  0.52968643  0.52576486 ...,  0.53276875  0.53276875\n",
      "  0.53276875]\n",
      "[ 0.52797742  0.52405585  0.52013428 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52797742  0.52405585  0.52013428 ...,  0.52910658  0.52518502\n",
      "  0.52126345]\n",
      "[ 0.52518502  0.51734188  0.50949874 ...,  0.52910658  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52013428  0.51621271  0.51621271 ...,  0.52910658  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.52797742  0.52405585  0.51621271 ...,  0.52910658  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.52910658  0.52910658  0.53694972 ...,  0.5453727   0.54929427\n",
      "  0.54929427]\n",
      "[ 0.52518502  0.52910658  0.53302815 ...,  0.53752956  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.52518502  0.52518502  0.52518502 ...,  0.53752956  0.53752956\n",
      "  0.5453727 ]\n",
      "[ 0.52518502  0.53302815  0.54087129 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.50836957  0.50836957  0.51621271 ...,  0.51734188  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.52126345  0.52126345  0.52518502 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.52518502  0.52910658  0.52126345 ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52518502  0.53302815  0.53694972 ...,  0.52968643  0.533608    0.533608  ]\n",
      "[ 0.52126345  0.52518502  0.52518502 ...,  0.533608    0.533608    0.54145113]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.53752956  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.52126345  0.52126345  0.52910658 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.51734188  0.51342031  0.50949874 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52518502  0.52126345  0.52126345 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.52797742  0.52405585  0.53189899 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.52910658  0.53302815  0.53302815 ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.56327153  0.55934997  0.54758526 ...,  0.54929427  0.54929427\n",
      "  0.5453727 ]\n",
      "[ 0.53582055  0.53974212  0.53974212 ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.54758526  0.55150683  0.54758526 ...,  0.55321584  0.55321584\n",
      "  0.55321584]\n",
      "[ 0.54758526  0.54366369  0.54366369 ...,  0.55713741  0.55713741\n",
      "  0.56498054]\n",
      "[ 0.55150683  0.54758526  0.54758526 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52013428  0.51621271  0.52013428 ...,  0.53694972  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.53694972  0.52910658\n",
      "  0.52126345]\n",
      "[ 0.50836957  0.50836957  0.51229114 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.52797742  0.52797742  0.52405585 ...,  0.54479286  0.54479286\n",
      "  0.53694972]\n",
      "[ 0.49463645  0.49855802  0.50247959 ...,  0.48874647  0.48874647\n",
      "  0.48874647]\n",
      "[ 0.49799344  0.49799344  0.49799344 ...,  0.49799344  0.49799344\n",
      "  0.4901503 ]\n",
      "[ 0.52265202  0.51873045  0.51873045 ...,  0.52011902  0.50835431\n",
      "  0.50835431]\n",
      "[ 0.51284047  0.5089189   0.5089189  ...,  0.50051118  0.50051118\n",
      "  0.50051118]\n",
      "[ 0.52265202  0.52265202  0.51873045 ...,  0.49266804  0.50443275\n",
      "  0.50051118]\n",
      "[ 0.51480888  0.51480888  0.5142443  ...,  0.50835431  0.50835431\n",
      "  0.50835431]\n",
      "[ 0.51873045  0.51873045  0.52265202 ...,  0.51227588  0.50835431\n",
      "  0.50835431]\n",
      "[ 0.50362402  0.49970245  0.50362402 ...,  0.51032273  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.49578088  0.49578088  0.49578088 ...,  0.49855802  0.50640116\n",
      "  0.50640116]\n",
      "[ 0.49185931  0.49185931  0.49970245 ...,  0.50247959  0.51032273\n",
      "  0.50640116]\n",
      "[ 0.49970245  0.49970245  0.50362402 ...,  0.49071489  0.49071489\n",
      "  0.49463645]\n",
      "[ 0.48287175  0.48287175  0.49855802 ...,  0.48874647  0.49266804\n",
      "  0.50443275]\n",
      "[ 0.50922408  0.51314565  0.51314565 ...,  0.49463645  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.49799344  0.49407187  0.4901503  ...,  0.49491112  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.49071489  0.48679332  0.48287175 ...,  0.49491112  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.49883268  0.49491112  0.49098955 ...,  0.49098955  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.49407187  0.49799344  0.49799344 ...,  0.48314641  0.48706798\n",
      "  0.48314641]\n",
      "[ 0.50640116  0.50640116  0.50247959 ...,  0.50191501  0.50191501\n",
      "  0.49407187]\n",
      "[ 0.52208743  0.5142443   0.50640116 ...,  0.50640116  0.51032273\n",
      "  0.51032273]\n",
      "[ 0.5142443   0.50640116  0.50640116 ...,  0.50640116  0.49855802\n",
      "  0.49855802]\n",
      "[ 0.5142443   0.5142443   0.5142443  ...,  0.5142443   0.51032273\n",
      "  0.50640116]\n",
      "[ 0.5142443   0.5142443   0.5142443  ...,  0.51816587  0.51816587\n",
      "  0.5142443 ]\n",
      "[ 0.49855802  0.49463645  0.49071489 ...,  0.49266804  0.49658961\n",
      "  0.49658961]\n",
      "[ 0.51816587  0.51816587  0.51816587 ...,  0.52208743  0.5142443\n",
      "  0.51032273]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.49071489  0.50247959  0.5142443  ...,  0.49463645  0.49071489\n",
      "  0.48679332]\n",
      "[ 0.50640116  0.50640116  0.50247959 ...,  0.48679332  0.49855802\n",
      "  0.5142443 ]\n",
      "[ 0.50640116  0.51032273  0.50640116 ...,  0.49407187  0.49407187\n",
      "  0.49407187]\n",
      "[ 0.51032273  0.50640116  0.49855802 ...,  0.49463645  0.49463645\n",
      "  0.49855802]\n",
      "[ 0.50640116  0.51032273  0.49855802 ...,  0.49520104  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.50247959  0.50640116  0.51032273 ...,  0.49463645  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.50247959  0.50640116  0.49855802 ...,  0.49463645  0.48287175\n",
      "  0.48679332]\n",
      "[ 0.49071489  0.49463645  0.49855802 ...,  0.4901503   0.4901503   0.4901503 ]\n",
      "[ 0.50247959  0.50247959  0.50247959 ...,  0.50275425  0.50667582\n",
      "  0.50275425]\n",
      "[ 0.50247959  0.50247959  0.49855802 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.52208743  0.526009    0.526009   ...,  0.5142443   0.5142443   0.5142443 ]\n",
      "[ 0.5142443   0.51816587  0.51816587 ...,  0.5142443   0.52208743  0.526009  ]\n",
      "[ 0.52208743  0.526009    0.526009   ...,  0.50640116  0.5142443   0.5142443 ]\n",
      "[ 0.52208743  0.52208743  0.51816587 ...,  0.52208743  0.51816587\n",
      "  0.5142443 ]\n",
      "[ 0.526009    0.526009    0.526009   ...,  0.52208743  0.526009    0.526009  ]\n",
      "[ 0.49463645  0.49463645  0.49463645 ...,  0.48874647  0.48874647\n",
      "  0.48874647]\n",
      "[ 0.50583658  0.50975814  0.49407187 ...,  0.49799344  0.49799344\n",
      "  0.50191501]\n",
      "[ 0.4901503   0.4901503   0.49799344 ...,  0.50191501  0.50191501\n",
      "  0.50191501]\n",
      "[ 0.49799344  0.49407187  0.50583658 ...,  0.49799344  0.49799344\n",
      "  0.49799344]\n",
      "[ 0.50191501  0.49799344  0.49407187 ...,  0.49799344  0.49799344\n",
      "  0.49407187]\n",
      "[ 0.52405585  0.52405585  0.52126345 ...,  0.52405585  0.52405585\n",
      "  0.52405585]\n",
      "[ 0.51314565  0.52098878  0.51706722 ...,  0.53049516  0.53441672\n",
      "  0.53049516]\n",
      "[ 0.52797742  0.52797742  0.53189899 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53582055  0.52797742  0.52405585 ...,  0.528542    0.528542    0.52462043]\n",
      "[ 0.53582055  0.53582055  0.53974212 ...,  0.54087129  0.54087129\n",
      "  0.54087129]\n",
      "[ 0.53582055  0.53582055  0.53582055 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52518502  0.52518502  0.52405585 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.51400015  0.51792172\n",
      "  0.52968643]\n",
      "[ 0.52576486  0.52184329  0.52184329 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.53752956  0.51792172  0.533608  ]\n",
      "[ 0.53302815  0.52910658  0.52518502 ...,  0.52968643  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.51734188  0.52126345  0.52518502 ...,  0.51621271  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.52518502  0.52518502  0.52910658 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.51229114  0.51621271  0.51621271 ...,  0.52405585  0.52013428\n",
      "  0.51621271]\n",
      "[ 0.51621271  0.51621271  0.52013428 ...,  0.52518502  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.51229114  0.53441672  0.52657359 ...,  0.52013428  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.52013428  0.52405585  0.52405585 ...,  0.52013428  0.52013428\n",
      "  0.51229114]\n",
      "[ 0.50949874  0.51229114  0.52013428 ...,  0.51229114  0.51621271\n",
      "  0.52013428]\n",
      "[ 0.53302815  0.53302815  0.52910658 ...,  0.52797742  0.54479286\n",
      "  0.54479286]\n",
      "[ 0.53694972  0.53302815  0.52910658 ...,  0.52797742  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.53302815  0.53302815  0.53302815 ...,  0.54366369  0.54479286\n",
      "  0.54479286]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.53302815  0.52910658\n",
      "  0.53302815]\n",
      "[ 0.51621271  0.52013428  0.51342031 ...,  0.52013428  0.52405585\n",
      "  0.52797742]\n",
      "[ 0.53694972  0.53302815  0.53302815 ...,  0.53582055  0.53189899\n",
      "  0.52405585]\n",
      "[ 0.51734188  0.51734188  0.52126345 ...,  0.52518502  0.52910658\n",
      "  0.53302815]\n",
      "[ 0.52910658  0.52518502  0.52518502 ...,  0.53302815  0.53302815\n",
      "  0.52910658]\n",
      "[ 0.52518502  0.52518502  0.51734188 ...,  0.52518502  0.52518502\n",
      "  0.52910658]\n",
      "[ 0.52518502  0.52518502  0.52910658 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.51734188  0.51342031  0.50949874 ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.54087129  0.53694972\n",
      "  0.53302815]\n",
      "[ 0.50949874  0.51342031  0.51734188 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.52126345  0.52518502  0.52126345 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.52013428  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.52126345  0.51734188  0.51734188 ...,  0.51734188  0.51342031\n",
      "  0.52126345]\n",
      "[ 0.54366369  0.54366369  0.54366369 ...,  0.53582055  0.52797742\n",
      "  0.52405585]\n",
      "[ 0.54366369  0.54366369  0.54366369 ...,  0.54366369  0.54366369\n",
      "  0.54758526]\n",
      "[ 0.53582055  0.53189899  0.52797742 ...,  0.53974212  0.53974212\n",
      "  0.53974212]\n",
      "[ 0.55150683  0.54758526  0.54366369 ...,  0.53974212  0.53974212\n",
      "  0.53974212]\n",
      "[ 0.54758526  0.54366369  0.54758526 ...,  0.53974212  0.54366369\n",
      "  0.55150683]\n",
      "[ 0.51621271  0.51621271  0.51229114 ...,  0.52013428  0.52013428\n",
      "  0.51621271]\n",
      "[ 0.51314565  0.51706722  0.52098878 ...,  0.52657359  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.52657359  0.52265202  0.52265202 ...,  0.53441672  0.52657359\n",
      "  0.52265202]\n",
      "[ 0.52265202  0.52265202  0.52265202 ...,  0.52657359  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.52098878  0.51088731  0.51480888 ...,  0.52657359  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.47866026  0.47866026  0.47866026 ...,  0.47473869  0.47473869\n",
      "  0.47473869]\n",
      "[ 0.48258183  0.47473869  0.47473869 ...,  0.47081712  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.49098955  0.49491112  0.49491112 ...,  0.4713817   0.46746014\n",
      "  0.47530327]\n",
      "[ 0.49883268  0.49491112  0.49098955 ...,  0.48314641  0.47530327\n",
      "  0.4713817 ]\n",
      "[ 0.49098955  0.49098955  0.49098955 ...,  0.49491112  0.47530327\n",
      "  0.47922484]\n",
      "[ 0.48706798  0.48706798  0.48706798 ...,  0.49098955  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.49098955  0.49491112  0.49491112 ...,  0.47922484  0.49098955\n",
      "  0.47922484]\n",
      "[ 0.47054246  0.47446403  0.47054246 ...,  0.4783856   0.4783856\n",
      "  0.47054246]\n",
      "[ 0.48622873  0.48622873  0.4783856  ...,  0.48230716  0.47446403\n",
      "  0.47054246]\n",
      "[ 0.48230716  0.48230716  0.47446403 ...,  0.47054246  0.47446403\n",
      "  0.4783856 ]\n",
      "[ 0.48622873  0.48230716  0.48230716 ...,  0.47054246  0.46662089\n",
      "  0.46269932]\n",
      "[ 0.4865034   0.49042496  0.4865034  ...,  0.47081712  0.46297398\n",
      "  0.46689555]\n",
      "[ 0.48230716  0.48230716  0.48230716 ...,  0.46269932  0.46662089\n",
      "  0.47054246]\n",
      "[ 0.46689555  0.47473869  0.46297398 ...,  0.46689555  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.46689555  0.46689555  0.47473869 ...,  0.45790799  0.46182956\n",
      "  0.46182956]\n",
      "[ 0.46297398  0.46297398  0.47081712 ...,  0.46967269  0.46575113\n",
      "  0.46182956]\n",
      "[ 0.47081712  0.47081712  0.47473869 ...,  0.46297398  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.4865034   0.4865034   0.47866026 ...,  0.47081712  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.48230716  0.48622873  0.48622873 ...,  0.48314641  0.47922484\n",
      "  0.47473869]\n",
      "[ 0.47446403  0.4783856   0.48622873 ...,  0.48314641  0.48314641\n",
      "  0.48258183]\n",
      "[ 0.49098955  0.49098955  0.49098955 ...,  0.48258183  0.4865034   0.4865034 ]\n",
      "[ 0.49407187  0.4901503   0.4901503  ...,  0.47866026  0.48258183\n",
      "  0.48258183]\n",
      "[ 0.47530327  0.4713817   0.4713817  ...,  0.47866026  0.46689555\n",
      "  0.47473869]\n",
      "[ 0.49407187  0.49407187  0.49407187 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.47922484  0.48314641  0.48314641 ...,  0.47866026  0.47473869\n",
      "  0.46689555]\n",
      "[ 0.48314641  0.48314641  0.48314641 ...,  0.47866026  0.48258183\n",
      "  0.4865034 ]\n",
      "[ 0.47922484  0.48314641  0.48314641 ...,  0.47473869  0.47473869\n",
      "  0.47473869]\n",
      "[ 0.48314641  0.48314641  0.48706798 ...,  0.47081712  0.47473869\n",
      "  0.45513085]\n",
      "[ 0.48314641  0.47530327  0.46746014 ...,  0.47866026  0.47473869\n",
      "  0.47866026]\n",
      "[ 0.48258183  0.48258183  0.48258183 ...,  0.48258183  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.47866026  0.47473869  0.47473869 ...,  0.48258183  0.4865034\n",
      "  0.48258183]\n",
      "[ 0.47866026  0.47866026  0.47866026 ...,  0.46689555  0.46689555\n",
      "  0.47081712]\n",
      "[ 0.4865034   0.47866026  0.47473869 ...,  0.46689555  0.46689555\n",
      "  0.46297398]\n",
      "[ 0.47866026  0.48258183  0.48258183 ...,  0.47081712  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.48258183  0.4865034   0.48258183 ...,  0.47473869  0.47866026\n",
      "  0.48258183]\n",
      "[ 0.49098955  0.49491112  0.50275425 ...,  0.49491112  0.49434653\n",
      "  0.4865034 ]\n",
      "[ 0.49491112  0.49883268  0.50275425 ...,  0.48314641  0.48258183\n",
      "  0.4865034 ]\n",
      "[ 0.50275425  0.49883268  0.49883268 ...,  0.4865034   0.4865034   0.4865034 ]\n",
      "[ 0.50275425  0.50275425  0.49883268 ...,  0.4865034   0.4865034   0.4865034 ]\n",
      "[ 0.49883268  0.49883268  0.49491112 ...,  0.49491112  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.47081712  0.47081712  0.47473869 ...,  0.48258183  0.47866026\n",
      "  0.47473869]\n",
      "[ 0.47473869  0.47473869  0.47473869 ...,  0.47359426  0.47359426\n",
      "  0.47359426]\n",
      "[ 0.47866026  0.47866026  0.47866026 ...,  0.46967269  0.46575113\n",
      "  0.46182956]\n",
      "[ 0.47866026  0.47866026  0.47866026 ...,  0.47359426  0.46967269\n",
      "  0.46575113]\n",
      "[ 0.47866026  0.47866026  0.48258183 ...,  0.47219043  0.47219043\n",
      "  0.46826886]\n",
      "[ 0.49912261  0.49520104  0.49912261 ...,  0.52265202  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.50696574  0.51088731  0.51480888 ...,  0.52208743  0.51816587\n",
      "  0.5142443 ]\n",
      "[ 0.52405585  0.52405585  0.52013428 ...,  0.53049516  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.52013428  0.52797742  0.52797742 ...,  0.51480888  0.51873045\n",
      "  0.52657359]\n",
      "[ 0.51229114  0.51621271  0.52013428 ...,  0.52265202  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.52405585  0.52405585  0.52405585 ...,  0.50304417  0.49912261\n",
      "  0.50304417]\n",
      "[ 0.52405585  0.52013428  0.52797742 ...,  0.52265202  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.50557717  0.50949874  0.51342031 ...,  0.50949874  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.50557717  0.50949874  0.50557717 ...,  0.52518502  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.51314565  0.51314565  0.50922408 ...,  0.5016556   0.50557717\n",
      "  0.51342031]\n",
      "[ 0.51706722  0.51706722  0.51706722 ...,  0.51314565  0.51314565\n",
      "  0.51314565]\n",
      "[ 0.51088731  0.51088731  0.51480888 ...,  0.51480888  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.50949874  0.50949874  0.51734188 ...,  0.52126345  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.50696574  0.51088731  0.51088731 ...,  0.52657359  0.52657359\n",
      "  0.52265202]\n",
      "[ 0.50696574  0.51480888  0.53049516 ...,  0.51873045  0.51480888\n",
      "  0.51088731]\n",
      "[ 0.51480888  0.51480888  0.51480888 ...,  0.51873045  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.52265202  0.51873045  0.51088731 ...,  0.51873045  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.50696574  0.51088731  0.52265202 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.52491035  0.52491035  0.52098878 ...,  0.51873045  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.50696574  0.52265202\n",
      "  0.51873045]\n",
      "[ 0.51706722  0.51706722  0.52098878 ...,  0.51873045  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.52883192  0.52883192  0.53275349 ...,  0.52265202  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.51088731  0.51088731  0.50304417 ...,  0.51088731  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.53275349  0.52883192  0.52098878 ...,  0.52657359  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.51480888  0.51480888  0.50696574 ...,  0.51088731  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.51314565  0.51314565  0.52657359 ...,  0.51873045  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.51480888  0.51480888  0.51873045 ...,  0.51873045  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.51088731  0.51480888  0.51088731 ...,  0.51088731  0.51088731\n",
      "  0.52265202]\n",
      "[ 0.51480888  0.51873045  0.51480888 ...,  0.50696574  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.51088731  0.51088731  0.51088731 ...,  0.51480888  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.51873045  0.51873045  0.51480888 ...,  0.52265202  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.52265202  0.51480888  0.51480888 ...,  0.51480888  0.52265202\n",
      "  0.52265202]\n",
      "[ 0.50304417  0.50304417  0.50304417 ...,  0.50696574  0.51088731\n",
      "  0.51873045]\n",
      "[ 0.51088731  0.51480888  0.51480888 ...,  0.51480888  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.51088731  0.51480888  0.51480888 ...,  0.52657359  0.52265202\n",
      "  0.51480888]\n",
      "[ 0.53275349  0.52883192  0.53275349 ...,  0.52657359  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.52910658  0.53302815  0.53302815 ...,  0.52265202  0.52657359\n",
      "  0.53441672]\n",
      "[ 0.52883192  0.52883192  0.52491035 ...,  0.52657359  0.52657359\n",
      "  0.52657359]\n",
      "[ 0.53275349  0.53275349  0.53275349 ...,  0.53049516  0.53049516\n",
      "  0.53049516]\n",
      "[ 0.53275349  0.53275349  0.53667506 ...,  0.53049516  0.53049516\n",
      "  0.53441672]\n",
      "[ 0.50696574  0.50696574  0.50304417 ...,  0.49520104  0.49520104\n",
      "  0.51480888]\n",
      "[ 0.51088731  0.50696574  0.50696574 ...,  0.5142443   0.51816587\n",
      "  0.51816587]\n",
      "[ 0.50696574  0.50696574  0.50696574 ...,  0.53777371  0.526009    0.52208743]\n",
      "[ 0.51816587  0.51816587  0.52208743 ...,  0.51032273  0.5142443\n",
      "  0.51032273]\n",
      "[ 0.5142443   0.51032273  0.49855802 ...,  0.51032273  0.50640116\n",
      "  0.51032273]\n",
      "[ 0.38788434  0.38788434  0.38788434 ...,  0.42824445  0.43608759\n",
      "  0.43608759]\n",
      "[ 0.40415045  0.40807202  0.40807202 ...,  0.43916991  0.44701305\n",
      "  0.44309148]\n",
      "[ 0.40471504  0.40863661  0.40079347 ...,  0.42824445  0.43608759\n",
      "  0.44393072]\n",
      "[ 0.40863661  0.40471504  0.40863661 ...,  0.44000916  0.44000916\n",
      "  0.44000916]\n",
      "[ 0.3968719   0.39295033  0.38902876 ...,  0.42824445  0.42824445\n",
      "  0.43216602]\n",
      "[ 0.40471504  0.40863661  0.40863661 ...,  0.44393072  0.44393072\n",
      "  0.44393072]\n",
      "[ 0.3968719   0.3968719   0.40079347 ...,  0.43608759  0.44000916\n",
      "  0.44393072]\n",
      "[ 0.39603265  0.39603265  0.38818952 ...,  0.43973449  0.43973449\n",
      "  0.43973449]\n",
      "[ 0.40079347  0.40863661  0.40387579 ...,  0.43973449  0.43973449\n",
      "  0.43581292]\n",
      "[ 0.40079347  0.40079347  0.40779736 ...,  0.43581292  0.43189136\n",
      "  0.43189136]\n",
      "[ 0.39995422  0.39603265  0.40779736 ...,  0.43581292  0.43581292\n",
      "  0.43581292]\n",
      "[ 0.3968719   0.40079347  0.40471504 ...,  0.44000916  0.43216602\n",
      "  0.43216602]\n",
      "[ 0.40779736  0.41171893  0.40779736 ...,  0.41228351  0.41620508\n",
      "  0.42012665]\n",
      "[ 0.40863661  0.40863661  0.40471504 ...,  0.43916991  0.43916991\n",
      "  0.44309148]\n",
      "[ 0.40471504  0.40471504  0.40863661 ...,  0.45093462  0.44701305\n",
      "  0.44701305]\n",
      "[ 0.40863661  0.40863661  0.41255818 ...,  0.44701305  0.44309148\n",
      "  0.44309148]\n",
      "[ 0.3968719   0.40079347  0.40471504 ...,  0.43916991  0.44309148\n",
      "  0.44309148]\n",
      "[ 0.40863661  0.40863661  0.40471504 ...,  0.43524834  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.41199359  0.41199359  0.41199359 ...,  0.43216602  0.43216602\n",
      "  0.43608759]\n",
      "[ 0.39630732  0.39630732  0.39630732 ...,  0.44785229  0.44785229\n",
      "  0.44393072]\n",
      "[ 0.3968719   0.40079347  0.40471504 ...,  0.43216602  0.43608759\n",
      "  0.43608759]\n",
      "[ 0.41199359  0.41591516  0.41591516 ...,  0.44785229  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.39572747  0.39572747  0.39964904 ...,  0.43216602  0.43608759\n",
      "  0.42824445]\n",
      "[ 0.39630732  0.40022889  0.40807202 ...,  0.44393072  0.44000916\n",
      "  0.43608759]\n",
      "[ 0.39295033  0.39295033  0.39295033 ...,  0.43916991  0.43916991\n",
      "  0.43916991]\n",
      "[ 0.3968719   0.3968719   0.39295033 ...,  0.43132677  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.3968719   0.40079347  0.40471504 ...,  0.43132677  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.40079347  0.40079347  0.40471504 ...,  0.4274052   0.4274052   0.4274052 ]\n",
      "[ 0.38510719  0.38902876  0.3968719  ...,  0.43608759  0.43608759\n",
      "  0.44000916]\n",
      "[ 0.40471504  0.40863661  0.40471504 ...,  0.43916991  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.41255818  0.40471504  0.41647974 ...,  0.44309148  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.3968719   0.39295033  0.39295033 ...,  0.43916991  0.43524834\n",
      "  0.43916991]\n",
      "[ 0.3968719   0.40863661  0.40079347 ...,  0.45485618  0.45093462\n",
      "  0.43916991]\n",
      "[ 0.39572747  0.39572747  0.40079347 ...,  0.42040131  0.41255818\n",
      "  0.42040131]\n",
      "[ 0.3968719   0.40079347  0.40079347 ...,  0.43524834  0.43524834\n",
      "  0.43524834]\n",
      "[ 0.40415045  0.41199359  0.41591516 ...,  0.44393072  0.44393072\n",
      "  0.44393072]\n",
      "[ 0.40415045  0.40022889  0.40022889 ...,  0.45569543  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.40807202  0.41199359  0.41591516 ...,  0.45177386  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.40807202  0.41199359  0.41983673 ...,  0.45569543  0.459617    0.47530327]\n",
      "[ 0.41199359  0.41591516  0.41591516 ...,  0.44000916  0.43608759\n",
      "  0.43608759]\n",
      "[ 0.39964904  0.39180591  0.39572747 ...,  0.42432288  0.42432288\n",
      "  0.42432288]\n",
      "[ 0.40807202  0.40022889  0.40807202 ...,  0.43916991  0.43916991\n",
      "  0.43916991]\n",
      "[ 0.41199359  0.40807202  0.41199359 ...,  0.44701305  0.45093462\n",
      "  0.44701305]\n",
      "[ 0.40022889  0.40022889  0.40807202 ...,  0.44701305  0.45093462\n",
      "  0.45485618]\n",
      "[ 0.41591516  0.40807202  0.40415045 ...,  0.44701305  0.44701305\n",
      "  0.44701305]\n",
      "[ 0.4632639   0.4632639   0.4632639  ...,  0.48343633  0.47559319\n",
      "  0.47951476]\n",
      "[ 0.48230716  0.4783856   0.4783856  ...,  0.49520104  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.46718547  0.47110704  0.47502861 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.50304417  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.45934234  0.4632639   0.47110704 ...,  0.49520104  0.49520104\n",
      "  0.48343633]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.46718547  0.47110704  0.46718547 ...,  0.49127947  0.4873579\n",
      "  0.47951476]\n",
      "[ 0.47000839  0.47392996  0.47392996 ...,  0.49381247  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.47392996  0.47785153  0.4817731  ...,  0.48596933  0.48596933\n",
      "  0.48596933]\n",
      "[ 0.45824369  0.46216526  0.46216526 ...,  0.48596933  0.48596933\n",
      "  0.4898909 ]\n",
      "[ 0.45824369  0.46216526  0.47392996 ...,  0.48596933  0.48596933\n",
      "  0.48204776]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.49912261  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.46216526  0.47392996  0.46608682 ...,  0.48596933  0.48596933\n",
      "  0.4898909 ]\n",
      "[ 0.46718547  0.4632639   0.4632639  ...,  0.50304417  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.47502861  0.47502861  0.47895018 ...,  0.49127947  0.48343633\n",
      "  0.4873579 ]\n",
      "[ 0.47502861  0.47110704  0.47110704 ...,  0.49127947  0.49520104\n",
      "  0.49127947]\n",
      "[ 0.47895018  0.48287175  0.48679332 ...,  0.49520104  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47502861  0.47895018  0.48287175 ...,  0.4873579   0.48343633\n",
      "  0.48343633]\n",
      "[ 0.47110704  0.47110704  0.47895018 ...,  0.50304417  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.47110704  0.47110704  0.47895018 ...,  0.49912261  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.46718547  0.47110704  0.47110704 ...,  0.50696574  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47054246  0.46662089  0.46269932 ...,  0.48343633  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.47502861  0.47502861  0.47502861 ...,  0.49127947  0.49127947\n",
      "  0.4873579 ]\n",
      "[ 0.47502861  0.47110704  0.46718547 ...,  0.49127947  0.49127947\n",
      "  0.49127947]\n",
      "[ 0.45934234  0.4632639   0.47110704 ...,  0.48343633  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.46718547  0.45934234  0.45542077 ...,  0.48343633  0.48343633\n",
      "  0.47951476]\n",
      "[ 0.47110704  0.46718547  0.46718547 ...,  0.48343633  0.4873579\n",
      "  0.49127947]\n",
      "[ 0.4514992   0.45542077  0.47110704 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47502861  0.47110704  0.47110704 ...,  0.49127947  0.49127947\n",
      "  0.49127947]\n",
      "[ 0.47110704  0.47895018  0.48287175 ...,  0.49912261  0.49520104\n",
      "  0.49127947]\n",
      "[ 0.47502861  0.47502861  0.47110704 ...,  0.49127947  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47110704  0.47895018  0.48679332 ...,  0.49520104  0.49127947\n",
      "  0.4873579 ]\n",
      "[ 0.46718547  0.4632639   0.45934234 ...,  0.4873579   0.48343633\n",
      "  0.48343633]\n",
      "[ 0.47110704  0.46718547  0.46718547 ...,  0.48343633  0.47951476\n",
      "  0.48343633]\n",
      "[ 0.47110704  0.47110704  0.47895018 ...,  0.50304417  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.47895018  0.47895018  0.47110704 ...,  0.50696574  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47446403  0.47054246  0.47895018 ...,  0.51873045  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.48679332  0.49071489  0.49071489 ...,  0.51873045  0.51873045\n",
      "  0.51088731]\n",
      "[ 0.47110704  0.48287175  0.47110704 ...,  0.49127947  0.49520104\n",
      "  0.50304417]\n",
      "[ 0.46662089  0.46269932  0.47110704 ...,  0.49127947  0.48343633\n",
      "  0.48343633]\n",
      "[ 0.48287175  0.46718547  0.47895018 ...,  0.49912261  0.49520104\n",
      "  0.49127947]\n",
      "[ 0.48287175  0.48679332  0.48287175 ...,  0.49127947  0.49127947\n",
      "  0.49912261]\n",
      "[ 0.46662089  0.47446403  0.48230716 ...,  0.4873579   0.47559319\n",
      "  0.47559319]\n",
      "[ 0.47895018  0.47895018  0.47895018 ...,  0.49127947  0.48343633\n",
      "  0.47559319]\n",
      "[ 0.42432288  0.42432288  0.42040131 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42824445  0.42824445  0.42040131 ...,  0.47054246  0.46269932\n",
      "  0.46269932]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.45934234  0.4514992\n",
      "  0.46718547]\n",
      "[ 0.44000916  0.43608759  0.43216602 ...,  0.4632639   0.46718547\n",
      "  0.46718547]\n",
      "[ 0.44393072  0.44000916  0.43608759 ...,  0.47110704  0.4632639\n",
      "  0.46718547]\n",
      "[ 0.42432288  0.42432288  0.42432288 ...,  0.46718547  0.4632639   0.4632639 ]\n",
      "[ 0.44000916  0.44000916  0.43608759 ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.42404822  0.43189136  0.43189136 ...,  0.47225147  0.45656519\n",
      "  0.46048676]\n",
      "[ 0.42796979  0.42796979  0.43189136 ...,  0.46440833  0.4683299   0.4683299 ]\n",
      "[ 0.43581292  0.43189136  0.43189136 ...,  0.46048676  0.46440833\n",
      "  0.45656519]\n",
      "[ 0.43581292  0.43581292  0.44757763 ...,  0.48401617  0.47617304\n",
      "  0.46440833]\n",
      "[ 0.42824445  0.42824445  0.42040131 ...,  0.45542077  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.41620508  0.42404822  0.43189136 ...,  0.46440833  0.46440833\n",
      "  0.46440833]\n",
      "[ 0.43608759  0.44000916  0.43216602 ...,  0.4632639   0.45934234\n",
      "  0.45934234]\n",
      "[ 0.44000916  0.43608759  0.43216602 ...,  0.45934234  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.42824445  0.43216602  0.43216602 ...,  0.45542077  0.45934234\n",
      "  0.4514992 ]\n",
      "[ 0.44000916  0.43608759  0.44000916 ...,  0.4632639   0.4632639\n",
      "  0.45934234]\n",
      "[ 0.43608759  0.43216602  0.43216602 ...,  0.4632639   0.4632639\n",
      "  0.45542077]\n",
      "[ 0.42824445  0.42432288  0.42824445 ...,  0.46718547  0.47110704\n",
      "  0.47502861]\n",
      "[ 0.4156405   0.4156405   0.42348363 ...,  0.46718547  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.42824445  0.43216602  0.43216602 ...,  0.45542077  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.44000916  0.44000916  0.43608759 ...,  0.4632639   0.47110704\n",
      "  0.46718547]\n",
      "[ 0.42432288  0.42040131  0.42824445 ...,  0.4514992   0.4514992   0.4514992 ]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.45485618  0.46662089\n",
      "  0.45093462]\n",
      "[ 0.42824445  0.42432288  0.42432288 ...,  0.45542077  0.4514992   0.4514992 ]\n",
      "[ 0.43608759  0.43608759  0.43216602 ...,  0.46718547  0.4632639   0.4632639 ]\n",
      "[ 0.42432288  0.42040131  0.42432288 ...,  0.45934234  0.45542077\n",
      "  0.45934234]\n",
      "[ 0.42432288  0.42824445  0.43216602 ...,  0.4632639   0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.42824445  0.42432288  0.42432288 ...,  0.45934234  0.46718547\n",
      "  0.47110704]\n",
      "[ 0.43608759  0.43608759  0.44000916 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.4274052   0.4274052   0.4274052  ...,  0.47502861  0.47895018\n",
      "  0.47110704]\n",
      "[ 0.42348363  0.42348363  0.42432288 ...,  0.4632639   0.4632639\n",
      "  0.45934234]\n",
      "[ 0.44000916  0.44000916  0.43608759 ...,  0.47502861  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.42824445  0.42432288  0.42432288 ...,  0.4632639   0.45934234\n",
      "  0.45542077]\n",
      "[ 0.43608759  0.43216602  0.42824445 ...,  0.45934234  0.45542077\n",
      "  0.45542077]\n",
      "[ 0.43608759  0.43608759  0.44000916 ...,  0.4783856   0.4783856\n",
      "  0.47446403]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.48230716  0.47446403\n",
      "  0.47446403]\n",
      "[ 0.43608759  0.44000916  0.44393072 ...,  0.46269932  0.47054246\n",
      "  0.47446403]\n",
      "[ 0.44393072  0.43608759  0.43608759 ...,  0.48230716  0.48230716\n",
      "  0.48230716]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.46662089  0.46662089\n",
      "  0.46662089]\n",
      "[ 0.42432288  0.42432288  0.42040131 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42824445  0.42824445  0.42824445 ...,  0.46269932  0.47054246\n",
      "  0.47054246]\n",
      "[ 0.44000916  0.43608759  0.42824445 ...,  0.46776532  0.46384375\n",
      "  0.46384375]\n",
      "[ 0.43608759  0.44000916  0.44000916 ...,  0.46269932  0.46269932\n",
      "  0.46269932]\n",
      "[ 0.44393072  0.44000916  0.43608759 ...,  0.46662089  0.47054246\n",
      "  0.4783856 ]\n",
      "[ 0.47502861  0.47502861  0.47895018 ...,  0.49127947  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.48679332  0.48679332  0.48679332 ...,  0.51088731  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.47502861  0.47502861  0.47110704 ...,  0.48876173  0.48876173\n",
      "  0.48876173]\n",
      "[ 0.48679332  0.48287175  0.47895018 ...,  0.50836957  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.47110704  0.47110704  0.46718547 ...,  0.49660487  0.49660487\n",
      "  0.50052644]\n",
      "[ 0.47895018  0.47895018  0.48679332 ...,  0.4926833   0.49660487\n",
      "  0.49660487]\n",
      "[ 0.4632639   0.46718547  0.47110704 ...,  0.51621271  0.51229114\n",
      "  0.50836957]\n",
      "[ 0.4817731   0.4817731   0.48569467 ...,  0.4898909   0.50557717\n",
      "  0.49773404]\n",
      "[ 0.47392996  0.47785153  0.47000839 ...,  0.49381247  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.47392996  0.47392996  0.47392996 ...,  0.49381247  0.5016556\n",
      "  0.49773404]\n",
      "[ 0.4817731   0.48569467  0.48569467 ...,  0.49381247  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.47502861  0.47502861  0.47502861 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.4817731   0.4817731   0.4817731  ...,  0.49773404  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.48679332  0.47895018  0.47502861 ...,  0.49660487  0.49660487\n",
      "  0.48876173]\n",
      "[ 0.47951476  0.47951476  0.47559319 ...,  0.50052644  0.49660487\n",
      "  0.49660487]\n",
      "[ 0.48287175  0.47895018  0.47895018 ...,  0.48343633  0.4873579   0.4873579 ]\n",
      "[ 0.48679332  0.48679332  0.48679332 ...,  0.50052644  0.50052644\n",
      "  0.50052644]\n",
      "[ 0.47502861  0.47502861  0.47502861 ...,  0.48484016  0.48484016\n",
      "  0.48876173]\n",
      "[ 0.47895018  0.48287175  0.48679332 ...,  0.49127947  0.49520104\n",
      "  0.49912261]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.51088731  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.47502861  0.47502861  0.47502861 ...,  0.51088731  0.50304417\n",
      "  0.4873579 ]\n",
      "[ 0.48287175  0.48287175  0.49071489 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47895018  0.47502861  0.46718547 ...,  0.49127947  0.49127947\n",
      "  0.49127947]\n",
      "[ 0.47502861  0.49071489  0.48287175 ...,  0.50304417  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.47502861  0.47502861  0.47895018 ...,  0.48569467  0.4935378\n",
      "  0.50138094]\n",
      "[ 0.48287175  0.48679332  0.49855802 ...,  0.49773404  0.5016556\n",
      "  0.50557717]\n",
      "[ 0.48287175  0.47895018  0.47895018 ...,  0.51088731  0.50304417\n",
      "  0.49127947]\n",
      "[ 0.47895018  0.47502861  0.47502861 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.48679332  0.47895018  0.47502861 ...,  0.4935378   0.4935378   0.4935378 ]\n",
      "[ 0.47559319  0.47167163  0.47951476 ...,  0.50052644  0.50052644\n",
      "  0.49660487]\n",
      "[ 0.47167163  0.48343633  0.4873579  ...,  0.50949874  0.50557717\n",
      "  0.49773404]\n",
      "[ 0.47167163  0.47167163  0.47167163 ...,  0.48876173  0.4926833   0.4926833 ]\n",
      "[ 0.4873579   0.48343633  0.48343633 ...,  0.4926833   0.49660487\n",
      "  0.50052644]\n",
      "[ 0.48287175  0.47895018  0.47502861 ...,  0.4873579   0.49520104\n",
      "  0.49127947]\n",
      "[ 0.47167163  0.47559319  0.47951476 ...,  0.4926833   0.4926833   0.4926833 ]\n",
      "[ 0.49071489  0.49071489  0.48679332 ...,  0.50696574  0.50304417\n",
      "  0.49912261]\n",
      "[ 0.47895018  0.48287175  0.48679332 ...,  0.51873045  0.51873045\n",
      "  0.51873045]\n",
      "[ 0.47502861  0.48287175  0.48287175 ...,  0.51088731  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.52657359  0.52657359\n",
      "  0.52265202]\n",
      "[ 0.48287175  0.48679332  0.49071489 ...,  0.51873045  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.48679332  0.48679332  0.48679332 ...,  0.49520104  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.48287175  0.47895018  0.47502861 ...,  0.50304417  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.50304417  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.47502861  0.48287175  0.47895018 ...,  0.50696574  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.47110704  0.47895018\n",
      "  0.47502861]\n",
      "[ 0.45093462  0.45093462  0.45093462 ...,  0.48287175  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.45934234  0.4632639   0.4514992  ...,  0.47895018  0.47895018\n",
      "  0.48287175]\n",
      "[ 0.44757763  0.44365606  0.45542077 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.45485618  0.45485618  0.46269932 ...,  0.49463645  0.49463645\n",
      "  0.49071489]\n",
      "[ 0.45934234  0.45542077  0.45542077 ...,  0.49071489  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.4514992   0.44365606  0.44757763 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.43973449  0.43581292  0.44757763 ...,  0.4817731   0.4817731\n",
      "  0.48569467]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.48961624  0.48569467\n",
      "  0.4817731 ]\n",
      "[ 0.4514992   0.4514992   0.4514992  ...,  0.47392996  0.47785153\n",
      "  0.4817731 ]\n",
      "[ 0.4514992   0.4514992   0.4514992  ...,  0.4817731   0.48569467\n",
      "  0.48961624]\n",
      "[ 0.44785229  0.44785229  0.44393072 ...,  0.47502861  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.4632639   0.4514992   0.44757763 ...,  0.4817731   0.48961624\n",
      "  0.48961624]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.48793774  0.48793774\n",
      "  0.48793774]\n",
      "[ 0.45934234  0.45934234  0.45934234 ...,  0.49185931  0.49185931\n",
      "  0.49185931]\n",
      "[ 0.44701305  0.45093462  0.45093462 ...,  0.47502861  0.47502861\n",
      "  0.48287175]\n",
      "[ 0.45934234  0.4632639   0.45934234 ...,  0.48009461  0.48009461\n",
      "  0.47617304]\n",
      "[ 0.45093462  0.45093462  0.44757763 ...,  0.48401617  0.48009461\n",
      "  0.48009461]\n",
      "[ 0.45093462  0.45485618  0.45485618 ...,  0.49071489  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.44701305  0.45093462  0.45093462 ...,  0.47895018  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.45485618  0.45485618  0.45485618 ...,  0.48679332  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.45485618  0.45877775  0.45877775 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.46353857  0.45177386  0.44393072 ...,  0.47895018  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.44701305  0.45485618  0.46269932 ...,  0.47895018  0.48287175\n",
      "  0.49463645]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.48401617  0.48009461\n",
      "  0.48401617]\n",
      "[ 0.4632639   0.45934234  0.4514992  ...,  0.48793774  0.48793774\n",
      "  0.48793774]\n",
      "[ 0.45093462  0.45485618  0.45877775 ...,  0.47225147  0.47617304\n",
      "  0.48009461]\n",
      "[ 0.44365606  0.44365606  0.44757763 ...,  0.48401617  0.48401617\n",
      "  0.48401617]\n",
      "[ 0.45877775  0.45877775  0.45877775 ...,  0.48009461  0.48009461\n",
      "  0.48009461]\n",
      "[ 0.46269932  0.46662089  0.46269932 ...,  0.47225147  0.47225147\n",
      "  0.48009461]\n",
      "[ 0.44757763  0.45542077  0.45934234 ...,  0.47617304  0.47225147\n",
      "  0.4683299 ]\n",
      "[ 0.44701305  0.45093462  0.45485618 ...,  0.50362402  0.49970245\n",
      "  0.48793774]\n",
      "[ 0.4514992   0.4514992   0.45542077 ...,  0.48793774  0.48401617\n",
      "  0.48009461]\n",
      "[ 0.44785229  0.44785229  0.45177386 ...,  0.48287175  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.45877775  0.45485618  0.45485618 ...,  0.48401617  0.48401617\n",
      "  0.48401617]\n",
      "[ 0.45877775  0.46269932  0.46662089 ...,  0.48679332  0.48679332\n",
      "  0.49071489]\n",
      "[ 0.47054246  0.46662089  0.46269932 ...,  0.50247959  0.50247959\n",
      "  0.50247959]\n",
      "[ 0.45093462  0.45877775  0.46269932 ...,  0.49071489  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.45485618  0.45093462  0.45093462 ...,  0.49463645  0.48679332\n",
      "  0.49071489]\n",
      "[ 0.46269932  0.45877775  0.45485618 ...,  0.47502861  0.47502861\n",
      "  0.47895018]\n",
      "[ 0.44785229  0.44785229  0.44785229 ...,  0.49071489  0.48679332\n",
      "  0.48287175]\n",
      "[ 0.45485618  0.45877775  0.46662089 ...,  0.47895018  0.48287175\n",
      "  0.48679332]\n",
      "[ 0.47054246  0.46269932  0.44701305 ...,  0.48287175  0.48679332\n",
      "  0.48679332]\n",
      "[ 0.44701305  0.44701305  0.45877775 ...,  0.49463645  0.49463645\n",
      "  0.49071489]\n",
      "[ 0.45877775  0.45877775  0.46662089 ...,  0.49071489  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.47110704  0.46718547  0.46718547 ...,  0.48343633  0.48343633\n",
      "  0.4873579 ]\n",
      "[ 0.46718547  0.47502861  0.48287175 ...,  0.51088731  0.51088731\n",
      "  0.50304417]\n",
      "[ 0.47559319  0.47951476  0.47951476 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47951476  0.47951476  0.47951476 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.48287175  0.48287175  0.48287175 ...,  0.4873579   0.49127947\n",
      "  0.50304417]\n",
      "[ 0.47951476  0.47951476  0.47951476 ...,  0.50304417  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47110704  0.47110704  0.47895018 ...,  0.50304417  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.47392996  0.47785153  0.4817731  ...,  0.49745937  0.50138094\n",
      "  0.50922408]\n",
      "[ 0.47000839  0.46216526  0.46608682 ...,  0.49745937  0.50138094\n",
      "  0.50530251]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.48961624  0.48961624\n",
      "  0.4935378 ]\n",
      "[ 0.4817731   0.4817731   0.4817731  ...,  0.50530251  0.50138094\n",
      "  0.49745937]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49912261  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.47392996  0.46608682  0.47785153 ...,  0.48961624  0.48569467\n",
      "  0.48569467]\n",
      "[ 0.47895018  0.47895018  0.47895018 ...,  0.49912261  0.49520104\n",
      "  0.49127947]\n",
      "[ 0.48287175  0.48287175  0.47895018 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47895018  0.47895018  0.47502861 ...,  0.48343633  0.4873579\n",
      "  0.49127947]\n",
      "[ 0.47110704  0.47502861  0.47895018 ...,  0.47951476  0.4873579\n",
      "  0.49520104]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.49520104  0.49127947\n",
      "  0.4873579 ]\n",
      "[ 0.47895018  0.47110704  0.47502861 ...,  0.50304417  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49520104  0.49520104\n",
      "  0.50696574]\n",
      "[ 0.48679332  0.48679332  0.49071489 ...,  0.49520104  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.48287175  0.47895018  0.47110704 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.48679332  0.48679332  0.47502861 ...,  0.49520104  0.49127947\n",
      "  0.49520104]\n",
      "[ 0.48287175  0.47895018  0.47502861 ...,  0.49912261  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.48287175  0.47502861  0.48287175 ...,  0.49127947  0.49127947\n",
      "  0.48961624]\n",
      "[ 0.46775006  0.46775006  0.48343633 ...,  0.50304417  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47110704  0.47110704  0.46718547 ...,  0.54196994  0.54196994\n",
      "  0.53298238]\n",
      "[ 0.47895018  0.48287175  0.48679332 ...,  0.4873579   0.4873579\n",
      "  0.49127947]\n",
      "[ 0.47502861  0.47895018  0.47895018 ...,  0.4935378   0.49912261\n",
      "  0.50304417]\n",
      "[ 0.47502861  0.47502861  0.47895018 ...,  0.49912261  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.47502861  0.47502861  0.47895018 ...,  0.49127947  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.47502861  0.47502861  0.47110704 ...,  0.49520104  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.47895018  0.47895018  0.47110704 ...,  0.49912261  0.49912261\n",
      "  0.50304417]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49127947  0.49127947\n",
      "  0.49127947]\n",
      "[ 0.46718547  0.47502861  0.48287175 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.49463645  0.49463645  0.49071489 ...,  0.51088731  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.50696574  0.51480888\n",
      "  0.51873045]\n",
      "[ 0.47895018  0.48287175  0.48679332 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.48679332  0.48679332  0.48679332 ...,  0.51873045  0.51480888\n",
      "  0.50696574]\n",
      "[ 0.49463645  0.49855802  0.49071489 ...,  0.51480888  0.51480888\n",
      "  0.51480888]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.48343633  0.49127947\n",
      "  0.49520104]\n",
      "[ 0.49071489  0.49071489  0.49071489 ...,  0.49912261  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.49071489  0.49071489  0.48287175 ...,  0.49912261  0.49520104\n",
      "  0.50696574]\n",
      "[ 0.49463645  0.49071489  0.48287175 ...,  0.49463645  0.49855802\n",
      "  0.50247959]\n",
      "[ 0.48287175  0.48679332  0.48679332 ...,  0.51088731  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.46297398  0.45905241  0.45513085 ...,  0.46689555  0.47473869\n",
      "  0.48258183]\n",
      "[ 0.46689555  0.46689555  0.46689555 ...,  0.47081712  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.46689555  0.46297398  0.46297398 ...,  0.46182956  0.45790799\n",
      "  0.46182956]\n",
      "[ 0.46689555  0.47866026  0.47473869 ...,  0.46575113  0.47359426\n",
      "  0.47751583]\n",
      "[ 0.45120928  0.45513085  0.45905241 ...,  0.46967269  0.47751583\n",
      "  0.47359426]\n",
      "[ 0.46689555  0.47081712  0.47081712 ...,  0.47359426  0.47359426\n",
      "  0.47359426]\n",
      "[ 0.47473869  0.47081712  0.47081712 ...,  0.47751583  0.47751583\n",
      "  0.47751583]\n",
      "[ 0.459617    0.459617    0.46353857 ...,  0.46746014  0.46746014\n",
      "  0.46746014]\n",
      "[ 0.46353857  0.46353857  0.459617   ...,  0.46353857  0.46353857\n",
      "  0.46269932]\n",
      "[ 0.46353857  0.46353857  0.46353857 ...,  0.46746014  0.46746014\n",
      "  0.46353857]\n",
      "[ 0.46353857  0.459617    0.459617   ...,  0.46746014  0.4713817   0.4713817 ]\n",
      "[ 0.46297398  0.46689555  0.46689555 ...,  0.46297398  0.45905241\n",
      "  0.45513085]\n",
      "[ 0.46746014  0.48314641  0.48314641 ...,  0.4713817   0.4713817\n",
      "  0.47530327]\n",
      "[ 0.45513085  0.45120928  0.45120928 ...,  0.45513085  0.45905241\n",
      "  0.45905241]\n",
      "[ 0.46297398  0.45905241  0.45905241 ...,  0.45513085  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.45513085  0.45120928  0.45120928 ...,  0.45905241  0.45905241\n",
      "  0.45120928]\n",
      "[ 0.45120928  0.45120928  0.44728771 ...,  0.46689555  0.46297398\n",
      "  0.46689555]\n",
      "[ 0.45513085  0.45513085  0.45513085 ...,  0.46689555  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.47081712  0.47081712  0.47081712 ...,  0.47473869  0.47081712\n",
      "  0.46689555]\n",
      "[ 0.47081712  0.47081712  0.47081712 ...,  0.45905241  0.45905241\n",
      "  0.46297398]\n",
      "[ 0.46297398  0.46297398  0.45905241 ...,  0.47473869  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.46689555  0.47081712  0.47081712 ...,  0.46297398  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.44728771  0.45513085  0.45905241 ...,  0.45905241  0.45905241\n",
      "  0.45513085]\n",
      "[ 0.46297398  0.46297398  0.47081712 ...,  0.47081712  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.46689555  0.46297398  0.45905241 ...,  0.47081712  0.47081712\n",
      "  0.46689555]\n",
      "[ 0.45905241  0.46297398  0.47081712 ...,  0.47866026  0.47866026\n",
      "  0.47473869]\n",
      "[ 0.46297398  0.45905241  0.45120928 ...,  0.45905241  0.45905241\n",
      "  0.46689555]\n",
      "[ 0.46297398  0.46297398  0.46297398 ...,  0.46689555  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.45905241  0.45905241  0.46297398 ...,  0.46297398  0.46297398\n",
      "  0.46297398]\n",
      "[ 0.45905241  0.45905241  0.45905241 ...,  0.46297398  0.45905241\n",
      "  0.45905241]\n",
      "[ 0.46689555  0.45905241  0.45120928 ...,  0.46689555  0.46689555\n",
      "  0.46297398]\n",
      "[ 0.45513085  0.45513085  0.45120928 ...,  0.46689555  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.46297398  0.47081712  0.46297398 ...,  0.46689555  0.46297398\n",
      "  0.45905241]\n",
      "[ 0.45513085  0.44728771  0.45120928 ...,  0.46689555  0.46689555\n",
      "  0.46689555]\n",
      "[ 0.45905241  0.45905241  0.45905241 ...,  0.46689555  0.46689555\n",
      "  0.45905241]\n",
      "[ 0.47473869  0.47081712  0.47081712 ...,  0.47866026  0.46689555\n",
      "  0.47081712]\n",
      "[ 0.47473869  0.47081712  0.47473869 ...,  0.47866026  0.48258183\n",
      "  0.48258183]\n",
      "[ 0.46689555  0.46689555  0.47081712 ...,  0.48258183  0.47866026\n",
      "  0.48258183]\n",
      "[ 0.47866026  0.47866026  0.48258183 ...,  0.4865034   0.47866026\n",
      "  0.47473869]\n",
      "[ 0.48258183  0.47866026  0.4865034  ...,  0.47473869  0.47473869\n",
      "  0.47866026]\n",
      "[ 0.46297398  0.45905241  0.45513085 ...,  0.45905241  0.45905241\n",
      "  0.46297398]\n",
      "[ 0.45905241  0.45513085  0.45513085 ...,  0.47081712  0.46689555\n",
      "  0.46297398]\n",
      "[ 0.47081712  0.47473869  0.47866026 ...,  0.47473869  0.47081712\n",
      "  0.46689555]\n",
      "[ 0.46689555  0.46689555  0.47081712 ...,  0.47866026  0.47866026\n",
      "  0.47473869]\n",
      "[ 0.46689555  0.46297398  0.45905241 ...,  0.46689555  0.47081712\n",
      "  0.47081712]\n",
      "[ 0.43830014  0.44222171  0.44222171 ...,  0.44222171  0.44614328\n",
      "  0.44614328]\n",
      "[ 0.44473945  0.44866102  0.44866102 ...,  0.44866102  0.45258259\n",
      "  0.44866102]\n",
      "[ 0.4186923   0.42261387  0.42653544 ...,  0.44222171  0.44222171\n",
      "  0.43437858]\n",
      "[ 0.44222171  0.43830014  0.44222171 ...,  0.43437858  0.43437858\n",
      "  0.44222171]\n",
      "[ 0.43830014  0.43830014  0.43830014 ...,  0.43830014  0.43437858\n",
      "  0.43437858]\n",
      "[ 0.43437858  0.42653544  0.42261387 ...,  0.43045701  0.43437858\n",
      "  0.43830014]\n",
      "[ 0.43830014  0.43437858  0.43830014 ...,  0.43045701  0.43437858\n",
      "  0.43830014]\n",
      "[ 0.44336614  0.44728771  0.45120928 ...,  0.44728771  0.43944457\n",
      "  0.44336614]\n",
      "[ 0.435523    0.44336614  0.44336614 ...,  0.43944457  0.44336614\n",
      "  0.44336614]\n",
      "[ 0.43944457  0.43944457  0.43944457 ...,  0.435523    0.435523    0.43944457]\n",
      "[ 0.44336614  0.44336614  0.44728771 ...,  0.45120928  0.45120928\n",
      "  0.44728771]\n",
      "[ 0.43830014  0.44222171  0.44614328 ...,  0.43437858  0.43437858\n",
      "  0.43830014]\n",
      "[ 0.43944457  0.44728771  0.45120928 ...,  0.435523    0.435523    0.42767987]\n",
      "[ 0.43045701  0.43045701  0.43045701 ...,  0.43297475  0.43297475\n",
      "  0.43297475]\n",
      "[ 0.43045701  0.42653544  0.42653544 ...,  0.43297475  0.43297475\n",
      "  0.43689631]\n",
      "[ 0.43045701  0.43045701  0.42261387 ...,  0.43689631  0.43689631\n",
      "  0.42513161]\n",
      "[ 0.42905318  0.43689631  0.42905318 ...,  0.43297475  0.42905318\n",
      "  0.42905318]\n",
      "[ 0.43045701  0.43045701  0.43045701 ...,  0.42905318  0.42905318\n",
      "  0.42905318]\n",
      "[ 0.43689631  0.43689631  0.43689631 ...,  0.43689631  0.44081788\n",
      "  0.44081788]\n",
      "[ 0.44473945  0.44473945  0.44473945 ...,  0.44866102  0.44866102\n",
      "  0.44866102]\n",
      "[ 0.44473945  0.44473945  0.44081788 ...,  0.45258259  0.45258259\n",
      "  0.44866102]\n",
      "[ 0.43689631  0.43689631  0.44081788 ...,  0.45258259  0.44866102\n",
      "  0.44473945]\n",
      "[ 0.43830014  0.43437858  0.43437858 ...,  0.43437858  0.43437858\n",
      "  0.43437858]\n",
      "[ 0.44473945  0.44473945  0.44081788 ...,  0.44473945  0.44473945\n",
      "  0.44081788]\n",
      "[ 0.45120928  0.43915465  0.43131151 ...,  0.44222171  0.44222171\n",
      "  0.44222171]\n",
      "[ 0.44728771  0.44336614  0.43944457 ...,  0.43437858  0.43437858\n",
      "  0.43830014]\n",
      "[ 0.44222171  0.44222171  0.43830014 ...,  0.43045701  0.43437858\n",
      "  0.42653544]\n",
      "[ 0.44307622  0.44307622  0.43915465 ...,  0.43830014  0.43830014\n",
      "  0.44222171]\n",
      "[ 0.43830014  0.44222171  0.43944457 ...,  0.43830014  0.43437858\n",
      "  0.43437858]\n",
      "[ 0.44222171  0.44222171  0.43437858 ...,  0.43045701  0.4186923\n",
      "  0.43045701]\n",
      "[ 0.43437858  0.43830014  0.43830014 ...,  0.43830014  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.4186923   0.43045701  0.43045701 ...,  0.42653544  0.42653544\n",
      "  0.42653544]\n",
      "[ 0.43437858  0.43830014  0.44222171 ...,  0.43689631  0.42905318\n",
      "  0.42513161]\n",
      "[ 0.43689631  0.44081788  0.44473945 ...,  0.43830014  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.43437858  0.43830014  0.43830014 ...,  0.43437858  0.43437858\n",
      "  0.43437858]\n",
      "[ 0.43689631  0.44081788  0.44473945 ...,  0.44081788  0.44866102\n",
      "  0.44866102]\n",
      "[ 0.44866102  0.44866102  0.44866102 ...,  0.45258259  0.45258259\n",
      "  0.44866102]\n",
      "[ 0.44473945  0.44081788  0.44081788 ...,  0.45258259  0.44866102\n",
      "  0.44866102]\n",
      "[ 0.44866102  0.45258259  0.44866102 ...,  0.45650416  0.45650416\n",
      "  0.44866102]\n",
      "[ 0.45650416  0.45258259  0.46042573 ...,  0.44866102  0.44866102\n",
      "  0.46042573]\n",
      "[ 0.43830014  0.44614328  0.44614328 ...,  0.43830014  0.43830014\n",
      "  0.43830014]\n",
      "[ 0.43297475  0.44081788  0.44081788 ...,  0.43689631  0.42905318\n",
      "  0.42905318]\n",
      "[ 0.44473945  0.46826886  0.46042573 ...,  0.44473945  0.44081788\n",
      "  0.44081788]\n",
      "[ 0.44473945  0.43689631  0.44081788 ...,  0.44081788  0.44081788\n",
      "  0.44081788]\n",
      "[ 0.43689631  0.43689631  0.43689631 ...,  0.42121004  0.42121004\n",
      "  0.42121004]\n",
      "[ 0.47054246  0.47446403  0.46746014 ...,  0.49491112  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.47530327  0.47922484  0.47922484 ...,  0.48706798  0.49491112\n",
      "  0.50667582]\n",
      "[ 0.47922484  0.47922484  0.48706798 ...,  0.49491112  0.49491112\n",
      "  0.49491112]\n",
      "[ 0.4713817   0.47530327  0.48314641 ...,  0.49883268  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.48314641  0.47530327  0.47530327 ...,  0.49491112  0.49491112\n",
      "  0.49491112]\n",
      "[ 0.48706798  0.47530327  0.4713817  ...,  0.50275425  0.50667582\n",
      "  0.51059739]\n",
      "[ 0.47922484  0.47922484  0.48314641 ...,  0.49883268  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.46718547  0.46718547  0.46718547 ...,  0.49463645  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.48679332  0.47502861  0.46718547 ...,  0.49463645  0.49071489\n",
      "  0.48679332]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.48679332  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.47110704  0.47110704  0.47502861 ...,  0.48679332  0.49071489\n",
      "  0.49071489]\n",
      "[ 0.47054246  0.45877775  0.46746014 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49071489  0.49463645\n",
      "  0.49463645]\n",
      "[ 0.4713817   0.47530327  0.47922484 ...,  0.48706798  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.4713817   0.4713817   0.47530327 ...,  0.48314641  0.48314641\n",
      "  0.48706798]\n",
      "[ 0.48314641  0.47922484  0.4713817  ...,  0.49883268  0.49491112\n",
      "  0.48706798]\n",
      "[ 0.4713817   0.4713817   0.4713817  ...,  0.49098955  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.4713817   0.4713817   0.49098955 ...,  0.48706798  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.4783856   0.4783856   0.47446403 ...,  0.49883268  0.49491112\n",
      "  0.48706798]\n",
      "[ 0.4901503   0.4901503   0.48622873 ...,  0.49098955  0.49098955\n",
      "  0.49491112]\n",
      "[ 0.4783856   0.48230716  0.4783856  ...,  0.49491112  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.47446403  0.47446403  0.47446403 ...,  0.47530327  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.47530327  0.47922484  0.46746014 ...,  0.48706798  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.4901503   0.49407187  0.49407187 ...,  0.49407187  0.49799344\n",
      "  0.49799344]\n",
      "[ 0.47054246  0.46662089  0.47446403 ...,  0.48706798  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.47446403  0.4783856   0.47054246 ...,  0.49098955  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.4783856   0.4783856   0.47446403 ...,  0.49098955  0.48706798\n",
      "  0.48314641]\n",
      "[ 0.47054246  0.47054246  0.47054246 ...,  0.49098955  0.49883268\n",
      "  0.50275425]\n",
      "[ 0.48622873  0.48230716  0.4783856  ...,  0.48314641  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.47530327  0.47530327  0.47530327 ...,  0.49491112  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.47922484  0.47530327  0.47530327 ...,  0.48706798  0.48706798\n",
      "  0.49098955]\n",
      "[ 0.4713817   0.4713817   0.4713817  ...,  0.48706798  0.47530327\n",
      "  0.48314641]\n",
      "[ 0.48314641  0.48314641  0.48314641 ...,  0.48706798  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.45569543  0.459617    0.46353857 ...,  0.48314641  0.48706798\n",
      "  0.48314641]\n",
      "[ 0.47922484  0.47922484  0.48314641 ...,  0.48706798  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.48622873  0.48622873  0.48230716 ...,  0.49491112  0.49491112\n",
      "  0.49883268]\n",
      "[ 0.48230716  0.47446403  0.48706798 ...,  0.49883268  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.48706798  0.47922484  0.47530327 ...,  0.49098955  0.49491112\n",
      "  0.49883268]\n",
      "[ 0.48622873  0.48622873  0.48622873 ...,  0.49883268  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.49407187  0.49407187  0.49407187 ...,  0.50667582  0.50275425\n",
      "  0.50275425]\n",
      "[ 0.4783856   0.4783856   0.47446403 ...,  0.48314641  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.47530327  0.47530327  0.47530327 ...,  0.49491112  0.49491112\n",
      "  0.49098955]\n",
      "[ 0.48314641  0.47922484  0.47922484 ...,  0.48706798  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.48706798  0.49098955  0.49491112 ...,  0.49098955  0.49434653\n",
      "  0.49434653]\n",
      "[ 0.47922484  0.48706798  0.47922484 ...,  0.48706798  0.49491112\n",
      "  0.49883268]\n",
      "[ 0.40471504  0.41647974  0.43216602 ...,  0.45485618  0.45485618\n",
      "  0.45485618]\n",
      "[ 0.42824445  0.42824445  0.43216602 ...,  0.46269932  0.46269932\n",
      "  0.45877775]\n",
      "[ 0.41255818  0.41255818  0.40863661 ...,  0.45569543  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.41647974  0.41647974  0.41647974 ...,  0.45877775  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.42710002  0.43102159  0.42317845 ...,  0.45177386  0.45569543\n",
      "  0.45177386]\n",
      "[ 0.41647974  0.40863661  0.41255818 ...,  0.45569543  0.459617    0.459617  ]\n",
      "[ 0.40471504  0.40863661  0.40863661 ...,  0.459617    0.459617    0.459617  ]\n",
      "[ 0.41956207  0.42348363  0.42348363 ...,  0.4632639   0.4514992   0.4514992 ]\n",
      "[ 0.4274052   0.43132677  0.4274052  ...,  0.45656519  0.46048676\n",
      "  0.46440833]\n",
      "[ 0.43524834  0.4274052   0.42348363 ...,  0.47895018  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.42348363  0.42348363  0.42348363 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42040131  0.41647974  0.41647974 ...,  0.45485618  0.45485618\n",
      "  0.45877775]\n",
      "[ 0.42348363  0.4274052   0.43132677 ...,  0.45934234  0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42432288  0.42824445  0.43216602 ...,  0.45542077  0.45542077\n",
      "  0.45542077]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.4632639   0.45934234\n",
      "  0.45934234]\n",
      "[ 0.42040131  0.43216602  0.44000916 ...,  0.46718547  0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.43216602  0.43608759  0.43216602 ...,  0.46718547  0.4632639   0.4632639 ]\n",
      "[ 0.42824445  0.43216602  0.43608759 ...,  0.47895018  0.48287175\n",
      "  0.47502861]\n",
      "[ 0.41255818  0.41255818  0.41647974 ...,  0.45177386  0.44785229\n",
      "  0.44785229]\n",
      "[ 0.42040131  0.42040131  0.42432288 ...,  0.459617    0.46353857\n",
      "  0.4713817 ]\n",
      "[ 0.42432288  0.41647974  0.41255818 ...,  0.44393072  0.459617    0.46353857]\n",
      "[ 0.41647974  0.42040131  0.42040131 ...,  0.45569543  0.45569543\n",
      "  0.45177386]\n",
      "[ 0.41647974  0.41647974  0.41647974 ...,  0.44785229  0.44000916\n",
      "  0.44785229]\n",
      "[ 0.41255818  0.40863661  0.40079347 ...,  0.459617    0.45569543  0.459617  ]\n",
      "[ 0.44785229  0.43216602  0.41647974 ...,  0.4514992   0.4514992\n",
      "  0.45542077]\n",
      "[ 0.42432288  0.42824445  0.43608759 ...,  0.45934234  0.4632639\n",
      "  0.46718547]\n",
      "[ 0.44000916  0.43608759  0.42432288 ...,  0.4514992   0.45542077\n",
      "  0.45934234]\n",
      "[ 0.43216602  0.43608759  0.43216602 ...,  0.4632639   0.47110704\n",
      "  0.4632639 ]\n",
      "[ 0.42824445  0.43216602  0.42824445 ...,  0.4514992   0.4514992   0.4514992 ]\n",
      "[ 0.42824445  0.42824445  0.42824445 ...,  0.4632639   0.4632639\n",
      "  0.46718547]\n",
      "[ 0.43216602  0.42824445  0.41255818 ...,  0.47502861  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.42040131  0.43216602  0.44000916 ...,  0.47110704  0.47110704\n",
      "  0.4632639 ]\n",
      "[ 0.43608759  0.42824445  0.43216602 ...,  0.45542077  0.45934234\n",
      "  0.4632639 ]\n",
      "[ 0.42432288  0.41647974  0.42040131 ...,  0.45485618  0.45485618\n",
      "  0.45485618]\n",
      "[ 0.43216602  0.42040131  0.41647974 ...,  0.4632639   0.4632639   0.4632639 ]\n",
      "[ 0.43608759  0.43216602  0.43216602 ...,  0.46353857  0.46353857\n",
      "  0.46353857]\n",
      "[ 0.43216602  0.42432288  0.42040131 ...,  0.46746014  0.46353857\n",
      "  0.45569543]\n",
      "[ 0.43608759  0.43216602  0.42824445 ...,  0.44393072  0.44785229\n",
      "  0.46746014]\n",
      "[ 0.43216602  0.42824445  0.42040131 ...,  0.46353857  0.45569543\n",
      "  0.44393072]\n",
      "[ 0.42432288  0.42432288  0.42432288 ...,  0.45177386  0.45177386\n",
      "  0.45177386]\n",
      "[ 0.42824445  0.42040131  0.41647974 ...,  0.45877775  0.45485618\n",
      "  0.45093462]\n",
      "[ 0.44393072  0.44393072  0.43608759 ...,  0.48230716  0.47054246\n",
      "  0.46662089]\n",
      "[ 0.43216602  0.43608759  0.44000916 ...,  0.46662089  0.47054246\n",
      "  0.47054246]\n",
      "[ 0.44000916  0.43608759  0.43608759 ...,  0.45877775  0.46269932\n",
      "  0.45877775]\n",
      "[ 0.43608759  0.43608759  0.43608759 ...,  0.47054246  0.47054246\n",
      "  0.47054246]\n",
      "[ 0.51734188  0.52126345  0.52518502 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.52126345  0.52518502  0.52518502 ...,  0.54145113  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.53920806  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.51792172  0.51400015  0.51792172 ...,  0.54312963  0.53920806\n",
      "  0.53136492]\n",
      "[ 0.52184329  0.51792172  0.51792172 ...,  0.53528649  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.533608    0.53752956  0.54145113 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.533608    0.52968643  0.52968643 ...,  0.55489433  0.55097276\n",
      "  0.54705119]\n",
      "[ 0.52100404  0.52100404  0.52100404 ...,  0.52941176  0.52941176\n",
      "  0.52941176]\n",
      "[ 0.52492561  0.52492561  0.52492561 ...,  0.5372549   0.54117647\n",
      "  0.5372549 ]\n",
      "[ 0.52100404  0.52100404  0.51708248 ...,  0.52941176  0.5254902   0.5254902 ]\n",
      "[ 0.51316091  0.50923934  0.51316091 ...,  0.52941176  0.53333333\n",
      "  0.53333333]\n",
      "[ 0.50949874  0.51342031  0.51342031 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.52941176  0.5254902   0.5254902 ]\n",
      "[ 0.51342031  0.50949874  0.51342031 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52910658  0.52126345  0.52126345 ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.52518502  0.51342031  0.50557717 ...,  0.54145113  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.51734188  0.51734188  0.52126345 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.52126345  0.51734188  0.51342031 ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.54145113  0.53752956  0.533608  ]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52184329  0.52576486  0.52576486 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.52518502  0.52126345  0.51734188 ...,  0.533608    0.52968643\n",
      "  0.52576486]\n",
      "[ 0.52968643  0.52968643  0.53752956 ...,  0.55321584  0.56105898\n",
      "  0.55713741]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.52126345  0.52518502  0.52518502 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.52518502  0.52518502  0.52126345 ...,  0.53752956  0.533608    0.533608  ]\n",
      "[ 0.52126345  0.52518502  0.52910658 ...,  0.53752956  0.53752956  0.533608  ]\n",
      "[ 0.52910658  0.52910658  0.52518502 ...,  0.53752956  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.52518502  0.52126345  0.51342031 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.52126345  0.50557717  0.5016556  ...,  0.54145113  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52518502  0.52518502  0.52910658 ...,  0.54145113  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.55321584  0.55321584  0.54929427 ...,  0.55713741  0.55713741\n",
      "  0.56105898]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.55321584  0.55713741\n",
      "  0.56105898]\n",
      "[ 0.53752956  0.53752956  0.54145113 ...,  0.5453727   0.5453727\n",
      "  0.54929427]\n",
      "[ 0.53752956  0.533608    0.52968643 ...,  0.5453727   0.55321584\n",
      "  0.5453727 ]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.53752956  0.54145113\n",
      "  0.5453727 ]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.52576486  0.52184329\n",
      "  0.51792172]\n",
      "[ 0.52126345  0.52126345  0.52126345 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.51734188  0.52126345  0.52126345 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.54145113  0.54145113  0.533608  ]\n",
      "[ 0.52518502  0.52126345  0.52126345 ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.52184329  0.52576486  0.52576486 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.53694972  0.53694972  0.53694972 ...,  0.54145113  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.53920806  0.53920806  0.54312963 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.5588159   0.55489433  0.55489433 ...,  0.5588159   0.5588159   0.5588159 ]\n",
      "[ 0.54705119  0.54312963  0.53920806 ...,  0.54312963  0.54312963\n",
      "  0.55097276]\n",
      "[ 0.53920806  0.53528649  0.53528649 ...,  0.54705119  0.54312963\n",
      "  0.53920806]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.53528649  0.52744335\n",
      "  0.52352178]\n",
      "[ 0.533608    0.53752956  0.52968643 ...,  0.53333333  0.53333333\n",
      "  0.5372549 ]\n",
      "[ 0.533608    0.53752956  0.52968643 ...,  0.54117647  0.5372549\n",
      "  0.53333333]\n",
      "[ 0.52968643  0.52968643  0.533608   ...,  0.54117647  0.54509804\n",
      "  0.54901961]\n",
      "[ 0.533608    0.53752956  0.53752956 ...,  0.54117647  0.54117647\n",
      "  0.54117647]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.54509804  0.54509804\n",
      "  0.54117647]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.52744335  0.53136492\n",
      "  0.53920806]\n",
      "[ 0.52968643  0.52968643  0.52576486 ...,  0.53528649  0.53136492\n",
      "  0.54705119]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.52576486  0.52184329\n",
      "  0.54145113]\n",
      "[ 0.54145113  0.54145113  0.53752956 ...,  0.53136492  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.52184329  0.52576486  0.52184329 ...,  0.53528649  0.53136492\n",
      "  0.52744335]\n",
      "[ 0.533608    0.533608    0.53752956 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.5453727   0.54145113  0.53752956 ...,  0.55097276  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53752956  0.54145113  0.54145113 ...,  0.55097276  0.53528649\n",
      "  0.54312963]\n",
      "[ 0.52968643  0.52968643  0.53136492 ...,  0.53920806  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.52576486  0.51792172  0.51400015 ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.533608    0.533608    0.54145113 ...,  0.53136492  0.53136492\n",
      "  0.53528649]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.54369421  0.53977264\n",
      "  0.54369421]\n",
      "[ 0.52184329  0.52184329  0.533608   ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.53752956  0.5453727   0.54145113 ...,  0.54145113  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.53752956  0.54145113\n",
      "  0.53752956]\n",
      "[ 0.51792172  0.52968643  0.53752956 ...,  0.53585107  0.53585107\n",
      "  0.53585107]\n",
      "[ 0.51792172  0.52576486  0.52576486 ...,  0.53920806  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.53528649  0.53920806\n",
      "  0.54312963]\n",
      "[ 0.533608    0.52968643  0.52968643 ...,  0.53528649  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.5453727   0.5453727   0.54145113 ...,  0.53136492  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.52576486  0.52184329  0.52576486 ...,  0.533608    0.533608    0.54145113]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.53920806  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54145113  0.53752956  0.5453727  ...,  0.55489433  0.55489433\n",
      "  0.55489433]\n",
      "[ 0.55713741  0.55321584  0.55321584 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.55321584  0.55321584  0.54145113 ...,  0.55489433  0.55489433\n",
      "  0.55489433]\n",
      "[ 0.54929427  0.54929427  0.5453727  ...,  0.56273747  0.5588159\n",
      "  0.55489433]\n",
      "[ 0.55713741  0.55713741  0.55713741 ...,  0.55489433  0.5588159\n",
      "  0.55489433]\n",
      "[ 0.533608    0.533608    0.52968643 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.53694972  0.54087129  0.53694972 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.53302815  0.53302815  0.53302815 ...,  0.5453727   0.5453727   0.5453727 ]\n",
      "[ 0.53302815  0.53694972  0.53694972 ...,  0.533608    0.533608    0.533608  ]\n",
      "[ 0.53302815  0.52910658  0.52910658 ...,  0.53752956  0.54145113\n",
      "  0.54145113]\n",
      "[ 0.43973449  0.43973449  0.43973449 ...,  0.47110704  0.47502861\n",
      "  0.47110704]\n",
      "[ 0.45093462  0.45485618  0.45093462 ...,  0.47895018  0.47895018\n",
      "  0.47895018]\n",
      "[ 0.45934234  0.45934234  0.45934234 ...,  0.47559319  0.47951476\n",
      "  0.47951476]\n",
      "[ 0.45598535  0.45206378  0.45598535 ...,  0.47559319  0.47951476\n",
      "  0.47951476]\n",
      "[ 0.47110704  0.46718547  0.4632639  ...,  0.49127947  0.4873579\n",
      "  0.48343633]\n",
      "[ 0.4632639   0.4632639   0.46718547 ...,  0.4873579   0.4873579   0.4873579 ]\n",
      "[ 0.4514992   0.45542077  0.45542077 ...,  0.4873579   0.4873579\n",
      "  0.49127947]\n",
      "[ 0.44480049  0.44480049  0.44872206 ...,  0.47000839  0.4817731\n",
      "  0.48961624]\n",
      "[ 0.44480049  0.44087892  0.43695735 ...,  0.46216526  0.46608682\n",
      "  0.47392996]\n",
      "[ 0.44087892  0.44872206  0.44480049 ...,  0.47785153  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.45656519  0.46048676  0.46048676 ...,  0.47000839  0.47000839\n",
      "  0.47392996]\n",
      "[ 0.43189136  0.44365606  0.44757763 ...,  0.4632639   0.45934234\n",
      "  0.46718547]\n",
      "[ 0.44480049  0.45264363  0.45264363 ...,  0.46608682  0.46608682\n",
      "  0.46608682]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.45934234  0.45542077  0.45542077 ...,  0.47559319  0.46775006\n",
      "  0.45990692]\n",
      "[ 0.44757763  0.44365606  0.44365606 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.46718547  0.45934234  0.4514992  ...,  0.47951476  0.47951476\n",
      "  0.4873579 ]\n",
      "[ 0.44757763  0.4514992   0.4514992  ...,  0.47167163  0.47951476\n",
      "  0.48343633]\n",
      "[ 0.4514992   0.4514992   0.45542077 ...,  0.47392996  0.47392996\n",
      "  0.47392996]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47392996  0.47785153\n",
      "  0.47785153]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.47785153  0.47000839\n",
      "  0.46608682]\n",
      "[ 0.45542077  0.45542077  0.45542077 ...,  0.48961624  0.4935378\n",
      "  0.49745937]\n",
      "[ 0.43581292  0.43581292  0.43581292 ...,  0.47110704  0.47110704\n",
      "  0.46718547]\n",
      "[ 0.4514992   0.45542077  0.43973449 ...,  0.4935378   0.48961624\n",
      "  0.4817731 ]\n",
      "[ 0.43973449  0.43581292  0.43973449 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.4514992   0.44365606  0.43189136 ...,  0.46775006  0.46382849\n",
      "  0.46382849]\n",
      "[ 0.43973449  0.43581292  0.43973449 ...,  0.46382849  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.43973449  0.43973449  0.43581292 ...,  0.47167163  0.47167163\n",
      "  0.47167163]\n",
      "[ 0.4514992   0.4514992   0.45542077 ...,  0.48343633  0.49127947\n",
      "  0.48343633]\n",
      "[ 0.44757763  0.45542077  0.45934234 ...,  0.46775006  0.46775006\n",
      "  0.46775006]\n",
      "[ 0.4514992   0.4514992   0.44757763 ...,  0.47167163  0.47167163\n",
      "  0.47951476]\n",
      "[ 0.44757763  0.45542077  0.4632639  ...,  0.47559319  0.47559319\n",
      "  0.47559319]\n",
      "[ 0.44757763  0.44365606  0.43581292 ...,  0.4632639   0.46718547\n",
      "  0.4632639 ]\n",
      "[ 0.44757763  0.4514992   0.44757763 ...,  0.47167163  0.46775006\n",
      "  0.47167163]\n",
      "[ 0.47110704  0.47110704  0.47110704 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.45934234  0.4632639   0.47110704 ...,  0.4873579   0.4873579\n",
      "  0.49127947]\n",
      "[ 0.45934234  0.45934234  0.45542077 ...,  0.4873579   0.49127947\n",
      "  0.49520104]\n",
      "[ 0.45934234  0.4632639   0.46718547 ...,  0.48961624  0.4935378   0.4935378 ]\n",
      "[ 0.45542077  0.4514992   0.4514992  ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.43581292  0.43189136  0.43973449 ...,  0.46718547  0.46718547\n",
      "  0.46718547]\n",
      "[ 0.45093462  0.44701305  0.45093462 ...,  0.47502861  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.44701305  0.44701305  0.44701305 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44701305  0.45093462  0.43916991 ...,  0.47502861  0.47502861\n",
      "  0.47502861]\n",
      "[ 0.44757763  0.44757763  0.44365606 ...,  0.47110704  0.47110704\n",
      "  0.47110704]\n",
      "[ 0.49520104  0.49127947  0.49520104 ...,  0.51734188  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.50530251  0.50530251  0.50530251 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50557717  0.50557717  0.50557717 ...,  0.52968643  0.53752956\n",
      "  0.52968643]\n",
      "[ 0.51734188  0.51342031  0.50949874 ...,  0.52184329  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.50949874  0.50949874  0.50949874 ...,  0.51792172  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50949874  0.50557717  0.50557717 ...,  0.52968643  0.533608    0.53752956]\n",
      "[ 0.51734188  0.51342031  0.51342031 ...,  0.51792172  0.52184329\n",
      "  0.52576486]\n",
      "[ 0.49579614  0.49579614  0.49971771 ...,  0.52884718  0.52884718\n",
      "  0.52492561]\n",
      "[ 0.49971771  0.50363928  0.50756085 ...,  0.52492561  0.52884718\n",
      "  0.52884718]\n",
      "[ 0.50756085  0.50756085  0.50756085 ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.49187457  0.49187457  0.49579614 ...,  0.52100404  0.52100404\n",
      "  0.51708248]\n",
      "[ 0.50304417  0.49912261  0.49912261 ...,  0.51734188  0.51342031\n",
      "  0.50949874]\n",
      "[ 0.49187457  0.51148241  0.50363928 ...,  0.51792172  0.51792172\n",
      "  0.51792172]\n",
      "[ 0.5016556   0.50557717  0.50949874 ...,  0.52576486  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.4898909   0.4898909   0.49381247 ...,  0.52968643  0.52576486\n",
      "  0.52184329]\n",
      "[ 0.5016556   0.5016556   0.5016556  ...,  0.52184329  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.49773404  0.5016556   0.5016556  ...,  0.52576486  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.49773404  0.49773404  0.50557717 ...,  0.52576486  0.52968643\n",
      "  0.52184329]\n",
      "[ 0.51342031  0.51342031  0.51342031 ...,  0.52126345  0.52518502\n",
      "  0.53302815]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.51342031  0.51734188\n",
      "  0.52126345]\n",
      "[ 0.49773404  0.5016556   0.5016556  ...,  0.52518502  0.52518502\n",
      "  0.52126345]\n",
      "[ 0.50949874  0.50557717  0.50557717 ...,  0.54087129  0.53694972\n",
      "  0.53694972]\n",
      "[ 0.49912261  0.49912261  0.49912261 ...,  0.51734188  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50557717  0.50949874  0.51734188 ...,  0.51734188  0.51342031\n",
      "  0.51342031]\n",
      "[ 0.50530251  0.50922408  0.50922408 ...,  0.51792172  0.51400015\n",
      "  0.51792172]\n",
      "[ 0.50557717  0.49381247  0.49773404 ...,  0.52968643  0.53752956  0.533608  ]\n",
      "[ 0.4935378   0.4935378   0.50138094 ...,  0.51734188  0.52126345\n",
      "  0.52518502]\n",
      "[ 0.49381247  0.49773404  0.5016556  ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.50530251  0.50530251  0.50530251 ...,  0.51400015  0.51400015\n",
      "  0.51792172]\n",
      "[ 0.5016556   0.5016556   0.5016556  ...,  0.55321584  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.51342031  0.51342031  0.50949874 ...,  0.51792172  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.5016556   0.5016556   0.50949874 ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.4898909   0.49381247  0.5016556  ...,  0.52968643  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.50304417  0.50304417  0.49520104 ...,  0.50949874  0.50557717\n",
      "  0.51342031]\n",
      "[ 0.49381247  0.50557717  0.5016556  ...,  0.52184329  0.52184329\n",
      "  0.52184329]\n",
      "[ 0.51342031  0.51342031  0.51342031 ...,  0.5453727   0.54145113\n",
      "  0.54145113]\n",
      "[ 0.51734188  0.50949874  0.50949874 ...,  0.54145113  0.53752956  0.533608  ]\n",
      "[ 0.52518502  0.51734188  0.51342031 ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.51342031  0.51342031  0.50949874 ...,  0.54929427  0.54929427\n",
      "  0.54145113]\n",
      "[ 0.51342031  0.51734188  0.52126345 ...,  0.533608    0.53752956\n",
      "  0.53752956]\n",
      "[ 0.49127947  0.49912261  0.50304417 ...,  0.51342031  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.50530251  0.50138094  0.50138094 ...,  0.52910658  0.52518502\n",
      "  0.52126345]\n",
      "[ 0.49745937  0.50138094  0.50530251 ...,  0.51734188  0.50557717\n",
      "  0.5016556 ]\n",
      "[ 0.50138094  0.49745937  0.49745937 ...,  0.52126345  0.52518502\n",
      "  0.52910658]\n",
      "[ 0.4935378   0.4935378   0.49745937 ...,  0.52518502  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.47225147  0.47617304  0.47617304 ...,  0.4898909   0.4898909\n",
      "  0.48596933]\n",
      "[ 0.47617304  0.47225147  0.47225147 ...,  0.48961624  0.49745937\n",
      "  0.49745937]\n",
      "[ 0.47951476  0.4873579   0.48343633 ...,  0.50949874  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4873579   0.47167163  0.4873579  ...,  0.51342031  0.50949874\n",
      "  0.50949874]\n",
      "[ 0.4873579   0.4873579   0.48343633 ...,  0.5016556   0.5016556   0.5016556 ]\n",
      "[ 0.47167163  0.47951476  0.4873579  ...,  0.50557717  0.50949874\n",
      "  0.51342031]\n",
      "[ 0.4873579   0.4873579   0.4873579  ...,  0.5016556   0.49773404\n",
      "  0.49773404]\n",
      "[ 0.47392996  0.47392996  0.47785153 ...,  0.49579614  0.49971771\n",
      "  0.48403143]\n",
      "[ 0.47392996  0.47392996  0.47392996 ...,  0.49579614  0.49971771\n",
      "  0.49971771]\n",
      "[ 0.47392996  0.47785153  0.47392996 ...,  0.49579614  0.49971771\n",
      "  0.49971771]\n",
      "[ 0.47785153  0.47785153  0.47392996 ...,  0.50363928  0.50756085\n",
      "  0.50756085]\n",
      "[ 0.47617304  0.47225147  0.47225147 ...,  0.49381247  0.49381247\n",
      "  0.4898909 ]\n",
      "[ 0.47785153  0.47785153  0.47392996 ...,  0.49971771  0.49579614\n",
      "  0.49579614]\n",
      "[ 0.47000839  0.46608682  0.46608682 ...,  0.49381247  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.4817731   0.48569467  0.4817731  ...,  0.4898909   0.49381247\n",
      "  0.5016556 ]\n",
      "[ 0.48569467  0.4817731   0.4817731  ...,  0.49773404  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.47392996  0.47392996  0.47392996 ...,  0.50557717  0.50557717\n",
      "  0.51342031]\n",
      "[ 0.47000839  0.47392996  0.47392996 ...,  0.49773404  0.49773404\n",
      "  0.49381247]\n",
      "[ 0.47559319  0.47167163  0.47167163 ...,  0.50949874  0.50949874\n",
      "  0.50557717]\n",
      "[ 0.47785153  0.47785153  0.47392996 ...,  0.5016556   0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4817731   0.48569467  0.48961624 ...,  0.5016556   0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4817731   0.47785153  0.47392996 ...,  0.50557717  0.49773404\n",
      "  0.49773404]\n",
      "[ 0.4683299   0.47225147  0.47617304 ...,  0.4898909   0.4898909   0.4898909 ]\n",
      "[ 0.48343633  0.48343633  0.48343633 ...,  0.49381247  0.49381247\n",
      "  0.50949874]\n",
      "[ 0.47617304  0.47617304  0.48009461 ...,  0.48596933  0.48204776\n",
      "  0.49381247]\n",
      "[ 0.48009461  0.48009461  0.47617304 ...,  0.49773404  0.5016556   0.5016556 ]\n",
      "[ 0.4683299   0.4683299   0.4683299  ...,  0.4898909   0.4898909\n",
      "  0.49381247]\n",
      "[ 0.47785153  0.47392996  0.47392996 ...,  0.5016556   0.50949874\n",
      "  0.50557717]\n",
      "[ 0.47617304  0.47617304  0.47617304 ...,  0.4898909   0.4898909   0.4898909 ]\n",
      "[ 0.47785153  0.4817731   0.48569467 ...,  0.50557717  0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4817731   0.4817731   0.47785153 ...,  0.49773404  0.49773404\n",
      "  0.5016556 ]\n",
      "[ 0.47951476  0.47951476  0.47951476 ...,  0.4898909   0.49773404\n",
      "  0.50557717]\n",
      "[ 0.48569467  0.48569467  0.4817731  ...,  0.5016556   0.50557717\n",
      "  0.50557717]\n",
      "[ 0.4683299   0.46440833  0.46440833 ...,  0.49773404  0.49381247\n",
      "  0.49381247]\n",
      "[ 0.4817731   0.47785153  0.47785153 ...,  0.4898909   0.49381247\n",
      "  0.49773404]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.5016556   0.50557717\n",
      "  0.50557717]\n",
      "[ 0.48961624  0.48569467  0.48569467 ...,  0.5016556   0.49773404\n",
      "  0.49381247]\n",
      "[ 0.48343633  0.4873579   0.49127947 ...,  0.5016556   0.50949874\n",
      "  0.51342031]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.51342031  0.50949874\n",
      "  0.50949874]\n",
      "[ 0.48569467  0.48569467  0.48569467 ...,  0.50557717  0.5016556   0.5016556 ]\n",
      "[ 0.46440833  0.4683299   0.47225147 ...,  0.4898909   0.4898909\n",
      "  0.49381247]\n",
      "[ 0.48009461  0.48401617  0.48793774 ...,  0.49745937  0.50138094\n",
      "  0.50138094]\n",
      "[ 0.48793774  0.48009461  0.47225147 ...,  0.48961624  0.50138094\n",
      "  0.50138094]\n",
      "[ 0.48009461  0.48009461  0.48009461 ...,  0.50530251  0.49745937\n",
      "  0.48961624]\n",
      "[ 0.48793774  0.48793774  0.48793774 ...,  0.49745937  0.50530251\n",
      "  0.50138094]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.54312963  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.54145113  0.5453727   0.5453727  ...,  0.53752956  0.53752956\n",
      "  0.53752956]\n",
      "[ 0.54087129  0.54087129  0.54087129 ...,  0.52352178  0.51960021\n",
      "  0.51960021]\n",
      "[ 0.54087129  0.53302815  0.54087129 ...,  0.52744335  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53302815  0.53302815  0.54087129 ...,  0.52352178  0.52744335\n",
      "  0.52744335]\n",
      "[ 0.53694972  0.53694972  0.52910658 ...,  0.51960021  0.52744335\n",
      "  0.53136492]\n",
      "[ 0.53302815  0.53302815  0.53694972 ...,  0.53528649  0.53528649\n",
      "  0.53528649]\n",
      "[ 0.53920806  0.53920806  0.53528649 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54705119  0.54312963\n",
      "  0.53528649]\n",
      "[ 0.55489433  0.55489433  0.55489433 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.53528649  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54705119  0.54705119 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53528649  0.53528649  0.53920806 ...,  0.54312963  0.53920806\n",
      "  0.53528649]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.53920806  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.53920806  0.54312963  0.54312963 ...,  0.53920806  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.53528649  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.54312963  0.54705119 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.52576486  0.52184329\n",
      "  0.52968643]\n",
      "[ 0.52744335  0.52744335  0.52744335 ...,  0.52576486  0.52184329\n",
      "  0.51792172]\n",
      "[ 0.53136492  0.52744335  0.52744335 ...,  0.51567864  0.51960021\n",
      "  0.52744335]\n",
      "[ 0.53920806  0.53920806  0.53920806 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.54312963  0.53920806  0.53920806 ...,  0.52744335  0.52744335\n",
      "  0.53528649]\n",
      "[ 0.53528649  0.51960021  0.52352178 ...,  0.533608    0.533608    0.53752956]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54312963  0.53920806\n",
      "  0.53920806]\n",
      "[ 0.55097276  0.54705119  0.53920806 ...,  0.54312963  0.54312963\n",
      "  0.54705119]\n",
      "[ 0.54312963  0.54705119  0.55097276 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.54705119  0.54705119  0.54705119 ...,  0.54312963  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53528649  0.53136492  0.53136492 ...,  0.55489433  0.55097276\n",
      "  0.55097276]\n",
      "[ 0.53528649  0.53528649  0.54312963 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54312963  0.54312963  0.54312963 ...,  0.54705119  0.54705119\n",
      "  0.54312963]\n",
      "[ 0.53920806  0.53528649  0.53920806 ...,  0.54705119  0.54705119\n",
      "  0.54705119]\n",
      "[ 0.53920806  0.53528649  0.53136492 ...,  0.54705119  0.54705119\n",
      "  0.55097276]\n",
      "[ 0.53528649  0.53528649  0.53528649 ...,  0.54312963  0.54312963\n",
      "  0.54312963]\n",
      "[ 0.54479286  0.54871443  0.54871443 ...,  0.51960021  0.51567864\n",
      "  0.51567864]\n",
      "[ 0.54087129  0.54479286  0.54871443 ...,  0.53528649  0.53528649\n",
      "  0.53136492]\n",
      "[ 0.54479286  0.54479286  0.53694972 ...,  0.53528649  0.53528649\n",
      "  0.53920806]\n",
      "[ 0.54479286  0.53694972  0.52910658 ...,  0.52744335  0.53136492\n",
      "  0.53136492]\n",
      "[ 0.54479286  0.54479286  0.54087129 ...,  0.53136492  0.52744335\n",
      "  0.53136492]\n",
      "[ 0.53136492  0.53528649  0.53920806 ...,  0.54312963  0.53528649\n",
      "  0.52744335]\n",
      "[ 0.53752956  0.53752956  0.5453727  ...,  0.54145113  0.53752956\n",
      "  0.54145113]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.5453727   0.5453727\n",
      "  0.54145113]\n",
      "[ 0.54145113  0.54145113  0.5453727  ...,  0.533608    0.53752956\n",
      "  0.54145113]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.55713741  0.55713741\n",
      "  0.55321584]\n",
      "[ 0.50836957  0.50836957  0.50836957 ...,  0.51480888  0.51873045\n",
      "  0.51088731]\n",
      "[ 0.51480888  0.51088731  0.51088731 ...,  0.50696574  0.50696574\n",
      "  0.50640116]\n",
      "[ 0.51229114  0.51621271  0.51621271 ...,  0.50107576  0.49715419\n",
      "  0.49715419]\n",
      "[ 0.52013428  0.51621271  0.51621271 ...,  0.48538949  0.49715419\n",
      "  0.49715419]\n",
      "[ 0.52797742  0.52405585  0.51621271 ...,  0.50499733  0.5089189   0.5089189 ]\n",
      "[ 0.51621271  0.51621271  0.52013428 ...,  0.50107576  0.50107576\n",
      "  0.49715419]\n",
      "[ 0.51229114  0.51229114  0.51229114 ...,  0.50499733  0.50107576\n",
      "  0.50107576]\n",
      "[ 0.51734188  0.52518502  0.52910658 ...,  0.51342031  0.50949874\n",
      "  0.50949874]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.51342031  0.51342031\n",
      "  0.51734188]\n",
      "[ 0.51342031  0.51342031  0.51734188 ...,  0.50557717  0.50557717\n",
      "  0.5016556 ]\n",
      "[ 0.51734188  0.50949874  0.50949874 ...,  0.49381247  0.49381247\n",
      "  0.49773404]\n",
      "[ 0.52013428  0.52013428  0.51621271 ...,  0.49912261  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.51734188  0.51734188  0.51734188 ...,  0.5016556   0.49381247\n",
      "  0.48596933]\n",
      "[ 0.51229114  0.50836957  0.50836957 ...,  0.50304417  0.50304417\n",
      "  0.50696574]\n",
      "[ 0.51229114  0.51229114  0.50836957 ...,  0.50696574  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.50836957  0.51229114  0.51621271 ...,  0.50304417  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.52405585  0.52013428  0.51621271 ...,  0.49912261  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.50836957  0.51229114  0.51229114 ...,  0.50696574  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.52405585  0.52013428  0.52013428 ...,  0.49912261  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.52013428  0.51621271  0.51229114 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.51621271  0.51621271  0.51229114 ...,  0.49912261  0.49912261\n",
      "  0.49912261]\n",
      "[ 0.52405585  0.51621271  0.51621271 ...,  0.49520104  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.50836957  0.52013428  0.52405585 ...,  0.50696574  0.51088731\n",
      "  0.50696574]\n",
      "[ 0.52013428  0.52797742  0.52013428 ...,  0.51088731  0.50304417\n",
      "  0.50304417]\n",
      "[ 0.50949874  0.50949874  0.51342031 ...,  0.51873045  0.51873045\n",
      "  0.51088731]\n",
      "[ 0.52910658  0.51734188  0.50949874 ...,  0.51088731  0.51088731\n",
      "  0.51088731]\n",
      "[ 0.52126345  0.51342031  0.51734188 ...,  0.51480888  0.51480888\n",
      "  0.50304417]\n",
      "[ 0.52126345  0.51734188  0.51734188 ...,  0.51480888  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.50949874  0.51342031  0.50949874 ...,  0.504448    0.504448    0.50836957]\n",
      "[ 0.51621271  0.50836957  0.52405585 ...,  0.50304417  0.50696574\n",
      "  0.50696574]\n",
      "[ 0.51621271  0.51621271  0.51621271 ...,  0.51088731  0.50696574\n",
      "  0.50304417]\n",
      "[ 0.51229114  0.51229114  0.51621271 ...,  0.51873045  0.51873045\n",
      "  0.51480888]\n",
      "[ 0.53189899  0.53189899  0.52405585 ...,  0.50304417  0.49912261\n",
      "  0.49520104]\n",
      "[ 0.504448    0.50836957  0.52013428 ...,  0.49912261  0.49520104\n",
      "  0.49520104]\n",
      "[ 0.52405585  0.52405585  0.52013428 ...,  0.51088731  0.51088731\n",
      "  0.51088731]\n",
      "[ 0.52405585  0.51621271  0.51621271 ...,  0.50107576  0.49323262\n",
      "  0.50107576]\n",
      "[ 0.53189899  0.52797742  0.52405585 ...,  0.50499733  0.50107576\n",
      "  0.50499733]\n",
      "[ 0.52797742  0.52405585  0.52405585 ...,  0.50107576  0.49323262\n",
      "  0.49715419]\n",
      "[ 0.51621271  0.51621271  0.51229114 ...,  0.50107576  0.48931106\n",
      "  0.49323262]\n",
      "[ 0.52405585  0.52405585  0.52405585 ...,  0.50107576  0.50107576\n",
      "  0.49715419]\n",
      "[ 0.51229114  0.504448    0.51229114 ...,  0.49520104  0.49912261\n",
      "  0.50696574]\n",
      "[ 0.53049516  0.53049516  0.53441672 ...,  0.50640116  0.49855802\n",
      "  0.49463645]\n",
      "[ 0.51480888  0.50304417  0.50304417 ...,  0.50443275  0.50051118\n",
      "  0.50051118]\n",
      "[ 0.51480888  0.51873045  0.51873045 ...,  0.50640116  0.49855802\n",
      "  0.50247959]\n",
      "[ 0.50696574  0.51088731  0.51480888 ...,  0.50640116  0.50640116\n",
      "  0.50247959]\n",
      "[ 0.50640116  0.50640116  0.50247959 ...,  0.46353857  0.46746014\n",
      "  0.4713817 ]\n",
      "[ 0.49855802  0.49463645  0.49463645 ...,  0.47922484  0.47922484\n",
      "  0.47530327]\n",
      "[ 0.48874647  0.48874647  0.49266804 ...,  0.45847257  0.45847257  0.454551  ]\n",
      "[ 0.4848249   0.4848249   0.4848249  ...,  0.45847257  0.45847257\n",
      "  0.46239414]\n",
      "[ 0.4848249   0.48874647  0.49266804 ...,  0.46746014  0.46353857\n",
      "  0.4713817 ]\n",
      "[ 0.49658961  0.49658961  0.49658961 ...,  0.46631571  0.46239414\n",
      "  0.46239414]\n",
      "[ 0.48874647  0.48874647  0.48874647 ...,  0.46631571  0.46239414\n",
      "  0.46239414]\n",
      "[ 0.50530251  0.51314565  0.52098878 ...,  0.48679332  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.4935378   0.49745937  0.49745937 ...,  0.48287175  0.48287175\n",
      "  0.48679332]\n",
      "[ 0.50922408  0.50138094  0.49745937 ...,  0.49071489  0.48287175\n",
      "  0.47895018]\n",
      "[ 0.50922408  0.51314565  0.50922408 ...,  0.47895018  0.48287175\n",
      "  0.48287175]\n",
      "[ 0.49463645  0.48679332  0.48679332 ...,  0.46746014  0.49883268\n",
      "  0.49883268]\n",
      "[ 0.49745937  0.49745937  0.50530251 ...,  0.49463645  0.49463645\n",
      "  0.48287175]\n",
      "[ 0.49658961  0.49658961  0.49266804 ...,  0.47530327  0.47922484\n",
      "  0.47922484]\n",
      "[ 0.50051118  0.50051118  0.49266804 ...,  0.47415885  0.48200198\n",
      "  0.47415885]\n",
      "[ 0.50835431  0.50835431  0.50443275 ...,  0.48314641  0.48314641\n",
      "  0.48314641]\n",
      "[ 0.4730602   0.4848249   0.49658961 ...,  0.47922484  0.48314641\n",
      "  0.48706798]\n",
      "[ 0.50443275  0.50051118  0.50051118 ...,  0.47922484  0.4713817\n",
      "  0.47922484]\n",
      "[ 0.48874647  0.49266804  0.49658961 ...,  0.46746014  0.459617    0.459617  ]\n",
      "[ 0.49266804  0.49266804  0.49266804 ...,  0.46746014  0.46746014\n",
      "  0.4713817 ]\n",
      "[ 0.48090333  0.48874647  0.47698177 ...,  0.4713817   0.4713817   0.4713817 ]\n",
      "[ 0.4848249   0.48874647  0.49266804 ...,  0.46353857  0.46353857\n",
      "  0.46353857]\n",
      "[ 0.50051118  0.51619745  0.50835431 ...,  0.49098955  0.48706798\n",
      "  0.48314641]\n",
      "[ 0.50051118  0.50835431  0.50835431 ...,  0.46746014  0.4713817   0.4713817 ]\n",
      "[ 0.50696574  0.51480888  0.51480888 ...,  0.49098955  0.49491112\n",
      "  0.49491112]\n",
      "[ 0.50696574  0.50696574  0.50304417 ...,  0.48706798  0.49098955\n",
      "  0.49491112]\n",
      "[ 0.51088731  0.50696574  0.51480888 ...,  0.48090333  0.48314641\n",
      "  0.48706798]\n",
      "[ 0.51480888  0.51480888  0.51873045 ...,  0.48706798  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.51088731  0.49912261  0.49520104 ...,  0.49098955  0.49491112\n",
      "  0.49491112]\n",
      "[ 0.49855802  0.51816587  0.49855802 ...,  0.48706798  0.48706798\n",
      "  0.49098955]\n",
      "[ 0.5142443   0.5142443   0.50640116 ...,  0.49098955  0.49098955\n",
      "  0.48706798]\n",
      "[ 0.48874647  0.49658961  0.51227588 ...,  0.49098955  0.49098955\n",
      "  0.49491112]\n",
      "[ 0.50247959  0.50247959  0.49855802 ...,  0.48706798  0.47922484\n",
      "  0.48706798]\n",
      "[ 0.50247959  0.49855802  0.50247959 ...,  0.48314641  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.52265202  0.51873045  0.51088731 ...,  0.49098955  0.49098955\n",
      "  0.49098955]\n",
      "[ 0.49266804  0.48874647  0.48874647 ...,  0.47530327  0.47530327\n",
      "  0.47530327]\n",
      "[ 0.49266804  0.49658961  0.49658961 ...,  0.4713817   0.4713817   0.4713817 ]\n",
      "[ 0.50051118  0.49658961  0.49658961 ...,  0.4713817   0.4713817\n",
      "  0.47530327]\n",
      "[ 0.48874647  0.49658961  0.49266804 ...,  0.459617    0.46353857\n",
      "  0.46353857]\n",
      "[ 0.49658961  0.49658961  0.49658961 ...,  0.47530327  0.47530327\n",
      "  0.4713817 ]\n",
      "[ 0.49855802  0.49855802  0.50247959 ...,  0.49098955  0.48706798\n",
      "  0.48706798]\n",
      "[ 0.50247959  0.50247959  0.50247959 ...,  0.4865034   0.48258183\n",
      "  0.47866026]\n",
      "[ 0.49463645  0.5142443   0.5142443  ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.49855802  0.49855802  0.49855802 ...,  0.47866026  0.47866026\n",
      "  0.47866026]\n",
      "[ 0.51032273  0.5142443   0.50640116 ...,  0.4814374   0.4814374\n",
      "  0.47359426]\n",
      "[ 0.52968643  0.52576486  0.52576486 ...,  0.52013428  0.51621271\n",
      "  0.52013428]\n",
      "[ 0.52910658  0.52910658  0.52518502 ...,  0.52797742  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.51229114  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.52576486  0.52576486  0.52968643 ...,  0.49660487  0.50052644  0.504448  ]\n",
      "[ 0.52576486  0.52968643  0.52968643 ...,  0.51229114  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.52184329  0.52576486  0.52968643 ...,  0.50836957  0.50836957  0.504448  ]\n",
      "[ 0.52184329  0.52184329  0.52576486 ...,  0.52405585  0.52013428\n",
      "  0.51621271]\n",
      "[ 0.53752956  0.533608    0.54145113 ...,  0.52576486  0.52184329\n",
      "  0.51792172]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.52968643  0.52968643\n",
      "  0.52576486]\n",
      "[ 0.5453727   0.5453727   0.533608   ...,  0.52968643  0.52968643\n",
      "  0.52968643]\n",
      "[ 0.53752956  0.53752956  0.53752956 ...,  0.52576486  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.52013428  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.5453727   0.54145113  0.53752956 ...,  0.52184329  0.52576486\n",
      "  0.52576486]\n",
      "[ 0.52657359  0.52657359  0.52657359 ...,  0.51229114  0.50836957\n",
      "  0.52013428]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.52405585  0.52013428\n",
      "  0.51734188]\n",
      "[ 0.53246357  0.53246357  0.53246357 ...,  0.51621271  0.51229114\n",
      "  0.51734188]\n",
      "[ 0.52797742  0.53189899  0.53582055 ...,  0.51229114  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.53049516  0.53441672  0.53833829 ...,  0.51229114  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.52184329  0.52968643  0.52968643 ...,  0.49660487  0.504448    0.50836957]\n",
      "[ 0.52576486  0.52184329  0.52184329 ...,  0.50052644  0.49660487\n",
      "  0.49660487]\n",
      "[ 0.52184329  0.52184329  0.52184329 ...,  0.504448    0.504448    0.50052644]\n",
      "[ 0.52184329  0.52968643  0.52968643 ...,  0.50836957  0.50052644\n",
      "  0.49660487]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.52013428  0.52013428\n",
      "  0.52013428]\n",
      "[ 0.52576486  0.52576486  0.52576486 ...,  0.51621271  0.50836957\n",
      "  0.50836957]\n",
      "[ 0.54145113  0.54145113  0.54145113 ...,  0.53302815  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.54145113  0.53752956  0.52968643 ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.53752956  0.53752956  0.533608   ...,  0.52910658  0.52910658\n",
      "  0.52518502]\n",
      "[ 0.53752956  0.52968643  0.53752956 ...,  0.52518502  0.52910658\n",
      "  0.52910658]\n",
      "[ 0.5453727   0.54145113  0.53752956 ...,  0.52126345  0.52518502\n",
      "  0.52518502]\n",
      "[ 0.54145113  0.533608    0.533608   ...,  0.52126345  0.52126345\n",
      "  0.52126345]\n",
      "[ 0.54145113  0.53752956  0.53752956 ...,  0.52126345  0.53302815\n",
      "  0.54087129]\n",
      "[ 0.51792172  0.51792172  0.52968643 ...,  0.52126345  0.51734188\n",
      "  0.51342031]\n",
      "[ 0.52968643  0.52968643  0.52968643 ...,  0.52126345  0.51734188\n",
      "  0.51734188]\n",
      "[ 0.52576486  0.52576486  0.533608   ...,  0.51621271  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.54145113  0.54145113  0.53752956 ...,  0.533608    0.533608    0.52968643]\n",
      "[ 0.53752956  0.533608    0.533608   ...,  0.51621271  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.533608    0.52968643  0.533608   ...,  0.52013428  0.51621271\n",
      "  0.50836957]\n",
      "[ 0.533608    0.533608    0.533608   ...,  0.51229114  0.52013428\n",
      "  0.52405585]\n",
      "[ 0.53752956  0.5453727   0.5453727  ...,  0.51229114  0.51229114\n",
      "  0.51229114]\n",
      "[ 0.52968643  0.52576486  0.52576486 ...,  0.51621271  0.51621271\n",
      "  0.51229114]\n",
      "[ 0.52968643  0.533608    0.533608   ...,  0.52405585  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.54087129  0.54087129  0.54087129 ...,  0.52013428  0.52405585\n",
      "  0.52013428]\n",
      "[ 0.52126345  0.52910658  0.52910658 ...,  0.51621271  0.51621271\n",
      "  0.51621271]\n",
      "[ 0.53302815  0.53302815  0.53302815 ...,  0.51229114  0.51229114\n",
      "  0.51621271]\n",
      "[ 0.52910658  0.52910658  0.52910658 ...,  0.51088731  0.51480888\n",
      "  0.52265202]\n"
     ]
    }
   ],
   "source": [
    "# %load prep_dataset.py\n",
    "import os\n",
    "import dataset\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#UCITAVA SLICICE U ONAJ FAJL\n",
    "\n",
    "rel_path = 'images'\n",
    "size = 22500 # 150x150 slicica\n",
    "\n",
    "folders = os.listdir(rel_path)\n",
    "\n",
    "kana_dict = dataset.characters()\n",
    "hiragana_dataset = dataset.HiraSet('dset', size)\n",
    "\n",
    "for folder in folders:\n",
    "    entry = dataset.HiraEntry(folder, kana_dict[folder])\n",
    "\n",
    "    files = os.listdir(rel_path + '/' + folder)\n",
    "    for file in files:\n",
    "       \n",
    "        #img = Image.open(rel_path + '/' + folder + '/' + file)\n",
    "        \n",
    "        #img = img.resize((50, 50), PIL.Image.ANTIALIAS)\n",
    "        #img.save(rel_path + '/' + folder + '/' + file)\n",
    "        img = imread(rel_path + '/' + folder + '/' + file)\n",
    "        re_img = np.reshape(img, size)\n",
    "        flt_img = re_img / 65535.0\n",
    "        print(flt_img)\n",
    "\n",
    "        entry.add(flt_img)\n",
    "\n",
    "    hiragana_dataset.add(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "dset = dataset.HiraSet('dset', 22500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dset.pull()\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new(25,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT nvcc warning : nvcc support for Microsoft Visual Studio 2010 and earlier has been deprecated and is no longer being maintained\n",
      "mod.cu\n",
      "support for Microsoft Visual Studio 2010 has been deprecated!\n",
      "   Creating library C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmppmlfy0t4/m848dd898e26d545ff6290e3aa98de3d5.lib and object C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmppmlfy0t4/m848dd898e26d545ff6290e3aa98de3d5.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT nvcc warning : nvcc support for Microsoft Visual Studio 2010 and earlier has been deprecated and is no longer being maintained\n",
      "mod.cu\n",
      "support for Microsoft Visual Studio 2010 has been deprecated!\n",
      "   Creating library C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmpzpryiou0/mcaee517fdbbfe5601d70389b5e9a720a.lib and object C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmpzpryiou0/mcaee517fdbbfe5601d70389b5e9a720a.exp\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x301ed978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 1, 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 1, 150, 150).astype('float32')\n",
    "train = train\n",
    "test = test\n",
    "\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, border_mode='valid', input_shape=(1, 150, 150), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(74, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "# Fit the model\n",
    "model.fit(train, train_labels, nb_epoch=300, batch_size=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "nsamples, nx, ny, nz = train.shape\n",
    "d2_train = train.reshape((nsamples,nx*ny*nz))\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 2)\n",
    "model.fit(d2_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.236486486486\n"
     ]
    }
   ],
   "source": [
    "nsamples, nx, ny, nz = test.shape\n",
    "d2_test = test.reshape((nsamples,nx*ny*nz))\n",
    "scores = model.score(d2_test, test_labels)\n",
    "print('test', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-1378c78a71ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Baseline Error: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "clf_knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "clf_knn.fit(d2_train, train_labels)\n",
    "y_pred_knn = clf_knn.predict(d2_test)\n",
    "acc_knn = accuracy_score(test_labels, y_pred_knn)\n",
    "print (\"nearest neighbors accuracy: \",acc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e9ce10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 1, 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 1, 150, 150).astype('float32')\n",
    "train = train\n",
    "test = test\n",
    "\n",
    "nsamples, nx, ny, nz = train.shape\n",
    "d3_train = train.reshape((nsamples,nx,ny*nz))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(2, batch_input_shape=(10, 1, 22500), input_dim=22500, stateful=True, return_sequences=True))\n",
    "model.add(LSTM(2, batch_input_shape=(10, 1, 22500), input_dim=22500, stateful=True))\n",
    "model.add(Dense(74))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(d3_train, train_labels, nb_epoch=300, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "nsamples, nx, ny, nz = test.shape\n",
    "d3_test = test.reshape((nsamples,nx,ny*nz))\n",
    "\n",
    "testPredict = model.predict(d3_test, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.12 RMSE\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "nsamples, nx, ny, nz = test.shape\n",
    "d2_test = test.reshape((nsamples,nx*ny*nz))\n",
    "scaler.fit_transform(testPredict)\n",
    "\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testL = scaler.inverse_transform(test_labels)\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(testL, testPredict))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0133163971454\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# prepare model\n",
    "model = Sequential()\n",
    "model.add(Dense(70, input_dim=22500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(140))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(50))\n",
    "#model.add(Activation('tanh'))\n",
    "model.add(Dense(74))\n",
    "model.add(Activation('relu'))\n",
    "# compile model with optimizer\n",
    "sgd = SGD(lr=0.1, decay=0.001, momentum=0.7)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "# training\n",
    "train=np.array(train)\n",
    "train_labels=np.array(train_labels)\n",
    "training = model.fit(d2_train, train_labels, nb_epoch=300, batch_size=222, verbose=0)\n",
    "print (training.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 1.26%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test, test_labels, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832/888 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain [0.74737350682954529, 0.79504504450806623]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on train data\n",
    "scores = model.evaluate(train, train_labels, verbose=1)\n",
    "print ('train', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nolearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-fc98e79d553f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDBN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_new_RNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nolearn'"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nolearn.dbn import DBN\n",
    "\n",
    "(train_data,test_data,train_labels,test_labels) = dset.require_new_RNN(25,20)\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "\n",
    "model = DBN([train_data.shape[1], 300, 74],learn_rates=0.3,learn_rate_decays=0.9,epochs=15)\n",
    "model.fit(train_data, train_labels)\n",
    "acc_nn = model.score(test_data,test_labels)\n",
    "print (\"Stochastic gradient descent accuracy: \",acc_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16cff4a8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAqSAkNBUmAoSAoO3XZyh9cdoGEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17509080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # za prikaz slika, grafika, itd.\n",
    "%matplotlib inline\n",
    "\n",
    "imgN = 32\n",
    "img = test[imgN]\n",
    "print (img.shape)\n",
    "img = img.reshape(150,150)\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/80 [=====================>........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b[  2.49056607e-01   2.49343276e-01   2.51412004e-01   2.50254691e-01\n",
      "  -5.89154661e-06   1.06939115e-06  -1.01104379e-05   3.35238874e-05\n",
      "   2.57045031e-06   2.52723694e-05   2.16811895e-06   1.70143321e-05\n",
      "   2.34004110e-05  -3.58819962e-05  -7.83242285e-06  -2.63750553e-06\n",
      "  -1.12093985e-05   2.19130889e-05   2.27130949e-05  -1.02762133e-05\n",
      "  -3.00025567e-05   4.71901149e-06  -1.12354755e-05  -6.20260835e-07\n",
      "  -3.56324017e-05  -1.19814649e-05   5.80493361e-06  -2.30045989e-05\n",
      "   4.64357436e-06  -1.28149986e-06   1.13663264e-05  -1.54078007e-05\n",
      "   1.12131238e-05  -3.10242176e-05  -1.41151249e-05  -2.31117010e-05\n",
      "   1.66594982e-05  -1.08852983e-05   2.52947211e-06   1.03935599e-06\n",
      "   1.04461797e-05  -6.37024641e-06  -2.51333695e-05  -6.77257776e-06\n",
      "  -1.47335231e-06  -1.09523535e-05   5.84870577e-06  -1.04913488e-06\n",
      "  -7.91288912e-05   1.13621354e-06  -2.85357237e-06   2.78651714e-06\n",
      "  -2.60007801e-05   4.03262675e-06   1.00024045e-05  -2.63154507e-05\n",
      "  -7.50180334e-06  -3.25255096e-05  -4.64171171e-06   3.13296914e-06\n",
      "   1.06357038e-05   1.85519457e-05   9.66805965e-06  -4.46662307e-06\n",
      "  -8.50530341e-06  -1.41784549e-05   1.21407211e-05   3.65450978e-06\n",
      "   1.61277130e-05  -1.31540000e-05  -1.03004277e-06  -1.33495778e-05\n",
      "   2.45738775e-05   1.06198713e-05]\n"
     ]
    }
   ],
   "source": [
    "t = model.predict(d3_test, verbose=1, batch_size=20)\n",
    "print (t[imgN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b31d3c7ad45a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimgN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"blue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVZ+Z6gsKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x370a4668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "    33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "    64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
    "plt.xticks(x)\n",
    "width = 1/1.5\n",
    "plt.bar(x, t[imgN], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rez_t = t.argmax(axis=1)\n",
    "print (rez_t[imgN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "print(dset.entries()[rez_t[imgN]].label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Current epoch: 1 | Number of Epochs: 1500 | Train loss: 7.37378 | Validation loss: 4.697162628173828 | Accuracy: 0.0135135135135\n",
      "Current epoch: 2 | Number of Epochs: 1500 | Train loss: 4.78402 | Validation loss: 4.566289901733398 | Accuracy: 0.00675675675676\n",
      "Current epoch: 3 | Number of Epochs: 1500 | Train loss: 4.67933 | Validation loss: 4.529451370239258 | Accuracy: 0.00675675675676\n",
      "Current epoch: 4 | Number of Epochs: 1500 | Train loss: 4.61876 | Validation loss: 4.462037086486816 | Accuracy: 0.0202702702703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e24a7f788687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mm_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import sklearn.metrics\n",
    "import lasagne.layers as L\n",
    "\n",
    "from lasagne.layers import InputLayer, LSTMLayer, ReshapeLayer, DenseLayer, GaussianNoiseLayer\n",
    "\n",
    "\n",
    "# Number of Units in hidden layers\n",
    "L1_UNITS = 50\n",
    "L2_UNITS = 100\n",
    "\n",
    "# Training Params \n",
    "LEARNING_RATE = 0.001\n",
    "N_BATCH = 10\n",
    "NUM_EPOCHS = 1500\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "(train,test,train_labels,test_labels) = dset.require_new_RNN(8,2)\n",
    "\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 150, 150).astype('float32')\n",
    "\n",
    "num_feat    = train.shape[1]\n",
    "num_classes = np.unique(test).size\n",
    "\n",
    "# Generate sequence masks (redundant here)\n",
    "mask_train = np.ones((train.shape[0], train.shape[1]))\n",
    "mask_test  = np.ones((test.shape[0], test.shape[1]))\n",
    "\n",
    "\n",
    "# Model\n",
    "tanh = lasagne.nonlinearities.tanh\n",
    "relu = lasagne.nonlinearities.rectify\n",
    "soft = lasagne.nonlinearities.softmax\n",
    "\n",
    "# Network Architecture\n",
    "l_in = InputLayer(shape=(None, None, num_feat))\n",
    "batchsize, seqlen, _ = l_in.input_var.shape\n",
    "\n",
    "l_noise = GaussianNoiseLayer(l_in, sigma=0.6) \n",
    "l_mask  = InputLayer(shape=(batchsize, seqlen))\n",
    "\n",
    "l_rnn_1 = LSTMLayer(l_noise, num_units=L1_UNITS, mask_input=l_mask)\n",
    "l_in_drop = lasagne.layers.DropoutLayer(l_rnn_1, p=0.25)\n",
    "l_rnn_2 = LSTMLayer(l_in_drop, num_units=L2_UNITS)\n",
    "l_in_drop2 = lasagne.layers.DropoutLayer(l_rnn_2, p=0.1)\n",
    "l_shp   = ReshapeLayer(l_in_drop2,(-1, L2_UNITS))\n",
    "l_dense = DenseLayer(l_shp, num_units=num_classes, nonlinearity=soft)\n",
    "l_out   = ReshapeLayer(l_dense, (batchsize, seqlen, num_classes)) \n",
    "\n",
    "\n",
    "# Symbols and Cost Function\n",
    "target_values = T.ivector('target_output')\n",
    "\n",
    "network_output   = L.get_output(l_out)\n",
    "predicted_values = network_output[:, -1]\n",
    "prediction = T.argmax(predicted_values, axis=1)\n",
    "all_params = L.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = lasagne.objectives.categorical_crossentropy(predicted_values, target_values)\n",
    "cost = cost.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Compute SGD updates for training\n",
    "print(\"Computing updates ...\")\n",
    "updates = lasagne.updates.rmsprop(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "# Theano functions for training and computing cost\n",
    "print(\"Compiling functions ...\")\n",
    "training   = theano.function(\n",
    "    [l_in.input_var, target_values, l_mask.input_var], cost, updates=updates,allow_input_downcast=True)\n",
    "predict = theano.function([l_in.input_var, l_mask.input_var], prediction,allow_input_downcast=True)\n",
    "compute_cost = theano.function([l_in.input_var, target_values, l_mask.input_var], cost,allow_input_downcast=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training\n",
    "print(\"Training ...\")\n",
    "num_batches_train = int(np.ceil(len(train) / N_BATCH))\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    now = time.time\n",
    "    losses = []        \n",
    "\n",
    "    batch_shuffle = np.random.choice(train.shape[0], train.shape[0], False)\n",
    "    sequences   = train[batch_shuffle]\n",
    "    labels      = train_labels[batch_shuffle]\n",
    "    train_masks = mask_train[batch_shuffle]\n",
    "\n",
    "    for batch in range(num_batches_train):\n",
    "        batch_slice = slice(N_BATCH * batch,\n",
    "                            N_BATCH * (batch + 1))\n",
    "        \n",
    "        X_batch = sequences[batch_slice]\n",
    "        y_batch = labels[batch_slice]\n",
    "        m_batch = train_masks[batch_slice]\n",
    "\n",
    "        loss = training(X_batch, y_batch, m_batch)\n",
    "        losses.append(loss)\n",
    "                \n",
    "    train_loss = np.mean(losses)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss = compute_cost(test, test_labels, mask_test)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    test_pred   = predict(test, mask_test)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_labels, test_pred)\n",
    "\n",
    "\n",
    "    print('Current epoch:', epoch+1,'|', 'Number of Epochs:', NUM_EPOCHS,'|','Train loss:', train_loss,'|','Validation loss:', valid_loss,'|','Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 0.272s\n",
      "  training loss:\t\t7.261192\n",
      "  validation loss:\t\t4.568270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2 of 10 took 0.241s\n",
      "  training loss:\t\t3.830766\n",
      "  validation loss:\t\t5.912274\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3 of 10 took 0.222s\n",
      "  training loss:\t\t3.453340\n",
      "  validation loss:\t\t1.592995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4 of 10 took 0.220s\n",
      "  training loss:\t\t1.934159\n",
      "  validation loss:\t\t3.680274\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 5 of 10 took 0.221s\n",
      "  training loss:\t\t2.569853\n",
      "  validation loss:\t\t1.606330\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 6 of 10 took 0.222s\n",
      "  training loss:\t\t1.535875\n",
      "  validation loss:\t\t1.493273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 7 of 10 took 0.234s\n",
      "  training loss:\t\t1.591092\n",
      "  validation loss:\t\t1.461871\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 8 of 10 took 0.226s\n",
      "  training loss:\t\t1.500916\n",
      "  validation loss:\t\t1.478548\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 9 of 10 took 0.243s\n",
      "  training loss:\t\t1.438344\n",
      "  validation loss:\t\t1.402445\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 10 of 10 took 0.218s\n",
      "  training loss:\t\t1.422702\n",
      "  validation loss:\t\t1.373957\n",
      "  validation accuracy:\t\t25.00 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "\n",
    "(train,test,train_labels,test_labels) = dset.require_new(25,20)\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 1, 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 1, 150, 150).astype('float32')\n",
    "num_epochs=10\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 150, 150), input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.1)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(l_in_drop, num_units=1600, nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.1)\n",
    "    l_hid2 = lasagne.layers.DenseLayer(l_hid1_drop, num_units=800, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.1)\n",
    "    l_out = lasagne.layers.DenseLayer(l_hid2_drop, num_units=74, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "# Create neural network model\n",
    "network = build_mlp(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var), dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(train, train_labels, 20, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(test, test_labels, 20, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "1 10 1.1016 0.6307196021080017 0.7948\n",
      "2 10 0.481657 0.35119104385375977 0.8871\n",
      "3 10 0.342173 0.29665690660476685 0.901\n",
      "4 10 0.276644 0.23253266513347626 0.9202\n",
      "5 10 0.244173 0.21249467134475708 0.9302\n",
      "6 10 0.214389 0.1950872391462326 0.9403\n",
      "7 10 0.200582 0.17442962527275085 0.9392\n",
      "8 10 0.186633 0.17781269550323486 0.9448\n",
      "9 10 0.172763 0.16662642359733582 0.9465\n",
      "10 10 0.168584 0.15904752910137177 0.9472\n"
     ]
    }
   ],
   "source": [
    "# Author: Justice Amoh\n",
    "# Date:   08/10/2015\n",
    "# Description: Script to train Recurrent Neural Network for MNIST Digit Recognition\n",
    "# Dependencies: Theano, Lasagne\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import sklearn.metrics\n",
    "import lasagne.layers as L\n",
    "\n",
    "from mnistT import load_dataset \n",
    "from lasagne.layers import InputLayer, LSTMLayer, ReshapeLayer, DenseLayer, GaussianNoiseLayer\n",
    "\n",
    "\n",
    "# Number of Units in hidden layers\n",
    "L1_UNITS = 50\n",
    "L2_UNITS = 100\n",
    "\n",
    "# Training Params \n",
    "LEARNING_RATE = 0.001\n",
    "N_BATCH = 100\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = load_dataset()\n",
    "\n",
    "\n",
    "X_train = np.squeeze(X_train)\n",
    "X_valid = np.squeeze(X_valid)\n",
    "X_test  = np.squeeze(X_test)\n",
    "\n",
    "num_feat    = X_train.shape[1]\n",
    "seq_len     = X_train.shape[2]\n",
    "num_classes = np.unique(y_train).size\n",
    "\n",
    "# Generate sequence masks (redundant here)\n",
    "mask_train = np.ones((X_train.shape[0], X_train.shape[1]))\n",
    "mask_valid = np.ones((X_valid.shape[0], X_valid.shape[1]))\n",
    "mask_test  = np.ones((X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "\n",
    "#################\n",
    "## BUILD MODEL ##\n",
    "#################\n",
    "tanh = lasagne.nonlinearities.tanh\n",
    "relu = lasagne.nonlinearities.rectify\n",
    "soft = lasagne.nonlinearities.softmax\n",
    "\n",
    "# Network Architecture\n",
    "l_in = InputLayer(shape=(None, None, num_feat))\n",
    "batchsize, seqlen, _ = l_in.input_var.shape\n",
    "\n",
    "l_noise = GaussianNoiseLayer(l_in, sigma=0.6) \n",
    "l_mask  = InputLayer(shape=(batchsize, seqlen))\n",
    "\n",
    "l_rnn_1 = LSTMLayer(l_noise, num_units=L1_UNITS, mask_input=l_mask)\n",
    "l_rnn_2 = LSTMLayer(l_rnn_1, num_units=L2_UNITS)\n",
    "l_shp   = ReshapeLayer(l_rnn_2,(-1, L2_UNITS))\n",
    "l_dense = DenseLayer(l_shp, num_units=num_classes, nonlinearity=soft)\n",
    "l_out   = ReshapeLayer(l_dense, (batchsize, seqlen, num_classes)) \n",
    "\n",
    "\n",
    "# Symbols and Cost Function\n",
    "target_values = T.ivector('target_output')\n",
    "\n",
    "network_output   = L.get_output(l_out)\n",
    "predicted_values = network_output[:, -1]\n",
    "prediction = T.argmax(predicted_values, axis=1)\n",
    "all_params = L.get_all_params(l_out, trainable=True)\n",
    "\n",
    "cost = lasagne.objectives.categorical_crossentropy(predicted_values, target_values)\n",
    "cost = cost.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Compute SGD updates for training\n",
    "print(\"Computing updates ...\")\n",
    "updates = lasagne.updates.rmsprop(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "# Theano functions for training and computing cost\n",
    "print(\"Compiling functions ...\")\n",
    "train   = theano.function(\n",
    "    [l_in.input_var, target_values, l_mask.input_var], cost, updates=updates,allow_input_downcast=True)\n",
    "predict = theano.function([l_in.input_var, l_mask.input_var], prediction, allow_input_downcast=True)\n",
    "compute_cost = theano.function([l_in.input_var, target_values, l_mask.input_var], cost, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "## TRAINING ##\n",
    "##############\n",
    "\n",
    "print(\"Training ...\")\n",
    "num_batches_train = int(np.ceil(len(X_train) / N_BATCH))\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    now = time.time\n",
    "    losses = []        \n",
    "\n",
    "    batch_shuffle = np.random.choice(X_train.shape[0], X_train.shape[0], False)\n",
    "    sequences   = X_train[batch_shuffle]\n",
    "    labels      = y_train[batch_shuffle]\n",
    "    train_masks = mask_train[batch_shuffle]\n",
    "\n",
    "    for batch in range(num_batches_train):\n",
    "        batch_slice = slice(N_BATCH * batch,\n",
    "                            N_BATCH * (batch + 1))\n",
    "        \n",
    "        X_batch = sequences[batch_slice]\n",
    "        y_batch = labels[batch_slice]\n",
    "        m_batch = train_masks[batch_slice]\n",
    "\n",
    "        loss = train(X_batch, y_batch, m_batch)\n",
    "        losses.append(loss)\n",
    "                \n",
    "    train_loss = np.mean(losses)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    valid_loss = compute_cost(X_valid, y_valid, mask_valid)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    y_pred   = predict(X_valid, mask_valid)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_valid, y_pred)\n",
    "\n",
    "\n",
    "    print(epoch+1, NUM_EPOCHS, train_loss, valid_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 5000 took 0.024s\n",
      "  training loss:\t\t4.156843\n",
      "  validation loss:\t\t2.754776\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2 of 5000 took 0.023s\n",
      "  training loss:\t\t2.934813\n",
      "  validation loss:\t\t2.940211\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3 of 5000 took 0.023s\n",
      "  training loss:\t\t3.250263\n",
      "  validation loss:\t\t4.972862\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4 of 5000 took 0.024s\n",
      "  training loss:\t\t4.520604\n",
      "  validation loss:\t\t7.495409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 5 of 5000 took 0.024s\n",
      "  training loss:\t\t7.710554\n",
      "  validation loss:\t\t4.356489\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 6 of 5000 took 0.024s\n",
      "  training loss:\t\t4.602727\n",
      "  validation loss:\t\t3.260150\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 7 of 5000 took 0.024s\n",
      "  training loss:\t\t3.373701\n",
      "  validation loss:\t\t2.237524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 8 of 5000 took 0.024s\n",
      "  training loss:\t\t2.372633\n",
      "  validation loss:\t\t1.985304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 9 of 5000 took 0.023s\n",
      "  training loss:\t\t2.222907\n",
      "  validation loss:\t\t1.782879\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 10 of 5000 took 0.024s\n",
      "  training loss:\t\t1.933822\n",
      "  validation loss:\t\t2.111168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 11 of 5000 took 0.024s\n",
      "  training loss:\t\t2.372470\n",
      "  validation loss:\t\t1.643655\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 12 of 5000 took 0.026s\n",
      "  training loss:\t\t1.832855\n",
      "  validation loss:\t\t1.757189\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 13 of 5000 took 0.023s\n",
      "  training loss:\t\t2.031386\n",
      "  validation loss:\t\t1.663036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 14 of 5000 took 0.023s\n",
      "  training loss:\t\t1.850586\n",
      "  validation loss:\t\t1.611031\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 15 of 5000 took 0.023s\n",
      "  training loss:\t\t1.608885\n",
      "  validation loss:\t\t1.564743\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 16 of 5000 took 0.022s\n",
      "  training loss:\t\t1.652144\n",
      "  validation loss:\t\t1.492438\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 17 of 5000 took 0.022s\n",
      "  training loss:\t\t1.558038\n",
      "  validation loss:\t\t1.421922\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 18 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422321\n",
      "  validation loss:\t\t1.413915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 19 of 5000 took 0.023s\n",
      "  training loss:\t\t1.471192\n",
      "  validation loss:\t\t1.386023\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 20 of 5000 took 0.025s\n",
      "  training loss:\t\t1.488391\n",
      "  validation loss:\t\t1.403937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 21 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409031\n",
      "  validation loss:\t\t1.392040\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 22 of 5000 took 0.022s\n",
      "  training loss:\t\t1.477176\n",
      "  validation loss:\t\t1.390113\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 23 of 5000 took 0.021s\n",
      "  training loss:\t\t1.492053\n",
      "  validation loss:\t\t1.377199\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 24 of 5000 took 0.021s\n",
      "  training loss:\t\t1.471820\n",
      "  validation loss:\t\t1.370874\n",
      "  validation accuracy:\t\t22.50 %\n",
      "Epoch 25 of 5000 took 0.022s\n",
      "  training loss:\t\t1.465531\n",
      "  validation loss:\t\t1.369888\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 26 of 5000 took 0.022s\n",
      "  training loss:\t\t1.449471\n",
      "  validation loss:\t\t1.373707\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 27 of 5000 took 0.022s\n",
      "  training loss:\t\t1.440962\n",
      "  validation loss:\t\t1.369548\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 28 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380735\n",
      "  validation loss:\t\t1.368822\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 29 of 5000 took 0.024s\n",
      "  training loss:\t\t1.409814\n",
      "  validation loss:\t\t1.368639\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 30 of 5000 took 0.023s\n",
      "  training loss:\t\t1.420052\n",
      "  validation loss:\t\t1.370137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 31 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393172\n",
      "  validation loss:\t\t1.360463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 32 of 5000 took 0.022s\n",
      "  training loss:\t\t1.359882\n",
      "  validation loss:\t\t1.366906\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 33 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392735\n",
      "  validation loss:\t\t1.354264\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 34 of 5000 took 0.022s\n",
      "  training loss:\t\t1.353956\n",
      "  validation loss:\t\t1.351754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 35 of 5000 took 0.021s\n",
      "  training loss:\t\t1.422475\n",
      "  validation loss:\t\t1.343809\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 36 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410685\n",
      "  validation loss:\t\t1.347057\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 37 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406903\n",
      "  validation loss:\t\t1.336161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 38 of 5000 took 0.022s\n",
      "  training loss:\t\t1.358069\n",
      "  validation loss:\t\t1.350380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 39 of 5000 took 0.024s\n",
      "  training loss:\t\t1.412572\n",
      "  validation loss:\t\t1.349127\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 40 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370212\n",
      "  validation loss:\t\t1.337200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 41 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387410\n",
      "  validation loss:\t\t1.320509\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 42 of 5000 took 0.023s\n",
      "  training loss:\t\t1.332144\n",
      "  validation loss:\t\t1.314346\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 43 of 5000 took 0.023s\n",
      "  training loss:\t\t1.342856\n",
      "  validation loss:\t\t1.335431\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 44 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375055\n",
      "  validation loss:\t\t1.312915\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 45 of 5000 took 0.022s\n",
      "  training loss:\t\t1.323638\n",
      "  validation loss:\t\t1.309669\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 46 of 5000 took 0.022s\n",
      "  training loss:\t\t1.336107\n",
      "  validation loss:\t\t1.302324\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 47 of 5000 took 0.022s\n",
      "  training loss:\t\t1.325910\n",
      "  validation loss:\t\t1.292091\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 48 of 5000 took 0.022s\n",
      "  training loss:\t\t1.279457\n",
      "  validation loss:\t\t1.285030\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 49 of 5000 took 0.024s\n",
      "  training loss:\t\t1.287712\n",
      "  validation loss:\t\t1.285157\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 50 of 5000 took 0.021s\n",
      "  training loss:\t\t1.294798\n",
      "  validation loss:\t\t1.267770\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 51 of 5000 took 0.021s\n",
      "  training loss:\t\t1.270344\n",
      "  validation loss:\t\t1.264284\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 52 of 5000 took 0.022s\n",
      "  training loss:\t\t1.250728\n",
      "  validation loss:\t\t1.259636\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 53 of 5000 took 0.021s\n",
      "  training loss:\t\t1.284571\n",
      "  validation loss:\t\t1.256933\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 54 of 5000 took 0.022s\n",
      "  training loss:\t\t1.240053\n",
      "  validation loss:\t\t1.242589\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 55 of 5000 took 0.022s\n",
      "  training loss:\t\t1.189335\n",
      "  validation loss:\t\t1.241849\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 56 of 5000 took 0.022s\n",
      "  training loss:\t\t1.241057\n",
      "  validation loss:\t\t1.338584\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 57 of 5000 took 0.022s\n",
      "  training loss:\t\t1.317251\n",
      "  validation loss:\t\t1.710070\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 58 of 5000 took 0.022s\n",
      "  training loss:\t\t1.682675\n",
      "  validation loss:\t\t1.289333\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 59 of 5000 took 0.024s\n",
      "  training loss:\t\t1.246114\n",
      "  validation loss:\t\t1.205000\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 60 of 5000 took 0.022s\n",
      "  training loss:\t\t1.196560\n",
      "  validation loss:\t\t1.244910\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 61 of 5000 took 0.022s\n",
      "  training loss:\t\t1.152531\n",
      "  validation loss:\t\t1.189527\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 62 of 5000 took 0.022s\n",
      "  training loss:\t\t1.166286\n",
      "  validation loss:\t\t1.312827\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 63 of 5000 took 0.022s\n",
      "  training loss:\t\t1.275560\n",
      "  validation loss:\t\t1.210273\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 64 of 5000 took 0.022s\n",
      "  training loss:\t\t1.326331\n",
      "  validation loss:\t\t1.273863\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 65 of 5000 took 0.021s\n",
      "  training loss:\t\t1.271224\n",
      "  validation loss:\t\t1.251268\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 66 of 5000 took 0.021s\n",
      "  training loss:\t\t1.205006\n",
      "  validation loss:\t\t1.497346\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 67 of 5000 took 0.022s\n",
      "  training loss:\t\t1.476841\n",
      "  validation loss:\t\t1.197168\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 68 of 5000 took 0.022s\n",
      "  training loss:\t\t1.200886\n",
      "  validation loss:\t\t1.143170\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 69 of 5000 took 0.023s\n",
      "  training loss:\t\t1.171676\n",
      "  validation loss:\t\t1.103515\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 70 of 5000 took 0.022s\n",
      "  training loss:\t\t1.056139\n",
      "  validation loss:\t\t1.182311\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 71 of 5000 took 0.022s\n",
      "  training loss:\t\t1.191929\n",
      "  validation loss:\t\t1.107521\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 72 of 5000 took 0.022s\n",
      "  training loss:\t\t1.156593\n",
      "  validation loss:\t\t1.100113\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 73 of 5000 took 0.022s\n",
      "  training loss:\t\t1.142022\n",
      "  validation loss:\t\t1.029386\n",
      "  validation accuracy:\t\t78.75 %\n",
      "Epoch 74 of 5000 took 0.023s\n",
      "  training loss:\t\t1.097422\n",
      "  validation loss:\t\t1.023081\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 75 of 5000 took 0.023s\n",
      "  training loss:\t\t1.009003\n",
      "  validation loss:\t\t1.012466\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 76 of 5000 took 0.023s\n",
      "  training loss:\t\t0.938279\n",
      "  validation loss:\t\t1.079878\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 77 of 5000 took 0.023s\n",
      "  training loss:\t\t1.061500\n",
      "  validation loss:\t\t1.146418\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 78 of 5000 took 0.023s\n",
      "  training loss:\t\t1.143889\n",
      "  validation loss:\t\t1.091940\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 79 of 5000 took 0.023s\n",
      "  training loss:\t\t1.125029\n",
      "  validation loss:\t\t1.228863\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 80 of 5000 took 0.022s\n",
      "  training loss:\t\t1.146916\n",
      "  validation loss:\t\t1.202925\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 81 of 5000 took 0.022s\n",
      "  training loss:\t\t1.144920\n",
      "  validation loss:\t\t1.233305\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 82 of 5000 took 0.022s\n",
      "  training loss:\t\t1.237630\n",
      "  validation loss:\t\t1.033686\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 83 of 5000 took 0.022s\n",
      "  training loss:\t\t0.912315\n",
      "  validation loss:\t\t0.969979\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 84 of 5000 took 0.022s\n",
      "  training loss:\t\t0.986185\n",
      "  validation loss:\t\t1.125672\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 85 of 5000 took 0.022s\n",
      "  training loss:\t\t1.053302\n",
      "  validation loss:\t\t1.060605\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 86 of 5000 took 0.022s\n",
      "  training loss:\t\t1.115910\n",
      "  validation loss:\t\t1.602733\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 87 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404053\n",
      "  validation loss:\t\t1.242304\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 88 of 5000 took 0.024s\n",
      "  training loss:\t\t1.135460\n",
      "  validation loss:\t\t1.151960\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 89 of 5000 took 0.022s\n",
      "  training loss:\t\t1.111912\n",
      "  validation loss:\t\t1.099934\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 90 of 5000 took 0.023s\n",
      "  training loss:\t\t0.987761\n",
      "  validation loss:\t\t1.487876\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 91 of 5000 took 0.023s\n",
      "  training loss:\t\t1.541629\n",
      "  validation loss:\t\t1.686216\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 92 of 5000 took 0.022s\n",
      "  training loss:\t\t1.609301\n",
      "  validation loss:\t\t1.398974\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 93 of 5000 took 0.021s\n",
      "  training loss:\t\t1.262835\n",
      "  validation loss:\t\t1.206523\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 94 of 5000 took 0.022s\n",
      "  training loss:\t\t1.159811\n",
      "  validation loss:\t\t1.128532\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 95 of 5000 took 0.022s\n",
      "  training loss:\t\t1.055568\n",
      "  validation loss:\t\t1.056704\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 96 of 5000 took 0.022s\n",
      "  training loss:\t\t1.099203\n",
      "  validation loss:\t\t1.133983\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 97 of 5000 took 0.024s\n",
      "  training loss:\t\t1.139422\n",
      "  validation loss:\t\t1.217895\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 98 of 5000 took 0.022s\n",
      "  training loss:\t\t1.177734\n",
      "  validation loss:\t\t1.189180\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 99 of 5000 took 0.022s\n",
      "  training loss:\t\t1.121290\n",
      "  validation loss:\t\t1.353208\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 100 of 5000 took 0.022s\n",
      "  training loss:\t\t1.427256\n",
      "  validation loss:\t\t1.821201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 101 of 5000 took 0.022s\n",
      "  training loss:\t\t1.820801\n",
      "  validation loss:\t\t1.896406\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 102 of 5000 took 0.022s\n",
      "  training loss:\t\t1.854995\n",
      "  validation loss:\t\t1.312877\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 103 of 5000 took 0.022s\n",
      "  training loss:\t\t1.325929\n",
      "  validation loss:\t\t1.251822\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 104 of 5000 took 0.022s\n",
      "  training loss:\t\t1.201407\n",
      "  validation loss:\t\t1.133067\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 105 of 5000 took 0.022s\n",
      "  training loss:\t\t1.165560\n",
      "  validation loss:\t\t1.061368\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 106 of 5000 took 0.022s\n",
      "  training loss:\t\t1.146629\n",
      "  validation loss:\t\t1.058767\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 107 of 5000 took 0.024s\n",
      "  training loss:\t\t1.062490\n",
      "  validation loss:\t\t1.021737\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 108 of 5000 took 0.023s\n",
      "  training loss:\t\t1.042373\n",
      "  validation loss:\t\t1.055091\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 109 of 5000 took 0.021s\n",
      "  training loss:\t\t1.100968\n",
      "  validation loss:\t\t1.226030\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 110 of 5000 took 0.022s\n",
      "  training loss:\t\t1.203102\n",
      "  validation loss:\t\t1.284550\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 111 of 5000 took 0.022s\n",
      "  training loss:\t\t1.246921\n",
      "  validation loss:\t\t1.096864\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 112 of 5000 took 0.022s\n",
      "  training loss:\t\t0.971396\n",
      "  validation loss:\t\t0.952161\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 113 of 5000 took 0.022s\n",
      "  training loss:\t\t0.880837\n",
      "  validation loss:\t\t0.999207\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 114 of 5000 took 0.022s\n",
      "  training loss:\t\t0.909724\n",
      "  validation loss:\t\t1.006665\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 115 of 5000 took 0.023s\n",
      "  training loss:\t\t0.978914\n",
      "  validation loss:\t\t1.130739\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 116 of 5000 took 0.022s\n",
      "  training loss:\t\t0.955948\n",
      "  validation loss:\t\t1.015126\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 117 of 5000 took 0.024s\n",
      "  training loss:\t\t0.976020\n",
      "  validation loss:\t\t1.068852\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 118 of 5000 took 0.022s\n",
      "  training loss:\t\t0.967889\n",
      "  validation loss:\t\t1.033261\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 119 of 5000 took 0.022s\n",
      "  training loss:\t\t0.924377\n",
      "  validation loss:\t\t0.972513\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 120 of 5000 took 0.022s\n",
      "  training loss:\t\t0.839648\n",
      "  validation loss:\t\t1.170237\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 121 of 5000 took 0.022s\n",
      "  training loss:\t\t1.113832\n",
      "  validation loss:\t\t1.181690\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 122 of 5000 took 0.022s\n",
      "  training loss:\t\t1.018089\n",
      "  validation loss:\t\t0.888030\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 123 of 5000 took 0.021s\n",
      "  training loss:\t\t0.815667\n",
      "  validation loss:\t\t1.055282\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 124 of 5000 took 0.022s\n",
      "  training loss:\t\t0.943900\n",
      "  validation loss:\t\t1.208626\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 125 of 5000 took 0.022s\n",
      "  training loss:\t\t1.061195\n",
      "  validation loss:\t\t0.873728\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 126 of 5000 took 0.022s\n",
      "  training loss:\t\t0.782209\n",
      "  validation loss:\t\t0.980296\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 127 of 5000 took 0.024s\n",
      "  training loss:\t\t0.823132\n",
      "  validation loss:\t\t1.100334\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 128 of 5000 took 0.024s\n",
      "  training loss:\t\t0.893298\n",
      "  validation loss:\t\t1.244771\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 129 of 5000 took 0.023s\n",
      "  training loss:\t\t1.275418\n",
      "  validation loss:\t\t1.014741\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 130 of 5000 took 0.023s\n",
      "  training loss:\t\t1.120624\n",
      "  validation loss:\t\t1.051228\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 131 of 5000 took 0.022s\n",
      "  training loss:\t\t1.044549\n",
      "  validation loss:\t\t1.027267\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 132 of 5000 took 0.022s\n",
      "  training loss:\t\t0.958128\n",
      "  validation loss:\t\t0.918881\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 133 of 5000 took 0.022s\n",
      "  training loss:\t\t1.022966\n",
      "  validation loss:\t\t0.916908\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 134 of 5000 took 0.022s\n",
      "  training loss:\t\t0.831447\n",
      "  validation loss:\t\t0.900470\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 135 of 5000 took 0.023s\n",
      "  training loss:\t\t0.895409\n",
      "  validation loss:\t\t1.192984\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 136 of 5000 took 0.023s\n",
      "  training loss:\t\t0.949829\n",
      "  validation loss:\t\t0.921172\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 137 of 5000 took 0.024s\n",
      "  training loss:\t\t1.031982\n",
      "  validation loss:\t\t1.306908\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 138 of 5000 took 0.023s\n",
      "  training loss:\t\t1.160938\n",
      "  validation loss:\t\t1.119451\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 139 of 5000 took 0.023s\n",
      "  training loss:\t\t1.065689\n",
      "  validation loss:\t\t1.430455\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 140 of 5000 took 0.022s\n",
      "  training loss:\t\t1.284224\n",
      "  validation loss:\t\t1.349764\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 141 of 5000 took 0.022s\n",
      "  training loss:\t\t1.162898\n",
      "  validation loss:\t\t1.312735\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 142 of 5000 took 0.022s\n",
      "  training loss:\t\t1.149154\n",
      "  validation loss:\t\t1.051103\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 143 of 5000 took 0.022s\n",
      "  training loss:\t\t1.001445\n",
      "  validation loss:\t\t1.324336\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 144 of 5000 took 0.022s\n",
      "  training loss:\t\t1.081022\n",
      "  validation loss:\t\t0.871843\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 145 of 5000 took 0.022s\n",
      "  training loss:\t\t0.746435\n",
      "  validation loss:\t\t0.867916\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 146 of 5000 took 0.024s\n",
      "  training loss:\t\t0.782905\n",
      "  validation loss:\t\t0.881912\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 147 of 5000 took 0.024s\n",
      "  training loss:\t\t0.751535\n",
      "  validation loss:\t\t0.890874\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 148 of 5000 took 0.023s\n",
      "  training loss:\t\t0.757979\n",
      "  validation loss:\t\t0.962323\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 149 of 5000 took 0.022s\n",
      "  training loss:\t\t0.835121\n",
      "  validation loss:\t\t0.853844\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 150 of 5000 took 0.021s\n",
      "  training loss:\t\t0.703433\n",
      "  validation loss:\t\t0.874541\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 151 of 5000 took 0.022s\n",
      "  training loss:\t\t0.732411\n",
      "  validation loss:\t\t1.160461\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 152 of 5000 took 0.022s\n",
      "  training loss:\t\t0.935737\n",
      "  validation loss:\t\t1.791907\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 153 of 5000 took 0.022s\n",
      "  training loss:\t\t1.586688\n",
      "  validation loss:\t\t1.060668\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 154 of 5000 took 0.022s\n",
      "  training loss:\t\t1.040610\n",
      "  validation loss:\t\t1.105515\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 155 of 5000 took 0.023s\n",
      "  training loss:\t\t0.905030\n",
      "  validation loss:\t\t1.070389\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 156 of 5000 took 0.022s\n",
      "  training loss:\t\t0.919713\n",
      "  validation loss:\t\t1.146669\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 157 of 5000 took 0.022s\n",
      "  training loss:\t\t0.983893\n",
      "  validation loss:\t\t1.130556\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 158 of 5000 took 0.023s\n",
      "  training loss:\t\t1.130399\n",
      "  validation loss:\t\t0.932384\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 159 of 5000 took 0.022s\n",
      "  training loss:\t\t0.838214\n",
      "  validation loss:\t\t1.000638\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 160 of 5000 took 0.023s\n",
      "  training loss:\t\t0.855557\n",
      "  validation loss:\t\t0.859060\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 161 of 5000 took 0.023s\n",
      "  training loss:\t\t0.748390\n",
      "  validation loss:\t\t0.857133\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 162 of 5000 took 0.022s\n",
      "  training loss:\t\t0.738468\n",
      "  validation loss:\t\t0.804651\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 163 of 5000 took 0.023s\n",
      "  training loss:\t\t0.676908\n",
      "  validation loss:\t\t0.780886\n",
      "  validation accuracy:\t\t77.50 %\n",
      "Epoch 164 of 5000 took 0.024s\n",
      "  training loss:\t\t0.722941\n",
      "  validation loss:\t\t0.802656\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 165 of 5000 took 0.022s\n",
      "  training loss:\t\t0.626997\n",
      "  validation loss:\t\t0.831934\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 166 of 5000 took 0.022s\n",
      "  training loss:\t\t0.614627\n",
      "  validation loss:\t\t1.011767\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 167 of 5000 took 0.023s\n",
      "  training loss:\t\t0.794558\n",
      "  validation loss:\t\t1.497260\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 168 of 5000 took 0.023s\n",
      "  training loss:\t\t1.234825\n",
      "  validation loss:\t\t1.986050\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 169 of 5000 took 0.023s\n",
      "  training loss:\t\t1.846090\n",
      "  validation loss:\t\t2.419451\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 170 of 5000 took 0.023s\n",
      "  training loss:\t\t2.518157\n",
      "  validation loss:\t\t1.753469\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 171 of 5000 took 0.023s\n",
      "  training loss:\t\t1.857073\n",
      "  validation loss:\t\t1.964555\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 172 of 5000 took 0.024s\n",
      "  training loss:\t\t1.936223\n",
      "  validation loss:\t\t1.803257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 173 of 5000 took 0.023s\n",
      "  training loss:\t\t2.030799\n",
      "  validation loss:\t\t1.411298\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 174 of 5000 took 0.021s\n",
      "  training loss:\t\t1.730245\n",
      "  validation loss:\t\t1.480365\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 175 of 5000 took 0.022s\n",
      "  training loss:\t\t1.537529\n",
      "  validation loss:\t\t1.351869\n",
      "  validation accuracy:\t\t21.25 %\n",
      "Epoch 176 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398288\n",
      "  validation loss:\t\t1.352198\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 177 of 5000 took 0.022s\n",
      "  training loss:\t\t1.442475\n",
      "  validation loss:\t\t1.314120\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 178 of 5000 took 0.022s\n",
      "  training loss:\t\t1.347458\n",
      "  validation loss:\t\t1.316836\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 179 of 5000 took 0.022s\n",
      "  training loss:\t\t1.329641\n",
      "  validation loss:\t\t1.318559\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 180 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391744\n",
      "  validation loss:\t\t1.298360\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 181 of 5000 took 0.022s\n",
      "  training loss:\t\t1.305243\n",
      "  validation loss:\t\t1.305563\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 182 of 5000 took 0.024s\n",
      "  training loss:\t\t1.375852\n",
      "  validation loss:\t\t1.310884\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 183 of 5000 took 0.023s\n",
      "  training loss:\t\t1.373797\n",
      "  validation loss:\t\t1.346431\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 184 of 5000 took 0.022s\n",
      "  training loss:\t\t1.349340\n",
      "  validation loss:\t\t1.281920\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 185 of 5000 took 0.022s\n",
      "  training loss:\t\t1.363345\n",
      "  validation loss:\t\t1.300592\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 186 of 5000 took 0.022s\n",
      "  training loss:\t\t1.328557\n",
      "  validation loss:\t\t1.320180\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 187 of 5000 took 0.022s\n",
      "  training loss:\t\t1.435979\n",
      "  validation loss:\t\t1.388207\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 188 of 5000 took 0.022s\n",
      "  training loss:\t\t1.354160\n",
      "  validation loss:\t\t1.255456\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 189 of 5000 took 0.022s\n",
      "  training loss:\t\t1.270965\n",
      "  validation loss:\t\t1.264020\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 190 of 5000 took 0.022s\n",
      "  training loss:\t\t1.241845\n",
      "  validation loss:\t\t1.244440\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 191 of 5000 took 0.022s\n",
      "  training loss:\t\t1.153439\n",
      "  validation loss:\t\t1.247844\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 192 of 5000 took 0.023s\n",
      "  training loss:\t\t1.284504\n",
      "  validation loss:\t\t1.282434\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 193 of 5000 took 0.023s\n",
      "  training loss:\t\t1.342717\n",
      "  validation loss:\t\t1.524400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 194 of 5000 took 0.022s\n",
      "  training loss:\t\t1.531568\n",
      "  validation loss:\t\t1.616030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 195 of 5000 took 0.022s\n",
      "  training loss:\t\t1.837276\n",
      "  validation loss:\t\t1.401289\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 196 of 5000 took 0.022s\n",
      "  training loss:\t\t1.519809\n",
      "  validation loss:\t\t1.463362\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 197 of 5000 took 0.022s\n",
      "  training loss:\t\t1.368722\n",
      "  validation loss:\t\t1.342338\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 198 of 5000 took 0.022s\n",
      "  training loss:\t\t1.272924\n",
      "  validation loss:\t\t1.349114\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 199 of 5000 took 0.022s\n",
      "  training loss:\t\t1.419030\n",
      "  validation loss:\t\t1.298341\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 200 of 5000 took 0.022s\n",
      "  training loss:\t\t1.372245\n",
      "  validation loss:\t\t1.232280\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 201 of 5000 took 0.022s\n",
      "  training loss:\t\t1.226990\n",
      "  validation loss:\t\t1.233567\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 202 of 5000 took 0.024s\n",
      "  training loss:\t\t1.270258\n",
      "  validation loss:\t\t1.231555\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 203 of 5000 took 0.024s\n",
      "  training loss:\t\t1.218422\n",
      "  validation loss:\t\t1.233577\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 204 of 5000 took 0.023s\n",
      "  training loss:\t\t1.139743\n",
      "  validation loss:\t\t1.215227\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 205 of 5000 took 0.022s\n",
      "  training loss:\t\t1.212641\n",
      "  validation loss:\t\t1.301415\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 206 of 5000 took 0.023s\n",
      "  training loss:\t\t1.253866\n",
      "  validation loss:\t\t1.445712\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 207 of 5000 took 0.022s\n",
      "  training loss:\t\t1.443906\n",
      "  validation loss:\t\t1.434437\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 208 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405214\n",
      "  validation loss:\t\t1.340545\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 209 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422706\n",
      "  validation loss:\t\t1.266057\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 210 of 5000 took 0.022s\n",
      "  training loss:\t\t1.308064\n",
      "  validation loss:\t\t1.257315\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 211 of 5000 took 0.022s\n",
      "  training loss:\t\t1.211941\n",
      "  validation loss:\t\t1.207698\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 212 of 5000 took 0.024s\n",
      "  training loss:\t\t1.182155\n",
      "  validation loss:\t\t1.186143\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 213 of 5000 took 0.022s\n",
      "  training loss:\t\t1.096234\n",
      "  validation loss:\t\t1.189785\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 214 of 5000 took 0.023s\n",
      "  training loss:\t\t1.175171\n",
      "  validation loss:\t\t1.331434\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 215 of 5000 took 0.023s\n",
      "  training loss:\t\t1.263848\n",
      "  validation loss:\t\t1.441623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 216 of 5000 took 0.022s\n",
      "  training loss:\t\t1.333933\n",
      "  validation loss:\t\t1.206083\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 217 of 5000 took 0.022s\n",
      "  training loss:\t\t1.149575\n",
      "  validation loss:\t\t1.141551\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 218 of 5000 took 0.022s\n",
      "  training loss:\t\t1.063746\n",
      "  validation loss:\t\t1.122415\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 219 of 5000 took 0.022s\n",
      "  training loss:\t\t1.087216\n",
      "  validation loss:\t\t1.100138\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 220 of 5000 took 0.023s\n",
      "  training loss:\t\t1.138362\n",
      "  validation loss:\t\t1.105271\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 221 of 5000 took 0.023s\n",
      "  training loss:\t\t1.058327\n",
      "  validation loss:\t\t1.196689\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 222 of 5000 took 0.024s\n",
      "  training loss:\t\t1.179216\n",
      "  validation loss:\t\t1.956497\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 223 of 5000 took 0.023s\n",
      "  training loss:\t\t1.863475\n",
      "  validation loss:\t\t1.241705\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 224 of 5000 took 0.022s\n",
      "  training loss:\t\t1.215863\n",
      "  validation loss:\t\t1.171991\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 225 of 5000 took 0.022s\n",
      "  training loss:\t\t1.081446\n",
      "  validation loss:\t\t1.218972\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 226 of 5000 took 0.022s\n",
      "  training loss:\t\t1.240429\n",
      "  validation loss:\t\t1.553909\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 227 of 5000 took 0.022s\n",
      "  training loss:\t\t1.508696\n",
      "  validation loss:\t\t1.203113\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 228 of 5000 took 0.022s\n",
      "  training loss:\t\t1.117831\n",
      "  validation loss:\t\t1.203211\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 229 of 5000 took 0.022s\n",
      "  training loss:\t\t1.107229\n",
      "  validation loss:\t\t1.715626\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 230 of 5000 took 0.022s\n",
      "  training loss:\t\t1.637293\n",
      "  validation loss:\t\t1.107846\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 231 of 5000 took 0.024s\n",
      "  training loss:\t\t1.113265\n",
      "  validation loss:\t\t1.217869\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 232 of 5000 took 0.021s\n",
      "  training loss:\t\t1.150026\n",
      "  validation loss:\t\t1.190704\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 233 of 5000 took 0.022s\n",
      "  training loss:\t\t1.145778\n",
      "  validation loss:\t\t1.181222\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 234 of 5000 took 0.022s\n",
      "  training loss:\t\t1.060093\n",
      "  validation loss:\t\t1.240616\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 235 of 5000 took 0.022s\n",
      "  training loss:\t\t1.260324\n",
      "  validation loss:\t\t1.491183\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 236 of 5000 took 0.022s\n",
      "  training loss:\t\t1.341905\n",
      "  validation loss:\t\t1.174434\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 237 of 5000 took 0.022s\n",
      "  training loss:\t\t1.048074\n",
      "  validation loss:\t\t1.397683\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 238 of 5000 took 0.023s\n",
      "  training loss:\t\t1.280297\n",
      "  validation loss:\t\t1.092966\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 239 of 5000 took 0.024s\n",
      "  training loss:\t\t1.023174\n",
      "  validation loss:\t\t1.230482\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 240 of 5000 took 0.022s\n",
      "  training loss:\t\t1.105318\n",
      "  validation loss:\t\t1.086013\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 241 of 5000 took 0.022s\n",
      "  training loss:\t\t0.929869\n",
      "  validation loss:\t\t1.057311\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 242 of 5000 took 0.022s\n",
      "  training loss:\t\t0.973177\n",
      "  validation loss:\t\t0.985684\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 243 of 5000 took 0.022s\n",
      "  training loss:\t\t0.859750\n",
      "  validation loss:\t\t1.024538\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 244 of 5000 took 0.022s\n",
      "  training loss:\t\t0.853209\n",
      "  validation loss:\t\t1.179317\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 245 of 5000 took 0.022s\n",
      "  training loss:\t\t1.198084\n",
      "  validation loss:\t\t2.081245\n",
      "  validation accuracy:\t\t22.50 %\n",
      "Epoch 246 of 5000 took 0.023s\n",
      "  training loss:\t\t1.866674\n",
      "  validation loss:\t\t1.199704\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 247 of 5000 took 0.023s\n",
      "  training loss:\t\t1.193486\n",
      "  validation loss:\t\t1.197602\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 248 of 5000 took 0.023s\n",
      "  training loss:\t\t1.201635\n",
      "  validation loss:\t\t1.125478\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 249 of 5000 took 0.023s\n",
      "  training loss:\t\t1.074614\n",
      "  validation loss:\t\t1.316919\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 250 of 5000 took 0.023s\n",
      "  training loss:\t\t1.296945\n",
      "  validation loss:\t\t1.126848\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 251 of 5000 took 0.023s\n",
      "  training loss:\t\t1.063251\n",
      "  validation loss:\t\t1.082003\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 252 of 5000 took 0.023s\n",
      "  training loss:\t\t0.965699\n",
      "  validation loss:\t\t1.067895\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 253 of 5000 took 0.023s\n",
      "  training loss:\t\t0.883543\n",
      "  validation loss:\t\t1.268692\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 254 of 5000 took 0.023s\n",
      "  training loss:\t\t1.199788\n",
      "  validation loss:\t\t2.458984\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 255 of 5000 took 0.023s\n",
      "  training loss:\t\t2.158759\n",
      "  validation loss:\t\t1.541673\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 256 of 5000 took 0.024s\n",
      "  training loss:\t\t1.307949\n",
      "  validation loss:\t\t1.267666\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 257 of 5000 took 0.024s\n",
      "  training loss:\t\t1.131950\n",
      "  validation loss:\t\t1.368341\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 258 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413160\n",
      "  validation loss:\t\t1.432986\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 259 of 5000 took 0.022s\n",
      "  training loss:\t\t1.305585\n",
      "  validation loss:\t\t1.087388\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 260 of 5000 took 0.022s\n",
      "  training loss:\t\t1.020783\n",
      "  validation loss:\t\t1.335988\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 261 of 5000 took 0.022s\n",
      "  training loss:\t\t1.204853\n",
      "  validation loss:\t\t1.177883\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 262 of 5000 took 0.022s\n",
      "  training loss:\t\t1.122279\n",
      "  validation loss:\t\t1.397232\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 263 of 5000 took 0.022s\n",
      "  training loss:\t\t1.307789\n",
      "  validation loss:\t\t1.204229\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 264 of 5000 took 0.021s\n",
      "  training loss:\t\t1.034197\n",
      "  validation loss:\t\t1.308762\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 265 of 5000 took 0.022s\n",
      "  training loss:\t\t1.080252\n",
      "  validation loss:\t\t1.030438\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 266 of 5000 took 0.022s\n",
      "  training loss:\t\t0.947944\n",
      "  validation loss:\t\t1.033212\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 267 of 5000 took 0.024s\n",
      "  training loss:\t\t0.881966\n",
      "  validation loss:\t\t1.103276\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 268 of 5000 took 0.022s\n",
      "  training loss:\t\t0.999436\n",
      "  validation loss:\t\t1.843681\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 269 of 5000 took 0.022s\n",
      "  training loss:\t\t1.705855\n",
      "  validation loss:\t\t1.332603\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 270 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385171\n",
      "  validation loss:\t\t1.176621\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 271 of 5000 took 0.022s\n",
      "  training loss:\t\t0.953410\n",
      "  validation loss:\t\t1.083429\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 272 of 5000 took 0.022s\n",
      "  training loss:\t\t0.949572\n",
      "  validation loss:\t\t0.924075\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 273 of 5000 took 0.022s\n",
      "  training loss:\t\t0.939360\n",
      "  validation loss:\t\t0.940045\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 274 of 5000 took 0.022s\n",
      "  training loss:\t\t0.849522\n",
      "  validation loss:\t\t0.925635\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 275 of 5000 took 0.022s\n",
      "  training loss:\t\t0.806575\n",
      "  validation loss:\t\t0.896842\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 276 of 5000 took 0.022s\n",
      "  training loss:\t\t0.956993\n",
      "  validation loss:\t\t0.897741\n",
      "  validation accuracy:\t\t76.25 %\n",
      "Epoch 277 of 5000 took 0.023s\n",
      "  training loss:\t\t0.746415\n",
      "  validation loss:\t\t0.994818\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 278 of 5000 took 0.021s\n",
      "  training loss:\t\t0.763241\n",
      "  validation loss:\t\t0.962395\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 279 of 5000 took 0.021s\n",
      "  training loss:\t\t0.918925\n",
      "  validation loss:\t\t1.613536\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 280 of 5000 took 0.022s\n",
      "  training loss:\t\t1.442864\n",
      "  validation loss:\t\t1.328131\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 281 of 5000 took 0.022s\n",
      "  training loss:\t\t1.290075\n",
      "  validation loss:\t\t1.537592\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 282 of 5000 took 0.022s\n",
      "  training loss:\t\t1.278826\n",
      "  validation loss:\t\t1.502533\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 283 of 5000 took 0.022s\n",
      "  training loss:\t\t1.348902\n",
      "  validation loss:\t\t1.162838\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 284 of 5000 took 0.022s\n",
      "  training loss:\t\t1.044222\n",
      "  validation loss:\t\t1.162181\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 285 of 5000 took 0.023s\n",
      "  training loss:\t\t0.944460\n",
      "  validation loss:\t\t1.398218\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 286 of 5000 took 0.022s\n",
      "  training loss:\t\t1.280030\n",
      "  validation loss:\t\t1.226845\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 287 of 5000 took 0.024s\n",
      "  training loss:\t\t0.999568\n",
      "  validation loss:\t\t1.252578\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 288 of 5000 took 0.023s\n",
      "  training loss:\t\t1.111038\n",
      "  validation loss:\t\t1.157030\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 289 of 5000 took 0.023s\n",
      "  training loss:\t\t0.961824\n",
      "  validation loss:\t\t1.090573\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 290 of 5000 took 0.023s\n",
      "  training loss:\t\t0.990235\n",
      "  validation loss:\t\t1.158849\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 291 of 5000 took 0.022s\n",
      "  training loss:\t\t0.928709\n",
      "  validation loss:\t\t0.965955\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 292 of 5000 took 0.022s\n",
      "  training loss:\t\t0.784234\n",
      "  validation loss:\t\t0.934626\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 293 of 5000 took 0.022s\n",
      "  training loss:\t\t0.780120\n",
      "  validation loss:\t\t0.904927\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 294 of 5000 took 0.023s\n",
      "  training loss:\t\t0.766984\n",
      "  validation loss:\t\t1.070574\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 295 of 5000 took 0.023s\n",
      "  training loss:\t\t0.821694\n",
      "  validation loss:\t\t1.196895\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 296 of 5000 took 0.023s\n",
      "  training loss:\t\t0.843315\n",
      "  validation loss:\t\t1.274999\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 297 of 5000 took 0.021s\n",
      "  training loss:\t\t1.001953\n",
      "  validation loss:\t\t0.887190\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 298 of 5000 took 0.021s\n",
      "  training loss:\t\t0.681508\n",
      "  validation loss:\t\t1.020689\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 299 of 5000 took 0.021s\n",
      "  training loss:\t\t0.730062\n",
      "  validation loss:\t\t0.886739\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 300 of 5000 took 0.021s\n",
      "  training loss:\t\t0.782075\n",
      "  validation loss:\t\t1.237284\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 301 of 5000 took 0.022s\n",
      "  training loss:\t\t0.916948\n",
      "  validation loss:\t\t1.166140\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 302 of 5000 took 0.021s\n",
      "  training loss:\t\t1.022951\n",
      "  validation loss:\t\t1.417929\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 303 of 5000 took 0.022s\n",
      "  training loss:\t\t0.966022\n",
      "  validation loss:\t\t0.872005\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 304 of 5000 took 0.022s\n",
      "  training loss:\t\t0.793811\n",
      "  validation loss:\t\t1.053994\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 305 of 5000 took 0.024s\n",
      "  training loss:\t\t0.695449\n",
      "  validation loss:\t\t0.843432\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 306 of 5000 took 0.023s\n",
      "  training loss:\t\t0.748059\n",
      "  validation loss:\t\t1.142906\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 307 of 5000 took 0.023s\n",
      "  training loss:\t\t0.820105\n",
      "  validation loss:\t\t1.133849\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 308 of 5000 took 0.023s\n",
      "  training loss:\t\t0.978674\n",
      "  validation loss:\t\t1.146205\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 309 of 5000 took 0.022s\n",
      "  training loss:\t\t0.873592\n",
      "  validation loss:\t\t0.967160\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 310 of 5000 took 0.023s\n",
      "  training loss:\t\t0.933636\n",
      "  validation loss:\t\t1.211224\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 311 of 5000 took 0.021s\n",
      "  training loss:\t\t0.921586\n",
      "  validation loss:\t\t1.024861\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 312 of 5000 took 0.022s\n",
      "  training loss:\t\t0.892423\n",
      "  validation loss:\t\t1.015768\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 313 of 5000 took 0.023s\n",
      "  training loss:\t\t0.819068\n",
      "  validation loss:\t\t0.908038\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 314 of 5000 took 0.022s\n",
      "  training loss:\t\t0.742978\n",
      "  validation loss:\t\t0.911660\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 315 of 5000 took 0.021s\n",
      "  training loss:\t\t0.650883\n",
      "  validation loss:\t\t0.808900\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 316 of 5000 took 0.021s\n",
      "  training loss:\t\t0.536607\n",
      "  validation loss:\t\t0.826423\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 317 of 5000 took 0.021s\n",
      "  training loss:\t\t0.649640\n",
      "  validation loss:\t\t1.289553\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 318 of 5000 took 0.022s\n",
      "  training loss:\t\t0.819938\n",
      "  validation loss:\t\t1.421520\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 319 of 5000 took 0.022s\n",
      "  training loss:\t\t1.254528\n",
      "  validation loss:\t\t1.189330\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 320 of 5000 took 0.021s\n",
      "  training loss:\t\t0.835608\n",
      "  validation loss:\t\t1.088162\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 321 of 5000 took 0.021s\n",
      "  training loss:\t\t0.808581\n",
      "  validation loss:\t\t1.070016\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 322 of 5000 took 0.022s\n",
      "  training loss:\t\t0.892480\n",
      "  validation loss:\t\t1.011778\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 323 of 5000 took 0.023s\n",
      "  training loss:\t\t0.534678\n",
      "  validation loss:\t\t0.801461\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 324 of 5000 took 0.021s\n",
      "  training loss:\t\t0.634255\n",
      "  validation loss:\t\t0.883118\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 325 of 5000 took 0.020s\n",
      "  training loss:\t\t0.651567\n",
      "  validation loss:\t\t0.793966\n",
      "  validation accuracy:\t\t77.50 %\n",
      "Epoch 326 of 5000 took 0.021s\n",
      "  training loss:\t\t0.574418\n",
      "  validation loss:\t\t0.851589\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 327 of 5000 took 0.022s\n",
      "  training loss:\t\t0.522307\n",
      "  validation loss:\t\t0.866717\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 328 of 5000 took 0.022s\n",
      "  training loss:\t\t0.566166\n",
      "  validation loss:\t\t0.805425\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 329 of 5000 took 0.022s\n",
      "  training loss:\t\t0.778367\n",
      "  validation loss:\t\t1.374532\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 330 of 5000 took 0.022s\n",
      "  training loss:\t\t0.773250\n",
      "  validation loss:\t\t1.223483\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 331 of 5000 took 0.022s\n",
      "  training loss:\t\t1.226408\n",
      "  validation loss:\t\t1.391274\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 332 of 5000 took 0.023s\n",
      "  training loss:\t\t1.365241\n",
      "  validation loss:\t\t1.225145\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 333 of 5000 took 0.023s\n",
      "  training loss:\t\t1.010595\n",
      "  validation loss:\t\t3.343721\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 334 of 5000 took 0.024s\n",
      "  training loss:\t\t2.677128\n",
      "  validation loss:\t\t1.386322\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 335 of 5000 took 0.023s\n",
      "  training loss:\t\t1.220174\n",
      "  validation loss:\t\t2.496295\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 336 of 5000 took 0.023s\n",
      "  training loss:\t\t2.524789\n",
      "  validation loss:\t\t0.986366\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 337 of 5000 took 0.023s\n",
      "  training loss:\t\t0.875615\n",
      "  validation loss:\t\t0.901371\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 338 of 5000 took 0.023s\n",
      "  training loss:\t\t0.775508\n",
      "  validation loss:\t\t1.127054\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 339 of 5000 took 0.022s\n",
      "  training loss:\t\t0.801450\n",
      "  validation loss:\t\t0.961083\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 340 of 5000 took 0.022s\n",
      "  training loss:\t\t0.851550\n",
      "  validation loss:\t\t1.203291\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 341 of 5000 took 0.023s\n",
      "  training loss:\t\t0.934239\n",
      "  validation loss:\t\t1.050158\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 342 of 5000 took 0.022s\n",
      "  training loss:\t\t0.751547\n",
      "  validation loss:\t\t1.094415\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 343 of 5000 took 0.023s\n",
      "  training loss:\t\t0.940551\n",
      "  validation loss:\t\t0.847080\n",
      "  validation accuracy:\t\t76.25 %\n",
      "Epoch 344 of 5000 took 0.022s\n",
      "  training loss:\t\t0.711283\n",
      "  validation loss:\t\t0.906101\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 345 of 5000 took 0.021s\n",
      "  training loss:\t\t0.676862\n",
      "  validation loss:\t\t0.888525\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 346 of 5000 took 0.022s\n",
      "  training loss:\t\t0.695645\n",
      "  validation loss:\t\t0.890040\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 347 of 5000 took 0.022s\n",
      "  training loss:\t\t0.619242\n",
      "  validation loss:\t\t0.982080\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 348 of 5000 took 0.022s\n",
      "  training loss:\t\t0.743270\n",
      "  validation loss:\t\t0.758474\n",
      "  validation accuracy:\t\t78.75 %\n",
      "Epoch 349 of 5000 took 0.022s\n",
      "  training loss:\t\t0.521177\n",
      "  validation loss:\t\t0.770844\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 350 of 5000 took 0.022s\n",
      "  training loss:\t\t0.469680\n",
      "  validation loss:\t\t0.809424\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 351 of 5000 took 0.023s\n",
      "  training loss:\t\t0.766479\n",
      "  validation loss:\t\t1.043244\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 352 of 5000 took 0.023s\n",
      "  training loss:\t\t0.664549\n",
      "  validation loss:\t\t1.124714\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 353 of 5000 took 0.022s\n",
      "  training loss:\t\t0.781684\n",
      "  validation loss:\t\t1.486634\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 354 of 5000 took 0.023s\n",
      "  training loss:\t\t1.201860\n",
      "  validation loss:\t\t1.709637\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 355 of 5000 took 0.022s\n",
      "  training loss:\t\t1.283931\n",
      "  validation loss:\t\t1.408118\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 356 of 5000 took 0.022s\n",
      "  training loss:\t\t1.109412\n",
      "  validation loss:\t\t1.167731\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 357 of 5000 took 0.022s\n",
      "  training loss:\t\t1.076715\n",
      "  validation loss:\t\t0.994843\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 358 of 5000 took 0.022s\n",
      "  training loss:\t\t0.761948\n",
      "  validation loss:\t\t1.043809\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 359 of 5000 took 0.022s\n",
      "  training loss:\t\t0.769402\n",
      "  validation loss:\t\t1.056730\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 360 of 5000 took 0.022s\n",
      "  training loss:\t\t0.806219\n",
      "  validation loss:\t\t1.350247\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 361 of 5000 took 0.024s\n",
      "  training loss:\t\t0.928299\n",
      "  validation loss:\t\t1.796615\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 362 of 5000 took 0.024s\n",
      "  training loss:\t\t1.907521\n",
      "  validation loss:\t\t1.186311\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 363 of 5000 took 0.023s\n",
      "  training loss:\t\t1.196445\n",
      "  validation loss:\t\t2.002880\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 364 of 5000 took 0.023s\n",
      "  training loss:\t\t1.616929\n",
      "  validation loss:\t\t1.248824\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 365 of 5000 took 0.023s\n",
      "  training loss:\t\t1.044272\n",
      "  validation loss:\t\t1.182652\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 366 of 5000 took 0.024s\n",
      "  training loss:\t\t1.205744\n",
      "  validation loss:\t\t1.366499\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 367 of 5000 took 0.023s\n",
      "  training loss:\t\t1.021454\n",
      "  validation loss:\t\t1.195104\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 368 of 5000 took 0.023s\n",
      "  training loss:\t\t1.153477\n",
      "  validation loss:\t\t1.099539\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 369 of 5000 took 0.023s\n",
      "  training loss:\t\t0.869979\n",
      "  validation loss:\t\t0.941895\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 370 of 5000 took 0.023s\n",
      "  training loss:\t\t0.778449\n",
      "  validation loss:\t\t0.998828\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 371 of 5000 took 0.022s\n",
      "  training loss:\t\t0.670968\n",
      "  validation loss:\t\t0.941113\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 372 of 5000 took 0.024s\n",
      "  training loss:\t\t0.800490\n",
      "  validation loss:\t\t1.100010\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 373 of 5000 took 0.022s\n",
      "  training loss:\t\t0.877049\n",
      "  validation loss:\t\t1.034015\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 374 of 5000 took 0.022s\n",
      "  training loss:\t\t1.014053\n",
      "  validation loss:\t\t1.276424\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 375 of 5000 took 0.022s\n",
      "  training loss:\t\t1.082572\n",
      "  validation loss:\t\t1.425758\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 376 of 5000 took 0.022s\n",
      "  training loss:\t\t1.301400\n",
      "  validation loss:\t\t1.331004\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 377 of 5000 took 0.022s\n",
      "  training loss:\t\t0.966864\n",
      "  validation loss:\t\t1.040329\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 378 of 5000 took 0.022s\n",
      "  training loss:\t\t0.929399\n",
      "  validation loss:\t\t1.113656\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 379 of 5000 took 0.022s\n",
      "  training loss:\t\t0.894551\n",
      "  validation loss:\t\t0.982888\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 380 of 5000 took 0.023s\n",
      "  training loss:\t\t0.845092\n",
      "  validation loss:\t\t1.024850\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 381 of 5000 took 0.024s\n",
      "  training loss:\t\t0.762879\n",
      "  validation loss:\t\t0.942354\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 382 of 5000 took 0.022s\n",
      "  training loss:\t\t0.728510\n",
      "  validation loss:\t\t0.925695\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 383 of 5000 took 0.023s\n",
      "  training loss:\t\t0.768745\n",
      "  validation loss:\t\t0.963875\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 384 of 5000 took 0.022s\n",
      "  training loss:\t\t0.674081\n",
      "  validation loss:\t\t0.917070\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 385 of 5000 took 0.022s\n",
      "  training loss:\t\t0.715483\n",
      "  validation loss:\t\t1.042767\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 386 of 5000 took 0.022s\n",
      "  training loss:\t\t0.710698\n",
      "  validation loss:\t\t0.981125\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 387 of 5000 took 0.022s\n",
      "  training loss:\t\t0.666829\n",
      "  validation loss:\t\t1.424446\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 388 of 5000 took 0.022s\n",
      "  training loss:\t\t0.952956\n",
      "  validation loss:\t\t1.549653\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 389 of 5000 took 0.022s\n",
      "  training loss:\t\t1.518127\n",
      "  validation loss:\t\t1.386626\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 390 of 5000 took 0.023s\n",
      "  training loss:\t\t1.043593\n",
      "  validation loss:\t\t1.793159\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 391 of 5000 took 0.024s\n",
      "  training loss:\t\t1.576476\n",
      "  validation loss:\t\t0.931587\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 392 of 5000 took 0.023s\n",
      "  training loss:\t\t0.614741\n",
      "  validation loss:\t\t1.081486\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 393 of 5000 took 0.023s\n",
      "  training loss:\t\t0.845593\n",
      "  validation loss:\t\t1.002028\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 394 of 5000 took 0.023s\n",
      "  training loss:\t\t0.762461\n",
      "  validation loss:\t\t0.971554\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 395 of 5000 took 0.024s\n",
      "  training loss:\t\t0.962958\n",
      "  validation loss:\t\t0.943435\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 396 of 5000 took 0.022s\n",
      "  training loss:\t\t0.633243\n",
      "  validation loss:\t\t0.918317\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 397 of 5000 took 0.023s\n",
      "  training loss:\t\t0.728377\n",
      "  validation loss:\t\t1.076104\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 398 of 5000 took 0.022s\n",
      "  training loss:\t\t0.727119\n",
      "  validation loss:\t\t1.019863\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 399 of 5000 took 0.022s\n",
      "  training loss:\t\t0.905397\n",
      "  validation loss:\t\t1.210368\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 400 of 5000 took 0.022s\n",
      "  training loss:\t\t0.834933\n",
      "  validation loss:\t\t1.011258\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 401 of 5000 took 0.023s\n",
      "  training loss:\t\t0.974186\n",
      "  validation loss:\t\t1.346638\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 402 of 5000 took 0.022s\n",
      "  training loss:\t\t0.924146\n",
      "  validation loss:\t\t1.381832\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 403 of 5000 took 0.021s\n",
      "  training loss:\t\t1.262507\n",
      "  validation loss:\t\t1.350881\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 404 of 5000 took 0.021s\n",
      "  training loss:\t\t0.705588\n",
      "  validation loss:\t\t0.961442\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 405 of 5000 took 0.021s\n",
      "  training loss:\t\t0.961016\n",
      "  validation loss:\t\t1.148903\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 406 of 5000 took 0.023s\n",
      "  training loss:\t\t0.766834\n",
      "  validation loss:\t\t1.052444\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 407 of 5000 took 0.021s\n",
      "  training loss:\t\t0.983291\n",
      "  validation loss:\t\t1.205753\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 408 of 5000 took 0.021s\n",
      "  training loss:\t\t0.793265\n",
      "  validation loss:\t\t0.977260\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 409 of 5000 took 0.023s\n",
      "  training loss:\t\t0.955936\n",
      "  validation loss:\t\t1.309382\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 410 of 5000 took 0.024s\n",
      "  training loss:\t\t0.757380\n",
      "  validation loss:\t\t1.137134\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 411 of 5000 took 0.023s\n",
      "  training loss:\t\t1.081290\n",
      "  validation loss:\t\t1.263397\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 412 of 5000 took 0.023s\n",
      "  training loss:\t\t0.823999\n",
      "  validation loss:\t\t1.096459\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 413 of 5000 took 0.023s\n",
      "  training loss:\t\t0.930857\n",
      "  validation loss:\t\t1.139033\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 414 of 5000 took 0.022s\n",
      "  training loss:\t\t0.660693\n",
      "  validation loss:\t\t0.886677\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 415 of 5000 took 0.022s\n",
      "  training loss:\t\t0.595488\n",
      "  validation loss:\t\t0.869905\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 416 of 5000 took 0.022s\n",
      "  training loss:\t\t0.573411\n",
      "  validation loss:\t\t0.849391\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 417 of 5000 took 0.022s\n",
      "  training loss:\t\t0.683433\n",
      "  validation loss:\t\t0.952344\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 418 of 5000 took 0.023s\n",
      "  training loss:\t\t0.651427\n",
      "  validation loss:\t\t0.849708\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 419 of 5000 took 0.022s\n",
      "  training loss:\t\t0.643843\n",
      "  validation loss:\t\t0.848730\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 420 of 5000 took 0.024s\n",
      "  training loss:\t\t0.721557\n",
      "  validation loss:\t\t0.926699\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 421 of 5000 took 0.022s\n",
      "  training loss:\t\t0.719898\n",
      "  validation loss:\t\t0.901831\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 422 of 5000 took 0.022s\n",
      "  training loss:\t\t0.668271\n",
      "  validation loss:\t\t0.918925\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 423 of 5000 took 0.021s\n",
      "  training loss:\t\t0.669797\n",
      "  validation loss:\t\t0.989785\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 424 of 5000 took 0.022s\n",
      "  training loss:\t\t0.618860\n",
      "  validation loss:\t\t0.961918\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 425 of 5000 took 0.022s\n",
      "  training loss:\t\t0.669766\n",
      "  validation loss:\t\t1.449495\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 426 of 5000 took 0.022s\n",
      "  training loss:\t\t0.945473\n",
      "  validation loss:\t\t1.777845\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 427 of 5000 took 0.022s\n",
      "  training loss:\t\t1.260465\n",
      "  validation loss:\t\t1.294217\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 428 of 5000 took 0.023s\n",
      "  training loss:\t\t1.071973\n",
      "  validation loss:\t\t3.611128\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 429 of 5000 took 0.024s\n",
      "  training loss:\t\t2.581558\n",
      "  validation loss:\t\t1.913402\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 430 of 5000 took 0.022s\n",
      "  training loss:\t\t1.827139\n",
      "  validation loss:\t\t1.364711\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 431 of 5000 took 0.022s\n",
      "  training loss:\t\t1.264248\n",
      "  validation loss:\t\t1.707075\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 432 of 5000 took 0.022s\n",
      "  training loss:\t\t1.290522\n",
      "  validation loss:\t\t1.519519\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 433 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397352\n",
      "  validation loss:\t\t1.932067\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 434 of 5000 took 0.022s\n",
      "  training loss:\t\t1.667418\n",
      "  validation loss:\t\t1.333269\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 435 of 5000 took 0.021s\n",
      "  training loss:\t\t1.193246\n",
      "  validation loss:\t\t2.117708\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 436 of 5000 took 0.022s\n",
      "  training loss:\t\t1.662014\n",
      "  validation loss:\t\t1.767484\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 437 of 5000 took 0.023s\n",
      "  training loss:\t\t1.312976\n",
      "  validation loss:\t\t1.559282\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 438 of 5000 took 0.024s\n",
      "  training loss:\t\t1.664821\n",
      "  validation loss:\t\t1.656387\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 439 of 5000 took 0.022s\n",
      "  training loss:\t\t1.333407\n",
      "  validation loss:\t\t2.562499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 440 of 5000 took 0.021s\n",
      "  training loss:\t\t2.091359\n",
      "  validation loss:\t\t1.647553\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 441 of 5000 took 0.021s\n",
      "  training loss:\t\t1.661995\n",
      "  validation loss:\t\t1.910889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 442 of 5000 took 0.022s\n",
      "  training loss:\t\t2.041242\n",
      "  validation loss:\t\t2.532334\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 443 of 5000 took 0.022s\n",
      "  training loss:\t\t2.063402\n",
      "  validation loss:\t\t1.513812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 444 of 5000 took 0.021s\n",
      "  training loss:\t\t1.509328\n",
      "  validation loss:\t\t1.610265\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 445 of 5000 took 0.022s\n",
      "  training loss:\t\t1.884860\n",
      "  validation loss:\t\t1.333839\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 446 of 5000 took 0.023s\n",
      "  training loss:\t\t1.345712\n",
      "  validation loss:\t\t1.315765\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 447 of 5000 took 0.024s\n",
      "  training loss:\t\t1.441981\n",
      "  validation loss:\t\t1.278208\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 448 of 5000 took 0.023s\n",
      "  training loss:\t\t1.468901\n",
      "  validation loss:\t\t1.226760\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 449 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375279\n",
      "  validation loss:\t\t1.230882\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 450 of 5000 took 0.022s\n",
      "  training loss:\t\t1.313350\n",
      "  validation loss:\t\t1.222814\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 451 of 5000 took 0.022s\n",
      "  training loss:\t\t1.437095\n",
      "  validation loss:\t\t1.219504\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 452 of 5000 took 0.022s\n",
      "  training loss:\t\t1.351335\n",
      "  validation loss:\t\t1.205995\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 453 of 5000 took 0.022s\n",
      "  training loss:\t\t1.317641\n",
      "  validation loss:\t\t1.201541\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 454 of 5000 took 0.023s\n",
      "  training loss:\t\t1.348735\n",
      "  validation loss:\t\t1.200893\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 455 of 5000 took 0.022s\n",
      "  training loss:\t\t1.358033\n",
      "  validation loss:\t\t1.212941\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 456 of 5000 took 0.024s\n",
      "  training loss:\t\t1.490784\n",
      "  validation loss:\t\t1.224969\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 457 of 5000 took 0.022s\n",
      "  training loss:\t\t1.311156\n",
      "  validation loss:\t\t1.191420\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 458 of 5000 took 0.021s\n",
      "  training loss:\t\t1.509472\n",
      "  validation loss:\t\t1.184677\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 459 of 5000 took 0.022s\n",
      "  training loss:\t\t1.418553\n",
      "  validation loss:\t\t1.190772\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 460 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404137\n",
      "  validation loss:\t\t1.174004\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 461 of 5000 took 0.022s\n",
      "  training loss:\t\t1.349920\n",
      "  validation loss:\t\t1.180243\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 462 of 5000 took 0.022s\n",
      "  training loss:\t\t1.533263\n",
      "  validation loss:\t\t1.168689\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 463 of 5000 took 0.023s\n",
      "  training loss:\t\t1.370003\n",
      "  validation loss:\t\t1.159329\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 464 of 5000 took 0.023s\n",
      "  training loss:\t\t1.334855\n",
      "  validation loss:\t\t1.165787\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 465 of 5000 took 0.026s\n",
      "  training loss:\t\t1.214539\n",
      "  validation loss:\t\t1.147634\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 466 of 5000 took 0.022s\n",
      "  training loss:\t\t1.279974\n",
      "  validation loss:\t\t1.142881\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 467 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414789\n",
      "  validation loss:\t\t1.183589\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 468 of 5000 took 0.021s\n",
      "  training loss:\t\t1.282257\n",
      "  validation loss:\t\t1.189919\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 469 of 5000 took 0.021s\n",
      "  training loss:\t\t1.417553\n",
      "  validation loss:\t\t1.283294\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 470 of 5000 took 0.021s\n",
      "  training loss:\t\t1.353049\n",
      "  validation loss:\t\t1.222700\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 471 of 5000 took 0.021s\n",
      "  training loss:\t\t1.371468\n",
      "  validation loss:\t\t1.131045\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 472 of 5000 took 0.022s\n",
      "  training loss:\t\t1.292719\n",
      "  validation loss:\t\t1.130086\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 473 of 5000 took 0.022s\n",
      "  training loss:\t\t1.260091\n",
      "  validation loss:\t\t1.123466\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 474 of 5000 took 0.024s\n",
      "  training loss:\t\t1.164172\n",
      "  validation loss:\t\t1.107551\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 475 of 5000 took 0.022s\n",
      "  training loss:\t\t1.298585\n",
      "  validation loss:\t\t1.111961\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 476 of 5000 took 0.022s\n",
      "  training loss:\t\t1.231610\n",
      "  validation loss:\t\t1.101293\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 477 of 5000 took 0.022s\n",
      "  training loss:\t\t1.183068\n",
      "  validation loss:\t\t1.097708\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 478 of 5000 took 0.022s\n",
      "  training loss:\t\t1.206192\n",
      "  validation loss:\t\t1.102176\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 479 of 5000 took 0.022s\n",
      "  training loss:\t\t1.203777\n",
      "  validation loss:\t\t1.113432\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 480 of 5000 took 0.022s\n",
      "  training loss:\t\t1.342240\n",
      "  validation loss:\t\t1.087327\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 481 of 5000 took 0.022s\n",
      "  training loss:\t\t1.344345\n",
      "  validation loss:\t\t1.091529\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 482 of 5000 took 0.023s\n",
      "  training loss:\t\t1.266659\n",
      "  validation loss:\t\t1.096091\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 483 of 5000 took 0.024s\n",
      "  training loss:\t\t1.233630\n",
      "  validation loss:\t\t1.225473\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 484 of 5000 took 0.023s\n",
      "  training loss:\t\t1.306340\n",
      "  validation loss:\t\t1.289028\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 485 of 5000 took 0.022s\n",
      "  training loss:\t\t1.610014\n",
      "  validation loss:\t\t1.212760\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 486 of 5000 took 0.022s\n",
      "  training loss:\t\t1.361872\n",
      "  validation loss:\t\t1.144203\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 487 of 5000 took 0.022s\n",
      "  training loss:\t\t1.220998\n",
      "  validation loss:\t\t1.078127\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 488 of 5000 took 0.022s\n",
      "  training loss:\t\t1.182136\n",
      "  validation loss:\t\t1.102532\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 489 of 5000 took 0.022s\n",
      "  training loss:\t\t1.243684\n",
      "  validation loss:\t\t1.081804\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 490 of 5000 took 0.022s\n",
      "  training loss:\t\t1.157425\n",
      "  validation loss:\t\t1.071452\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 491 of 5000 took 0.022s\n",
      "  training loss:\t\t1.159722\n",
      "  validation loss:\t\t1.070897\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 492 of 5000 took 0.024s\n",
      "  training loss:\t\t1.220081\n",
      "  validation loss:\t\t1.063613\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 493 of 5000 took 0.022s\n",
      "  training loss:\t\t1.237615\n",
      "  validation loss:\t\t1.067472\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 494 of 5000 took 0.022s\n",
      "  training loss:\t\t1.121037\n",
      "  validation loss:\t\t1.056101\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 495 of 5000 took 0.021s\n",
      "  training loss:\t\t1.097994\n",
      "  validation loss:\t\t1.061070\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 496 of 5000 took 0.022s\n",
      "  training loss:\t\t1.053570\n",
      "  validation loss:\t\t1.057138\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 497 of 5000 took 0.021s\n",
      "  training loss:\t\t1.265480\n",
      "  validation loss:\t\t1.061184\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 498 of 5000 took 0.021s\n",
      "  training loss:\t\t1.151893\n",
      "  validation loss:\t\t1.063624\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 499 of 5000 took 0.023s\n",
      "  training loss:\t\t1.175424\n",
      "  validation loss:\t\t1.053641\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 500 of 5000 took 0.023s\n",
      "  training loss:\t\t1.051717\n",
      "  validation loss:\t\t1.043457\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 501 of 5000 took 0.024s\n",
      "  training loss:\t\t1.099658\n",
      "  validation loss:\t\t1.044401\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 502 of 5000 took 0.022s\n",
      "  training loss:\t\t1.160516\n",
      "  validation loss:\t\t1.049407\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 503 of 5000 took 0.022s\n",
      "  training loss:\t\t1.088305\n",
      "  validation loss:\t\t1.071604\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 504 of 5000 took 0.022s\n",
      "  training loss:\t\t1.120831\n",
      "  validation loss:\t\t1.074157\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 505 of 5000 took 0.022s\n",
      "  training loss:\t\t1.210123\n",
      "  validation loss:\t\t1.044195\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 506 of 5000 took 0.022s\n",
      "  training loss:\t\t1.054060\n",
      "  validation loss:\t\t1.041863\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 507 of 5000 took 0.022s\n",
      "  training loss:\t\t1.165816\n",
      "  validation loss:\t\t1.044460\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 508 of 5000 took 0.022s\n",
      "  training loss:\t\t1.084561\n",
      "  validation loss:\t\t1.038027\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 509 of 5000 took 0.023s\n",
      "  training loss:\t\t1.077125\n",
      "  validation loss:\t\t1.093964\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 510 of 5000 took 0.023s\n",
      "  training loss:\t\t1.119743\n",
      "  validation loss:\t\t1.222265\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 511 of 5000 took 0.024s\n",
      "  training loss:\t\t1.223285\n",
      "  validation loss:\t\t1.069284\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 512 of 5000 took 0.022s\n",
      "  training loss:\t\t1.142845\n",
      "  validation loss:\t\t1.122586\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 513 of 5000 took 0.021s\n",
      "  training loss:\t\t1.345526\n",
      "  validation loss:\t\t1.395983\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 514 of 5000 took 0.021s\n",
      "  training loss:\t\t1.533300\n",
      "  validation loss:\t\t2.430473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 515 of 5000 took 0.022s\n",
      "  training loss:\t\t2.444069\n",
      "  validation loss:\t\t1.366907\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 516 of 5000 took 0.021s\n",
      "  training loss:\t\t1.374401\n",
      "  validation loss:\t\t1.419255\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 517 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386755\n",
      "  validation loss:\t\t1.595202\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 518 of 5000 took 0.022s\n",
      "  training loss:\t\t1.643550\n",
      "  validation loss:\t\t1.034472\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 519 of 5000 took 0.023s\n",
      "  training loss:\t\t1.123907\n",
      "  validation loss:\t\t1.076883\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 520 of 5000 took 0.023s\n",
      "  training loss:\t\t1.063131\n",
      "  validation loss:\t\t1.045645\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 521 of 5000 took 0.024s\n",
      "  training loss:\t\t1.181599\n",
      "  validation loss:\t\t1.042373\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 522 of 5000 took 0.023s\n",
      "  training loss:\t\t1.149053\n",
      "  validation loss:\t\t1.047270\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 523 of 5000 took 0.022s\n",
      "  training loss:\t\t1.238973\n",
      "  validation loss:\t\t1.047731\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 524 of 5000 took 0.022s\n",
      "  training loss:\t\t1.210732\n",
      "  validation loss:\t\t1.040250\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 525 of 5000 took 0.021s\n",
      "  training loss:\t\t1.134185\n",
      "  validation loss:\t\t1.042428\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 526 of 5000 took 0.022s\n",
      "  training loss:\t\t1.114321\n",
      "  validation loss:\t\t1.042835\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 527 of 5000 took 0.022s\n",
      "  training loss:\t\t1.086453\n",
      "  validation loss:\t\t1.034577\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 528 of 5000 took 0.023s\n",
      "  training loss:\t\t1.127641\n",
      "  validation loss:\t\t1.033519\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 529 of 5000 took 0.022s\n",
      "  training loss:\t\t1.136652\n",
      "  validation loss:\t\t1.028026\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 530 of 5000 took 0.024s\n",
      "  training loss:\t\t1.175785\n",
      "  validation loss:\t\t1.028971\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 531 of 5000 took 0.022s\n",
      "  training loss:\t\t0.981335\n",
      "  validation loss:\t\t1.023959\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 532 of 5000 took 0.022s\n",
      "  training loss:\t\t1.172132\n",
      "  validation loss:\t\t1.035028\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 533 of 5000 took 0.022s\n",
      "  training loss:\t\t1.130008\n",
      "  validation loss:\t\t1.042896\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 534 of 5000 took 0.022s\n",
      "  training loss:\t\t1.024140\n",
      "  validation loss:\t\t1.044142\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 535 of 5000 took 0.022s\n",
      "  training loss:\t\t1.134766\n",
      "  validation loss:\t\t1.039497\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 536 of 5000 took 0.022s\n",
      "  training loss:\t\t1.177357\n",
      "  validation loss:\t\t1.027880\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 537 of 5000 took 0.022s\n",
      "  training loss:\t\t0.979942\n",
      "  validation loss:\t\t1.025545\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 538 of 5000 took 0.022s\n",
      "  training loss:\t\t1.075668\n",
      "  validation loss:\t\t1.021098\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 539 of 5000 took 0.021s\n",
      "  training loss:\t\t1.085202\n",
      "  validation loss:\t\t1.028270\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 540 of 5000 took 0.023s\n",
      "  training loss:\t\t1.181059\n",
      "  validation loss:\t\t1.018646\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 541 of 5000 took 0.022s\n",
      "  training loss:\t\t1.072796\n",
      "  validation loss:\t\t1.020530\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 542 of 5000 took 0.022s\n",
      "  training loss:\t\t1.078811\n",
      "  validation loss:\t\t1.015614\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 543 of 5000 took 0.022s\n",
      "  training loss:\t\t1.086488\n",
      "  validation loss:\t\t1.014984\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 544 of 5000 took 0.022s\n",
      "  training loss:\t\t1.194185\n",
      "  validation loss:\t\t1.015791\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 545 of 5000 took 0.022s\n",
      "  training loss:\t\t1.111693\n",
      "  validation loss:\t\t1.015401\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 546 of 5000 took 0.022s\n",
      "  training loss:\t\t0.956545\n",
      "  validation loss:\t\t1.016834\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 547 of 5000 took 0.022s\n",
      "  training loss:\t\t1.020676\n",
      "  validation loss:\t\t1.027208\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 548 of 5000 took 0.022s\n",
      "  training loss:\t\t1.036965\n",
      "  validation loss:\t\t1.021890\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 549 of 5000 took 0.023s\n",
      "  training loss:\t\t1.014885\n",
      "  validation loss:\t\t1.021055\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 550 of 5000 took 0.023s\n",
      "  training loss:\t\t0.993905\n",
      "  validation loss:\t\t1.016483\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 551 of 5000 took 0.022s\n",
      "  training loss:\t\t1.023147\n",
      "  validation loss:\t\t1.017794\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 552 of 5000 took 0.022s\n",
      "  training loss:\t\t1.109485\n",
      "  validation loss:\t\t1.016882\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 553 of 5000 took 0.021s\n",
      "  training loss:\t\t1.132823\n",
      "  validation loss:\t\t1.014871\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 554 of 5000 took 0.021s\n",
      "  training loss:\t\t0.976340\n",
      "  validation loss:\t\t1.009427\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 555 of 5000 took 0.021s\n",
      "  training loss:\t\t1.083183\n",
      "  validation loss:\t\t1.009146\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 556 of 5000 took 0.021s\n",
      "  training loss:\t\t1.169810\n",
      "  validation loss:\t\t1.006824\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 557 of 5000 took 0.023s\n",
      "  training loss:\t\t1.107907\n",
      "  validation loss:\t\t1.010209\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 558 of 5000 took 0.022s\n",
      "  training loss:\t\t1.004450\n",
      "  validation loss:\t\t1.010070\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 559 of 5000 took 0.024s\n",
      "  training loss:\t\t1.079575\n",
      "  validation loss:\t\t1.011559\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 560 of 5000 took 0.023s\n",
      "  training loss:\t\t0.998358\n",
      "  validation loss:\t\t1.006199\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 561 of 5000 took 0.022s\n",
      "  training loss:\t\t1.055009\n",
      "  validation loss:\t\t1.030349\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 562 of 5000 took 0.022s\n",
      "  training loss:\t\t1.221850\n",
      "  validation loss:\t\t1.010498\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 563 of 5000 took 0.021s\n",
      "  training loss:\t\t1.072198\n",
      "  validation loss:\t\t1.023026\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 564 of 5000 took 0.022s\n",
      "  training loss:\t\t1.030201\n",
      "  validation loss:\t\t1.027799\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 565 of 5000 took 0.022s\n",
      "  training loss:\t\t1.008850\n",
      "  validation loss:\t\t1.012383\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 566 of 5000 took 0.022s\n",
      "  training loss:\t\t1.137275\n",
      "  validation loss:\t\t1.009519\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 567 of 5000 took 0.023s\n",
      "  training loss:\t\t0.922365\n",
      "  validation loss:\t\t1.056614\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 568 of 5000 took 0.023s\n",
      "  training loss:\t\t0.999873\n",
      "  validation loss:\t\t1.007518\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 569 of 5000 took 0.024s\n",
      "  training loss:\t\t1.083513\n",
      "  validation loss:\t\t1.004998\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 570 of 5000 took 0.022s\n",
      "  training loss:\t\t0.931728\n",
      "  validation loss:\t\t1.029162\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 571 of 5000 took 0.023s\n",
      "  training loss:\t\t1.041834\n",
      "  validation loss:\t\t1.024557\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 572 of 5000 took 0.021s\n",
      "  training loss:\t\t1.041734\n",
      "  validation loss:\t\t1.006656\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 573 of 5000 took 0.021s\n",
      "  training loss:\t\t1.122356\n",
      "  validation loss:\t\t1.001562\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 574 of 5000 took 0.022s\n",
      "  training loss:\t\t1.032158\n",
      "  validation loss:\t\t1.085231\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 575 of 5000 took 0.022s\n",
      "  training loss:\t\t1.022342\n",
      "  validation loss:\t\t1.001758\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 576 of 5000 took 0.022s\n",
      "  training loss:\t\t1.107528\n",
      "  validation loss:\t\t1.018364\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 577 of 5000 took 0.024s\n",
      "  training loss:\t\t1.114113\n",
      "  validation loss:\t\t1.012046\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 578 of 5000 took 0.022s\n",
      "  training loss:\t\t0.938715\n",
      "  validation loss:\t\t1.030939\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 579 of 5000 took 0.023s\n",
      "  training loss:\t\t1.066277\n",
      "  validation loss:\t\t1.004571\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 580 of 5000 took 0.022s\n",
      "  training loss:\t\t0.991423\n",
      "  validation loss:\t\t1.015297\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 581 of 5000 took 0.022s\n",
      "  training loss:\t\t0.968032\n",
      "  validation loss:\t\t1.006125\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 582 of 5000 took 0.021s\n",
      "  training loss:\t\t1.121924\n",
      "  validation loss:\t\t1.024160\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 583 of 5000 took 0.022s\n",
      "  training loss:\t\t1.024357\n",
      "  validation loss:\t\t1.018058\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 584 of 5000 took 0.022s\n",
      "  training loss:\t\t1.058265\n",
      "  validation loss:\t\t1.004532\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 585 of 5000 took 0.022s\n",
      "  training loss:\t\t1.047968\n",
      "  validation loss:\t\t1.004309\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 586 of 5000 took 0.023s\n",
      "  training loss:\t\t0.947180\n",
      "  validation loss:\t\t1.027715\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 587 of 5000 took 0.022s\n",
      "  training loss:\t\t0.983880\n",
      "  validation loss:\t\t1.003526\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 588 of 5000 took 0.022s\n",
      "  training loss:\t\t1.069466\n",
      "  validation loss:\t\t1.016040\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 589 of 5000 took 0.023s\n",
      "  training loss:\t\t1.043262\n",
      "  validation loss:\t\t1.006749\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 590 of 5000 took 0.023s\n",
      "  training loss:\t\t1.079716\n",
      "  validation loss:\t\t1.015594\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 591 of 5000 took 0.023s\n",
      "  training loss:\t\t0.974244\n",
      "  validation loss:\t\t1.025944\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 592 of 5000 took 0.023s\n",
      "  training loss:\t\t1.006423\n",
      "  validation loss:\t\t1.076754\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 593 of 5000 took 0.021s\n",
      "  training loss:\t\t1.024697\n",
      "  validation loss:\t\t1.006732\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 594 of 5000 took 0.022s\n",
      "  training loss:\t\t0.994059\n",
      "  validation loss:\t\t0.997920\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 595 of 5000 took 0.021s\n",
      "  training loss:\t\t1.027721\n",
      "  validation loss:\t\t0.999640\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 596 of 5000 took 0.022s\n",
      "  training loss:\t\t1.138576\n",
      "  validation loss:\t\t1.024989\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 597 of 5000 took 0.021s\n",
      "  training loss:\t\t0.977457\n",
      "  validation loss:\t\t1.011032\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 598 of 5000 took 0.023s\n",
      "  training loss:\t\t1.005039\n",
      "  validation loss:\t\t0.996741\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 599 of 5000 took 0.022s\n",
      "  training loss:\t\t0.981665\n",
      "  validation loss:\t\t0.995762\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 600 of 5000 took 0.021s\n",
      "  training loss:\t\t0.941564\n",
      "  validation loss:\t\t1.062947\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 601 of 5000 took 0.021s\n",
      "  training loss:\t\t1.057332\n",
      "  validation loss:\t\t1.025332\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 602 of 5000 took 0.022s\n",
      "  training loss:\t\t1.096261\n",
      "  validation loss:\t\t0.996223\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 603 of 5000 took 0.022s\n",
      "  training loss:\t\t0.980123\n",
      "  validation loss:\t\t1.030482\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 604 of 5000 took 0.022s\n",
      "  training loss:\t\t1.033099\n",
      "  validation loss:\t\t0.996937\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 605 of 5000 took 0.021s\n",
      "  training loss:\t\t0.992749\n",
      "  validation loss:\t\t1.018641\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 606 of 5000 took 0.023s\n",
      "  training loss:\t\t0.926877\n",
      "  validation loss:\t\t1.010781\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 607 of 5000 took 0.023s\n",
      "  training loss:\t\t1.047106\n",
      "  validation loss:\t\t0.992255\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 608 of 5000 took 0.024s\n",
      "  training loss:\t\t0.997316\n",
      "  validation loss:\t\t1.002373\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 609 of 5000 took 0.021s\n",
      "  training loss:\t\t0.967962\n",
      "  validation loss:\t\t1.011444\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 610 of 5000 took 0.021s\n",
      "  training loss:\t\t1.055771\n",
      "  validation loss:\t\t0.995797\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 611 of 5000 took 0.021s\n",
      "  training loss:\t\t0.985416\n",
      "  validation loss:\t\t1.022682\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 612 of 5000 took 0.021s\n",
      "  training loss:\t\t0.973961\n",
      "  validation loss:\t\t1.005803\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 613 of 5000 took 0.022s\n",
      "  training loss:\t\t1.010341\n",
      "  validation loss:\t\t0.991625\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 614 of 5000 took 0.021s\n",
      "  training loss:\t\t0.970413\n",
      "  validation loss:\t\t0.997318\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 615 of 5000 took 0.021s\n",
      "  training loss:\t\t0.812968\n",
      "  validation loss:\t\t1.062670\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 616 of 5000 took 0.022s\n",
      "  training loss:\t\t1.043657\n",
      "  validation loss:\t\t0.990126\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 617 of 5000 took 0.022s\n",
      "  training loss:\t\t0.949437\n",
      "  validation loss:\t\t0.991599\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 618 of 5000 took 0.024s\n",
      "  training loss:\t\t0.962615\n",
      "  validation loss:\t\t1.012919\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 619 of 5000 took 0.022s\n",
      "  training loss:\t\t0.982421\n",
      "  validation loss:\t\t0.995341\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 620 of 5000 took 0.021s\n",
      "  training loss:\t\t0.952526\n",
      "  validation loss:\t\t1.091328\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 621 of 5000 took 0.021s\n",
      "  training loss:\t\t0.958650\n",
      "  validation loss:\t\t1.037132\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 622 of 5000 took 0.022s\n",
      "  training loss:\t\t0.890880\n",
      "  validation loss:\t\t1.030491\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 623 of 5000 took 0.021s\n",
      "  training loss:\t\t0.929509\n",
      "  validation loss:\t\t1.030518\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 624 of 5000 took 0.022s\n",
      "  training loss:\t\t1.056857\n",
      "  validation loss:\t\t1.480614\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 625 of 5000 took 0.021s\n",
      "  training loss:\t\t1.218635\n",
      "  validation loss:\t\t2.169030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 626 of 5000 took 0.022s\n",
      "  training loss:\t\t2.001413\n",
      "  validation loss:\t\t1.038471\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 627 of 5000 took 0.021s\n",
      "  training loss:\t\t0.959260\n",
      "  validation loss:\t\t1.252229\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 628 of 5000 took 0.024s\n",
      "  training loss:\t\t0.948245\n",
      "  validation loss:\t\t1.066801\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 629 of 5000 took 0.022s\n",
      "  training loss:\t\t0.965178\n",
      "  validation loss:\t\t1.438440\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 630 of 5000 took 0.022s\n",
      "  training loss:\t\t1.073116\n",
      "  validation loss:\t\t2.127817\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 631 of 5000 took 0.022s\n",
      "  training loss:\t\t2.170622\n",
      "  validation loss:\t\t2.074483\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 632 of 5000 took 0.022s\n",
      "  training loss:\t\t2.094407\n",
      "  validation loss:\t\t2.006361\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 633 of 5000 took 0.022s\n",
      "  training loss:\t\t2.040267\n",
      "  validation loss:\t\t1.929106\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 634 of 5000 took 0.021s\n",
      "  training loss:\t\t1.909575\n",
      "  validation loss:\t\t1.850959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 635 of 5000 took 0.022s\n",
      "  training loss:\t\t1.848647\n",
      "  validation loss:\t\t1.774620\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 636 of 5000 took 0.022s\n",
      "  training loss:\t\t1.786427\n",
      "  validation loss:\t\t1.703899\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 637 of 5000 took 0.021s\n",
      "  training loss:\t\t1.680450\n",
      "  validation loss:\t\t1.642621\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 638 of 5000 took 0.023s\n",
      "  training loss:\t\t1.606863\n",
      "  validation loss:\t\t1.591511\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 639 of 5000 took 0.023s\n",
      "  training loss:\t\t1.596209\n",
      "  validation loss:\t\t1.550915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 640 of 5000 took 0.021s\n",
      "  training loss:\t\t1.581205\n",
      "  validation loss:\t\t1.520127\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 641 of 5000 took 0.021s\n",
      "  training loss:\t\t1.496697\n",
      "  validation loss:\t\t1.499160\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 642 of 5000 took 0.022s\n",
      "  training loss:\t\t1.527939\n",
      "  validation loss:\t\t1.484365\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 643 of 5000 took 0.022s\n",
      "  training loss:\t\t1.509336\n",
      "  validation loss:\t\t1.474939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 644 of 5000 took 0.022s\n",
      "  training loss:\t\t1.450787\n",
      "  validation loss:\t\t1.470673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 645 of 5000 took 0.021s\n",
      "  training loss:\t\t1.488102\n",
      "  validation loss:\t\t1.469017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 646 of 5000 took 0.021s\n",
      "  training loss:\t\t1.484086\n",
      "  validation loss:\t\t1.468756\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 647 of 5000 took 0.022s\n",
      "  training loss:\t\t1.477652\n",
      "  validation loss:\t\t1.469821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 648 of 5000 took 0.024s\n",
      "  training loss:\t\t1.497942\n",
      "  validation loss:\t\t1.470593\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 649 of 5000 took 0.022s\n",
      "  training loss:\t\t1.483882\n",
      "  validation loss:\t\t1.471588\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 650 of 5000 took 0.023s\n",
      "  training loss:\t\t1.476664\n",
      "  validation loss:\t\t1.472571\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 651 of 5000 took 0.022s\n",
      "  training loss:\t\t1.478995\n",
      "  validation loss:\t\t1.473763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 652 of 5000 took 0.022s\n",
      "  training loss:\t\t1.467540\n",
      "  validation loss:\t\t1.473951\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 653 of 5000 took 0.023s\n",
      "  training loss:\t\t1.445294\n",
      "  validation loss:\t\t1.473813\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 654 of 5000 took 0.023s\n",
      "  training loss:\t\t1.517781\n",
      "  validation loss:\t\t1.471563\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 655 of 5000 took 0.022s\n",
      "  training loss:\t\t1.506375\n",
      "  validation loss:\t\t1.468554\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 656 of 5000 took 0.022s\n",
      "  training loss:\t\t1.488395\n",
      "  validation loss:\t\t1.464646\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 657 of 5000 took 0.023s\n",
      "  training loss:\t\t1.483961\n",
      "  validation loss:\t\t1.460169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 658 of 5000 took 0.023s\n",
      "  training loss:\t\t1.468322\n",
      "  validation loss:\t\t1.456049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 659 of 5000 took 0.021s\n",
      "  training loss:\t\t1.462216\n",
      "  validation loss:\t\t1.451771\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 660 of 5000 took 0.022s\n",
      "  training loss:\t\t1.465984\n",
      "  validation loss:\t\t1.447245\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 661 of 5000 took 0.022s\n",
      "  training loss:\t\t1.457497\n",
      "  validation loss:\t\t1.443014\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 662 of 5000 took 0.022s\n",
      "  training loss:\t\t1.455708\n",
      "  validation loss:\t\t1.439002\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 663 of 5000 took 0.021s\n",
      "  training loss:\t\t1.450934\n",
      "  validation loss:\t\t1.435390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 664 of 5000 took 0.022s\n",
      "  training loss:\t\t1.436514\n",
      "  validation loss:\t\t1.432225\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 665 of 5000 took 0.021s\n",
      "  training loss:\t\t1.444837\n",
      "  validation loss:\t\t1.429909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 666 of 5000 took 0.021s\n",
      "  training loss:\t\t1.443896\n",
      "  validation loss:\t\t1.428145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 667 of 5000 took 0.023s\n",
      "  training loss:\t\t1.431571\n",
      "  validation loss:\t\t1.426961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 668 of 5000 took 0.022s\n",
      "  training loss:\t\t1.425076\n",
      "  validation loss:\t\t1.426123\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 669 of 5000 took 0.022s\n",
      "  training loss:\t\t1.452289\n",
      "  validation loss:\t\t1.425310\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 670 of 5000 took 0.022s\n",
      "  training loss:\t\t1.425694\n",
      "  validation loss:\t\t1.424713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 671 of 5000 took 0.022s\n",
      "  training loss:\t\t1.421796\n",
      "  validation loss:\t\t1.424338\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 672 of 5000 took 0.022s\n",
      "  training loss:\t\t1.452485\n",
      "  validation loss:\t\t1.423909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 673 of 5000 took 0.022s\n",
      "  training loss:\t\t1.420045\n",
      "  validation loss:\t\t1.423311\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 674 of 5000 took 0.022s\n",
      "  training loss:\t\t1.429159\n",
      "  validation loss:\t\t1.422511\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 675 of 5000 took 0.023s\n",
      "  training loss:\t\t1.422339\n",
      "  validation loss:\t\t1.421677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 676 of 5000 took 0.023s\n",
      "  training loss:\t\t1.431166\n",
      "  validation loss:\t\t1.420829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 677 of 5000 took 0.022s\n",
      "  training loss:\t\t1.428648\n",
      "  validation loss:\t\t1.419961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 678 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404373\n",
      "  validation loss:\t\t1.419065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 679 of 5000 took 0.022s\n",
      "  training loss:\t\t1.427048\n",
      "  validation loss:\t\t1.418112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 680 of 5000 took 0.021s\n",
      "  training loss:\t\t1.421934\n",
      "  validation loss:\t\t1.417307\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 681 of 5000 took 0.022s\n",
      "  training loss:\t\t1.430797\n",
      "  validation loss:\t\t1.416343\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 682 of 5000 took 0.022s\n",
      "  training loss:\t\t1.417828\n",
      "  validation loss:\t\t1.415464\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 683 of 5000 took 0.022s\n",
      "  training loss:\t\t1.418027\n",
      "  validation loss:\t\t1.414762\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 684 of 5000 took 0.022s\n",
      "  training loss:\t\t1.426430\n",
      "  validation loss:\t\t1.414133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 685 of 5000 took 0.024s\n",
      "  training loss:\t\t1.417492\n",
      "  validation loss:\t\t1.413469\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 686 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415384\n",
      "  validation loss:\t\t1.413015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 687 of 5000 took 0.022s\n",
      "  training loss:\t\t1.434740\n",
      "  validation loss:\t\t1.412508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 688 of 5000 took 0.021s\n",
      "  training loss:\t\t1.432005\n",
      "  validation loss:\t\t1.412085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 689 of 5000 took 0.022s\n",
      "  training loss:\t\t1.426298\n",
      "  validation loss:\t\t1.411569\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 690 of 5000 took 0.021s\n",
      "  training loss:\t\t1.423079\n",
      "  validation loss:\t\t1.411116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 691 of 5000 took 0.021s\n",
      "  training loss:\t\t1.413365\n",
      "  validation loss:\t\t1.410754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 692 of 5000 took 0.023s\n",
      "  training loss:\t\t1.415810\n",
      "  validation loss:\t\t1.410382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 693 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416807\n",
      "  validation loss:\t\t1.409975\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 694 of 5000 took 0.024s\n",
      "  training loss:\t\t1.417575\n",
      "  validation loss:\t\t1.409423\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 695 of 5000 took 0.022s\n",
      "  training loss:\t\t1.420897\n",
      "  validation loss:\t\t1.408966\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 696 of 5000 took 0.022s\n",
      "  training loss:\t\t1.427365\n",
      "  validation loss:\t\t1.408525\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 697 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407400\n",
      "  validation loss:\t\t1.408077\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 698 of 5000 took 0.021s\n",
      "  training loss:\t\t1.423978\n",
      "  validation loss:\t\t1.407700\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 699 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414416\n",
      "  validation loss:\t\t1.407199\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 700 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415192\n",
      "  validation loss:\t\t1.406830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 701 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406490\n",
      "  validation loss:\t\t1.406425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 702 of 5000 took 0.023s\n",
      "  training loss:\t\t1.435304\n",
      "  validation loss:\t\t1.406065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 703 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416273\n",
      "  validation loss:\t\t1.405767\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 704 of 5000 took 0.024s\n",
      "  training loss:\t\t1.413157\n",
      "  validation loss:\t\t1.405498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 705 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411762\n",
      "  validation loss:\t\t1.405249\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 706 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414378\n",
      "  validation loss:\t\t1.405014\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 707 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391813\n",
      "  validation loss:\t\t1.404765\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 708 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405967\n",
      "  validation loss:\t\t1.404499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 709 of 5000 took 0.021s\n",
      "  training loss:\t\t1.425240\n",
      "  validation loss:\t\t1.404330\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 710 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410102\n",
      "  validation loss:\t\t1.404208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 711 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403034\n",
      "  validation loss:\t\t1.404090\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 712 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409388\n",
      "  validation loss:\t\t1.404036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 713 of 5000 took 0.024s\n",
      "  training loss:\t\t1.426560\n",
      "  validation loss:\t\t1.403726\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 714 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416420\n",
      "  validation loss:\t\t1.403508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 715 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414134\n",
      "  validation loss:\t\t1.403271\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 716 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411438\n",
      "  validation loss:\t\t1.403030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 717 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402154\n",
      "  validation loss:\t\t1.402924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 718 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404589\n",
      "  validation loss:\t\t1.402790\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 719 of 5000 took 0.021s\n",
      "  training loss:\t\t1.408595\n",
      "  validation loss:\t\t1.402642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 720 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413580\n",
      "  validation loss:\t\t1.402505\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 721 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399552\n",
      "  validation loss:\t\t1.402313\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 722 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407905\n",
      "  validation loss:\t\t1.401990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 723 of 5000 took 0.023s\n",
      "  training loss:\t\t1.417289\n",
      "  validation loss:\t\t1.401779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 724 of 5000 took 0.022s\n",
      "  training loss:\t\t1.421478\n",
      "  validation loss:\t\t1.401569\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 725 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402101\n",
      "  validation loss:\t\t1.401237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 726 of 5000 took 0.021s\n",
      "  training loss:\t\t1.421331\n",
      "  validation loss:\t\t1.401035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 727 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401714\n",
      "  validation loss:\t\t1.400762\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 728 of 5000 took 0.021s\n",
      "  training loss:\t\t1.416702\n",
      "  validation loss:\t\t1.400570\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 729 of 5000 took 0.021s\n",
      "  training loss:\t\t1.415315\n",
      "  validation loss:\t\t1.400401\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 730 of 5000 took 0.023s\n",
      "  training loss:\t\t1.414664\n",
      "  validation loss:\t\t1.400264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 731 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413844\n",
      "  validation loss:\t\t1.400126\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 732 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388560\n",
      "  validation loss:\t\t1.400044\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 733 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398283\n",
      "  validation loss:\t\t1.400010\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 734 of 5000 took 0.022s\n",
      "  training loss:\t\t1.418378\n",
      "  validation loss:\t\t1.399820\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 735 of 5000 took 0.021s\n",
      "  training loss:\t\t1.430497\n",
      "  validation loss:\t\t1.399632\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 736 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401406\n",
      "  validation loss:\t\t1.399474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 737 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386977\n",
      "  validation loss:\t\t1.399335\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 738 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398305\n",
      "  validation loss:\t\t1.399273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 739 of 5000 took 0.021s\n",
      "  training loss:\t\t1.412037\n",
      "  validation loss:\t\t1.399135\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 740 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394029\n",
      "  validation loss:\t\t1.398982\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 741 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398087\n",
      "  validation loss:\t\t1.398781\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 742 of 5000 took 0.024s\n",
      "  training loss:\t\t1.403514\n",
      "  validation loss:\t\t1.398622\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 743 of 5000 took 0.021s\n",
      "  training loss:\t\t1.402247\n",
      "  validation loss:\t\t1.398516\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 744 of 5000 took 0.021s\n",
      "  training loss:\t\t1.422369\n",
      "  validation loss:\t\t1.398400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 745 of 5000 took 0.021s\n",
      "  training loss:\t\t1.402647\n",
      "  validation loss:\t\t1.398461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 746 of 5000 took 0.021s\n",
      "  training loss:\t\t1.417199\n",
      "  validation loss:\t\t1.398371\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 747 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401929\n",
      "  validation loss:\t\t1.398274\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 748 of 5000 took 0.021s\n",
      "  training loss:\t\t1.415454\n",
      "  validation loss:\t\t1.398178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 749 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412906\n",
      "  validation loss:\t\t1.398146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 750 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407284\n",
      "  validation loss:\t\t1.397985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 751 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406114\n",
      "  validation loss:\t\t1.397964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 752 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397740\n",
      "  validation loss:\t\t1.397781\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 753 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406429\n",
      "  validation loss:\t\t1.397510\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 754 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422266\n",
      "  validation loss:\t\t1.397388\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 755 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399892\n",
      "  validation loss:\t\t1.397230\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 756 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403199\n",
      "  validation loss:\t\t1.397081\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 757 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388973\n",
      "  validation loss:\t\t1.396982\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 758 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405351\n",
      "  validation loss:\t\t1.396877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 759 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402494\n",
      "  validation loss:\t\t1.396789\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 760 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404105\n",
      "  validation loss:\t\t1.396751\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 761 of 5000 took 0.024s\n",
      "  training loss:\t\t1.406467\n",
      "  validation loss:\t\t1.396807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 762 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395195\n",
      "  validation loss:\t\t1.396927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 763 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392974\n",
      "  validation loss:\t\t1.396946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 764 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392038\n",
      "  validation loss:\t\t1.397097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 765 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405524\n",
      "  validation loss:\t\t1.397210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 766 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407290\n",
      "  validation loss:\t\t1.397333\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 767 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410869\n",
      "  validation loss:\t\t1.397284\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 768 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398598\n",
      "  validation loss:\t\t1.397200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 769 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406542\n",
      "  validation loss:\t\t1.396961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 770 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409348\n",
      "  validation loss:\t\t1.396626\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 771 of 5000 took 0.024s\n",
      "  training loss:\t\t1.404549\n",
      "  validation loss:\t\t1.396388\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 772 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405935\n",
      "  validation loss:\t\t1.396138\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 773 of 5000 took 0.021s\n",
      "  training loss:\t\t1.409907\n",
      "  validation loss:\t\t1.396020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 774 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390798\n",
      "  validation loss:\t\t1.395865\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 775 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412656\n",
      "  validation loss:\t\t1.395808\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 776 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402093\n",
      "  validation loss:\t\t1.395762\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 777 of 5000 took 0.022s\n",
      "  training loss:\t\t1.421254\n",
      "  validation loss:\t\t1.395678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 778 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406627\n",
      "  validation loss:\t\t1.395641\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 779 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402894\n",
      "  validation loss:\t\t1.395466\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 780 of 5000 took 0.024s\n",
      "  training loss:\t\t1.402224\n",
      "  validation loss:\t\t1.395373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 781 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409794\n",
      "  validation loss:\t\t1.395325\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 782 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381034\n",
      "  validation loss:\t\t1.395331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 783 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398308\n",
      "  validation loss:\t\t1.395381\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 784 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406495\n",
      "  validation loss:\t\t1.395289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 785 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395251\n",
      "  validation loss:\t\t1.395151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 786 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405907\n",
      "  validation loss:\t\t1.395079\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 787 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413625\n",
      "  validation loss:\t\t1.394980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 788 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395460\n",
      "  validation loss:\t\t1.394864\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 789 of 5000 took 0.024s\n",
      "  training loss:\t\t1.410879\n",
      "  validation loss:\t\t1.394763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 790 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402040\n",
      "  validation loss:\t\t1.394716\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 791 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399402\n",
      "  validation loss:\t\t1.394688\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 792 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387451\n",
      "  validation loss:\t\t1.394680\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 793 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394717\n",
      "  validation loss:\t\t1.394725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 794 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392553\n",
      "  validation loss:\t\t1.394713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 795 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390702\n",
      "  validation loss:\t\t1.394683\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 796 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390100\n",
      "  validation loss:\t\t1.394717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 797 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377034\n",
      "  validation loss:\t\t1.394745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 798 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407508\n",
      "  validation loss:\t\t1.394726\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 799 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400608\n",
      "  validation loss:\t\t1.394642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 800 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403526\n",
      "  validation loss:\t\t1.394516\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 801 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405690\n",
      "  validation loss:\t\t1.394486\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 802 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409544\n",
      "  validation loss:\t\t1.394332\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 803 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402212\n",
      "  validation loss:\t\t1.394093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 804 of 5000 took 0.022s\n",
      "  training loss:\t\t1.418615\n",
      "  validation loss:\t\t1.393994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 805 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398630\n",
      "  validation loss:\t\t1.393965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 806 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399785\n",
      "  validation loss:\t\t1.393897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 807 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390352\n",
      "  validation loss:\t\t1.393878\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 808 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396119\n",
      "  validation loss:\t\t1.393969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 809 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395102\n",
      "  validation loss:\t\t1.393924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 810 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397771\n",
      "  validation loss:\t\t1.393877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 811 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422325\n",
      "  validation loss:\t\t1.393828\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 812 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408480\n",
      "  validation loss:\t\t1.393847\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 813 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405293\n",
      "  validation loss:\t\t1.393897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 814 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411215\n",
      "  validation loss:\t\t1.394051\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 815 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404220\n",
      "  validation loss:\t\t1.394156\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 816 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397476\n",
      "  validation loss:\t\t1.394087\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 817 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391785\n",
      "  validation loss:\t\t1.393928\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 818 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407022\n",
      "  validation loss:\t\t1.393936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 819 of 5000 took 0.023s\n",
      "  training loss:\t\t1.416042\n",
      "  validation loss:\t\t1.393888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 820 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394724\n",
      "  validation loss:\t\t1.393810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 821 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395571\n",
      "  validation loss:\t\t1.393955\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 822 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411323\n",
      "  validation loss:\t\t1.393956\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 823 of 5000 took 0.023s\n",
      "  training loss:\t\t1.410746\n",
      "  validation loss:\t\t1.393839\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 824 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404576\n",
      "  validation loss:\t\t1.393803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 825 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397775\n",
      "  validation loss:\t\t1.393549\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 826 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384464\n",
      "  validation loss:\t\t1.393411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 827 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407450\n",
      "  validation loss:\t\t1.393283\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 828 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377430\n",
      "  validation loss:\t\t1.393097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 829 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391408\n",
      "  validation loss:\t\t1.393005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 830 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412926\n",
      "  validation loss:\t\t1.393024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 831 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395684\n",
      "  validation loss:\t\t1.393064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 832 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397041\n",
      "  validation loss:\t\t1.393221\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 833 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391222\n",
      "  validation loss:\t\t1.393142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 834 of 5000 took 0.022s\n",
      "  training loss:\t\t1.420768\n",
      "  validation loss:\t\t1.393040\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 835 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400136\n",
      "  validation loss:\t\t1.392954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 836 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387954\n",
      "  validation loss:\t\t1.392832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 837 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394319\n",
      "  validation loss:\t\t1.392769\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 838 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391361\n",
      "  validation loss:\t\t1.392677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 839 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411427\n",
      "  validation loss:\t\t1.392549\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 840 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392361\n",
      "  validation loss:\t\t1.392508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 841 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387705\n",
      "  validation loss:\t\t1.392471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 842 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395548\n",
      "  validation loss:\t\t1.392471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 843 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395004\n",
      "  validation loss:\t\t1.392500\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 844 of 5000 took 0.025s\n",
      "  training loss:\t\t1.399485\n",
      "  validation loss:\t\t1.392656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 845 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387001\n",
      "  validation loss:\t\t1.392849\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 846 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396751\n",
      "  validation loss:\t\t1.393023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 847 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407956\n",
      "  validation loss:\t\t1.393188\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 848 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402689\n",
      "  validation loss:\t\t1.393157\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 849 of 5000 took 0.021s\n",
      "  training loss:\t\t1.414795\n",
      "  validation loss:\t\t1.393039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 850 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398513\n",
      "  validation loss:\t\t1.392897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 851 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394532\n",
      "  validation loss:\t\t1.392706\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 852 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385891\n",
      "  validation loss:\t\t1.392499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 853 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390951\n",
      "  validation loss:\t\t1.392321\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 854 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400566\n",
      "  validation loss:\t\t1.392244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 855 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405956\n",
      "  validation loss:\t\t1.392127\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 856 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397046\n",
      "  validation loss:\t\t1.392070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 857 of 5000 took 0.021s\n",
      "  training loss:\t\t1.402238\n",
      "  validation loss:\t\t1.392017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 858 of 5000 took 0.022s\n",
      "  training loss:\t\t1.419874\n",
      "  validation loss:\t\t1.391995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 859 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391365\n",
      "  validation loss:\t\t1.391970\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 860 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401452\n",
      "  validation loss:\t\t1.391936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 861 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398276\n",
      "  validation loss:\t\t1.391948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 862 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406725\n",
      "  validation loss:\t\t1.391983\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 863 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379301\n",
      "  validation loss:\t\t1.391989\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 864 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389375\n",
      "  validation loss:\t\t1.392104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 865 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399992\n",
      "  validation loss:\t\t1.392210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 866 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382325\n",
      "  validation loss:\t\t1.392351\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 867 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387293\n",
      "  validation loss:\t\t1.392623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 868 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389950\n",
      "  validation loss:\t\t1.392679\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 869 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405619\n",
      "  validation loss:\t\t1.392450\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 870 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397326\n",
      "  validation loss:\t\t1.392336\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 871 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405016\n",
      "  validation loss:\t\t1.392195\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 872 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398964\n",
      "  validation loss:\t\t1.392006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 873 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394092\n",
      "  validation loss:\t\t1.391821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 874 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410508\n",
      "  validation loss:\t\t1.391733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 875 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388821\n",
      "  validation loss:\t\t1.391635\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 876 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393719\n",
      "  validation loss:\t\t1.391542\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 877 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382203\n",
      "  validation loss:\t\t1.391499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 878 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381578\n",
      "  validation loss:\t\t1.391480\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 879 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405495\n",
      "  validation loss:\t\t1.391536\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 880 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391792\n",
      "  validation loss:\t\t1.391664\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 881 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393334\n",
      "  validation loss:\t\t1.391815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 882 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385515\n",
      "  validation loss:\t\t1.391934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 883 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382741\n",
      "  validation loss:\t\t1.391890\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 884 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405861\n",
      "  validation loss:\t\t1.391914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 885 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388097\n",
      "  validation loss:\t\t1.391924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 886 of 5000 took 0.021s\n",
      "  training loss:\t\t1.402506\n",
      "  validation loss:\t\t1.392035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 887 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404006\n",
      "  validation loss:\t\t1.391850\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 888 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402721\n",
      "  validation loss:\t\t1.391581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 889 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385716\n",
      "  validation loss:\t\t1.391378\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 890 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387039\n",
      "  validation loss:\t\t1.391320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 891 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387263\n",
      "  validation loss:\t\t1.391332\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 892 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391677\n",
      "  validation loss:\t\t1.391385\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 893 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422064\n",
      "  validation loss:\t\t1.391483\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 894 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388685\n",
      "  validation loss:\t\t1.391600\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 895 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395860\n",
      "  validation loss:\t\t1.391764\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 896 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384237\n",
      "  validation loss:\t\t1.391883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 897 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406334\n",
      "  validation loss:\t\t1.391890\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 898 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394301\n",
      "  validation loss:\t\t1.391789\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 899 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399547\n",
      "  validation loss:\t\t1.391839\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 900 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411153\n",
      "  validation loss:\t\t1.391736\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 901 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399441\n",
      "  validation loss:\t\t1.391668\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 902 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407385\n",
      "  validation loss:\t\t1.391435\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 903 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399376\n",
      "  validation loss:\t\t1.391295\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 904 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380293\n",
      "  validation loss:\t\t1.391273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 905 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394562\n",
      "  validation loss:\t\t1.391291\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 906 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405242\n",
      "  validation loss:\t\t1.391295\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 907 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379943\n",
      "  validation loss:\t\t1.391257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 908 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384498\n",
      "  validation loss:\t\t1.391265\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 909 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393600\n",
      "  validation loss:\t\t1.391187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 910 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413043\n",
      "  validation loss:\t\t1.391210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 911 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399185\n",
      "  validation loss:\t\t1.391240\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 912 of 5000 took 0.021s\n",
      "  training loss:\t\t1.404467\n",
      "  validation loss:\t\t1.391138\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 913 of 5000 took 0.021s\n",
      "  training loss:\t\t1.420051\n",
      "  validation loss:\t\t1.391014\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 914 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396797\n",
      "  validation loss:\t\t1.390978\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 915 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383145\n",
      "  validation loss:\t\t1.390888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 916 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394329\n",
      "  validation loss:\t\t1.390833\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 917 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394561\n",
      "  validation loss:\t\t1.390772\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 918 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389226\n",
      "  validation loss:\t\t1.390795\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 919 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389965\n",
      "  validation loss:\t\t1.390804\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 920 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406716\n",
      "  validation loss:\t\t1.390839\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 921 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396174\n",
      "  validation loss:\t\t1.390734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 922 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400831\n",
      "  validation loss:\t\t1.390664\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 923 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399877\n",
      "  validation loss:\t\t1.390635\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 924 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398787\n",
      "  validation loss:\t\t1.390640\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 925 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380331\n",
      "  validation loss:\t\t1.390595\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 926 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398836\n",
      "  validation loss:\t\t1.390638\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 927 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399560\n",
      "  validation loss:\t\t1.390656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 928 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396848\n",
      "  validation loss:\t\t1.390721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 929 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397675\n",
      "  validation loss:\t\t1.390730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 930 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388101\n",
      "  validation loss:\t\t1.390748\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 931 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408525\n",
      "  validation loss:\t\t1.390856\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 932 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402180\n",
      "  validation loss:\t\t1.391081\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 933 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399080\n",
      "  validation loss:\t\t1.391093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 934 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403132\n",
      "  validation loss:\t\t1.391242\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 935 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390870\n",
      "  validation loss:\t\t1.391164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 936 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383582\n",
      "  validation loss:\t\t1.391088\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 937 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391044\n",
      "  validation loss:\t\t1.390929\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 938 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396159\n",
      "  validation loss:\t\t1.390812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 939 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397779\n",
      "  validation loss:\t\t1.390647\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 940 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407655\n",
      "  validation loss:\t\t1.390459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 941 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396781\n",
      "  validation loss:\t\t1.390400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 942 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376985\n",
      "  validation loss:\t\t1.390359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 943 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394198\n",
      "  validation loss:\t\t1.390369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 944 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398178\n",
      "  validation loss:\t\t1.390388\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 945 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391987\n",
      "  validation loss:\t\t1.390380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 946 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378072\n",
      "  validation loss:\t\t1.390325\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 947 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390037\n",
      "  validation loss:\t\t1.390295\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 948 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380387\n",
      "  validation loss:\t\t1.390289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 949 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388366\n",
      "  validation loss:\t\t1.390245\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 950 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389048\n",
      "  validation loss:\t\t1.390295\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 951 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386672\n",
      "  validation loss:\t\t1.390383\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 952 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378630\n",
      "  validation loss:\t\t1.390437\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 953 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391252\n",
      "  validation loss:\t\t1.390364\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 954 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397463\n",
      "  validation loss:\t\t1.390290\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 955 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377478\n",
      "  validation loss:\t\t1.390266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 956 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405505\n",
      "  validation loss:\t\t1.390266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 957 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389020\n",
      "  validation loss:\t\t1.390262\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 958 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395683\n",
      "  validation loss:\t\t1.390262\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 959 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379064\n",
      "  validation loss:\t\t1.390293\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 960 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388827\n",
      "  validation loss:\t\t1.390381\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 961 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396239\n",
      "  validation loss:\t\t1.390458\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 962 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381045\n",
      "  validation loss:\t\t1.390404\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 963 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401749\n",
      "  validation loss:\t\t1.390267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 964 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404266\n",
      "  validation loss:\t\t1.390228\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 965 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400494\n",
      "  validation loss:\t\t1.390196\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 966 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388683\n",
      "  validation loss:\t\t1.390165\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 967 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399669\n",
      "  validation loss:\t\t1.390184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 968 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386217\n",
      "  validation loss:\t\t1.390185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 969 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401586\n",
      "  validation loss:\t\t1.390206\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 970 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389170\n",
      "  validation loss:\t\t1.390164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 971 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389550\n",
      "  validation loss:\t\t1.390139\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 972 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393850\n",
      "  validation loss:\t\t1.390177\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 973 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390095\n",
      "  validation loss:\t\t1.390420\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 974 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397165\n",
      "  validation loss:\t\t1.390413\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 975 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406483\n",
      "  validation loss:\t\t1.390465\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 976 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396944\n",
      "  validation loss:\t\t1.390423\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 977 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385465\n",
      "  validation loss:\t\t1.390540\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 978 of 5000 took 0.024s\n",
      "  training loss:\t\t1.374576\n",
      "  validation loss:\t\t1.390519\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 979 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386413\n",
      "  validation loss:\t\t1.390686\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 980 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390262\n",
      "  validation loss:\t\t1.390958\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 981 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395355\n",
      "  validation loss:\t\t1.391253\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 982 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393173\n",
      "  validation loss:\t\t1.391747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 983 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413609\n",
      "  validation loss:\t\t1.391480\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 984 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393975\n",
      "  validation loss:\t\t1.391470\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 985 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401283\n",
      "  validation loss:\t\t1.391168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 986 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395722\n",
      "  validation loss:\t\t1.391059\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 987 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412812\n",
      "  validation loss:\t\t1.390832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 988 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384969\n",
      "  validation loss:\t\t1.390787\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 989 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391620\n",
      "  validation loss:\t\t1.390721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 990 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402060\n",
      "  validation loss:\t\t1.390435\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 991 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399097\n",
      "  validation loss:\t\t1.390387\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 992 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386899\n",
      "  validation loss:\t\t1.390527\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 993 of 5000 took 0.024s\n",
      "  training loss:\t\t1.405786\n",
      "  validation loss:\t\t1.390658\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 994 of 5000 took 0.023s\n",
      "  training loss:\t\t1.415118\n",
      "  validation loss:\t\t1.390717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 995 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393536\n",
      "  validation loss:\t\t1.390444\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 996 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411569\n",
      "  validation loss:\t\t1.390208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 997 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389677\n",
      "  validation loss:\t\t1.389928\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 998 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397908\n",
      "  validation loss:\t\t1.389745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 999 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393662\n",
      "  validation loss:\t\t1.389649\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1000 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387198\n",
      "  validation loss:\t\t1.389656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1001 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407211\n",
      "  validation loss:\t\t1.389720\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1002 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396611\n",
      "  validation loss:\t\t1.389688\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1003 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384322\n",
      "  validation loss:\t\t1.389709\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1004 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398304\n",
      "  validation loss:\t\t1.389729\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1005 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385486\n",
      "  validation loss:\t\t1.389901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1006 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387727\n",
      "  validation loss:\t\t1.390015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1007 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404009\n",
      "  validation loss:\t\t1.390202\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1008 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400658\n",
      "  validation loss:\t\t1.390119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1009 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397147\n",
      "  validation loss:\t\t1.389976\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1010 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399573\n",
      "  validation loss:\t\t1.389832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1011 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386393\n",
      "  validation loss:\t\t1.389820\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1012 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394804\n",
      "  validation loss:\t\t1.389747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1013 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395336\n",
      "  validation loss:\t\t1.389673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1014 of 5000 took 0.024s\n",
      "  training loss:\t\t1.405358\n",
      "  validation loss:\t\t1.389561\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1015 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396089\n",
      "  validation loss:\t\t1.389552\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1016 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411924\n",
      "  validation loss:\t\t1.389583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1017 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407660\n",
      "  validation loss:\t\t1.389572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1018 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397333\n",
      "  validation loss:\t\t1.389524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1019 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383342\n",
      "  validation loss:\t\t1.389504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1020 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385947\n",
      "  validation loss:\t\t1.389502\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1021 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394947\n",
      "  validation loss:\t\t1.389578\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1022 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403139\n",
      "  validation loss:\t\t1.389615\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1023 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394102\n",
      "  validation loss:\t\t1.389621\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1024 of 5000 took 0.024s\n",
      "  training loss:\t\t1.368062\n",
      "  validation loss:\t\t1.389763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1025 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395790\n",
      "  validation loss:\t\t1.389946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1026 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393704\n",
      "  validation loss:\t\t1.390093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1027 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390738\n",
      "  validation loss:\t\t1.390442\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1028 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400472\n",
      "  validation loss:\t\t1.390682\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1029 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410724\n",
      "  validation loss:\t\t1.390598\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1030 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392413\n",
      "  validation loss:\t\t1.390458\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1031 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380315\n",
      "  validation loss:\t\t1.390256\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1032 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404811\n",
      "  validation loss:\t\t1.390008\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1033 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398953\n",
      "  validation loss:\t\t1.390006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1034 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379883\n",
      "  validation loss:\t\t1.390024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1035 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400486\n",
      "  validation loss:\t\t1.390039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1036 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398839\n",
      "  validation loss:\t\t1.389942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1037 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379202\n",
      "  validation loss:\t\t1.389812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1038 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407887\n",
      "  validation loss:\t\t1.389717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1039 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406117\n",
      "  validation loss:\t\t1.389844\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1040 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392509\n",
      "  validation loss:\t\t1.389931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1041 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401649\n",
      "  validation loss:\t\t1.390068\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1042 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396729\n",
      "  validation loss:\t\t1.390204\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1043 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391941\n",
      "  validation loss:\t\t1.390270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1044 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390040\n",
      "  validation loss:\t\t1.390162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1045 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405731\n",
      "  validation loss:\t\t1.390235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1046 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390218\n",
      "  validation loss:\t\t1.390468\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1047 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384460\n",
      "  validation loss:\t\t1.390693\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1048 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411164\n",
      "  validation loss:\t\t1.390815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1049 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384078\n",
      "  validation loss:\t\t1.390660\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1050 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392232\n",
      "  validation loss:\t\t1.390726\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1051 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387745\n",
      "  validation loss:\t\t1.391008\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1052 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384523\n",
      "  validation loss:\t\t1.390731\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1053 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401609\n",
      "  validation loss:\t\t1.390713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1054 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391620\n",
      "  validation loss:\t\t1.390458\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1055 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411943\n",
      "  validation loss:\t\t1.390209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1056 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391258\n",
      "  validation loss:\t\t1.390004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1057 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387495\n",
      "  validation loss:\t\t1.389771\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1058 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407923\n",
      "  validation loss:\t\t1.389557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1059 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397095\n",
      "  validation loss:\t\t1.389411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1060 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383215\n",
      "  validation loss:\t\t1.389219\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1061 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387935\n",
      "  validation loss:\t\t1.389137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1062 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393216\n",
      "  validation loss:\t\t1.389169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1063 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373970\n",
      "  validation loss:\t\t1.389246\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1064 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409837\n",
      "  validation loss:\t\t1.389360\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1065 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395625\n",
      "  validation loss:\t\t1.389408\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1066 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416785\n",
      "  validation loss:\t\t1.389479\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1067 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388563\n",
      "  validation loss:\t\t1.389601\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1068 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397435\n",
      "  validation loss:\t\t1.389564\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1069 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399873\n",
      "  validation loss:\t\t1.389678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1070 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389266\n",
      "  validation loss:\t\t1.389674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1071 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398777\n",
      "  validation loss:\t\t1.389695\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1072 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397126\n",
      "  validation loss:\t\t1.389718\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1073 of 5000 took 0.021s\n",
      "  training loss:\t\t1.375958\n",
      "  validation loss:\t\t1.389900\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1074 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399094\n",
      "  validation loss:\t\t1.389845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1075 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388682\n",
      "  validation loss:\t\t1.389761\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1076 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394734\n",
      "  validation loss:\t\t1.389508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1077 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395063\n",
      "  validation loss:\t\t1.389379\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1078 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377131\n",
      "  validation loss:\t\t1.389297\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1079 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391180\n",
      "  validation loss:\t\t1.389275\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1080 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386025\n",
      "  validation loss:\t\t1.389187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1081 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383107\n",
      "  validation loss:\t\t1.389240\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1082 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392199\n",
      "  validation loss:\t\t1.389422\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1083 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389861\n",
      "  validation loss:\t\t1.389669\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1084 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385822\n",
      "  validation loss:\t\t1.389912\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1085 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398739\n",
      "  validation loss:\t\t1.390299\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1086 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394330\n",
      "  validation loss:\t\t1.390508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1087 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394594\n",
      "  validation loss:\t\t1.390462\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1088 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393859\n",
      "  validation loss:\t\t1.390393\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1089 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395546\n",
      "  validation loss:\t\t1.390331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1090 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386425\n",
      "  validation loss:\t\t1.390141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1091 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400009\n",
      "  validation loss:\t\t1.389818\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1092 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401399\n",
      "  validation loss:\t\t1.389603\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1093 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390769\n",
      "  validation loss:\t\t1.389310\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1094 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394066\n",
      "  validation loss:\t\t1.389170\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1095 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401024\n",
      "  validation loss:\t\t1.389093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1096 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388825\n",
      "  validation loss:\t\t1.389089\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1097 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391493\n",
      "  validation loss:\t\t1.389082\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1098 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407035\n",
      "  validation loss:\t\t1.388979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1099 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387864\n",
      "  validation loss:\t\t1.388970\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1100 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388925\n",
      "  validation loss:\t\t1.388995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1101 of 5000 took 0.021s\n",
      "  training loss:\t\t1.375514\n",
      "  validation loss:\t\t1.388998\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1102 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392444\n",
      "  validation loss:\t\t1.388994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1103 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396803\n",
      "  validation loss:\t\t1.389032\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1104 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382793\n",
      "  validation loss:\t\t1.389097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1105 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405405\n",
      "  validation loss:\t\t1.389125\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1106 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387578\n",
      "  validation loss:\t\t1.389157\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1107 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415295\n",
      "  validation loss:\t\t1.389105\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1108 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384559\n",
      "  validation loss:\t\t1.389206\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1109 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390322\n",
      "  validation loss:\t\t1.389109\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1110 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383897\n",
      "  validation loss:\t\t1.388887\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1111 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393014\n",
      "  validation loss:\t\t1.388123\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1112 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396149\n",
      "  validation loss:\t\t1.375658\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1113 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384715\n",
      "  validation loss:\t\t1.367918\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1114 of 5000 took 0.022s\n",
      "  training loss:\t\t1.372929\n",
      "  validation loss:\t\t1.362103\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1115 of 5000 took 0.024s\n",
      "  training loss:\t\t1.352520\n",
      "  validation loss:\t\t1.358117\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1116 of 5000 took 0.022s\n",
      "  training loss:\t\t1.353478\n",
      "  validation loss:\t\t1.351780\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1117 of 5000 took 0.022s\n",
      "  training loss:\t\t1.366276\n",
      "  validation loss:\t\t1.356094\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1118 of 5000 took 0.022s\n",
      "  training loss:\t\t1.351306\n",
      "  validation loss:\t\t1.340961\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1119 of 5000 took 0.022s\n",
      "  training loss:\t\t1.365968\n",
      "  validation loss:\t\t1.342666\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1120 of 5000 took 0.022s\n",
      "  training loss:\t\t1.330889\n",
      "  validation loss:\t\t1.326116\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1121 of 5000 took 0.022s\n",
      "  training loss:\t\t1.315872\n",
      "  validation loss:\t\t1.319639\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1122 of 5000 took 0.023s\n",
      "  training loss:\t\t1.319833\n",
      "  validation loss:\t\t1.313880\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1123 of 5000 took 0.022s\n",
      "  training loss:\t\t1.315398\n",
      "  validation loss:\t\t1.310308\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1124 of 5000 took 0.022s\n",
      "  training loss:\t\t1.317772\n",
      "  validation loss:\t\t1.298168\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1125 of 5000 took 0.024s\n",
      "  training loss:\t\t1.326145\n",
      "  validation loss:\t\t1.297244\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1126 of 5000 took 0.022s\n",
      "  training loss:\t\t1.314678\n",
      "  validation loss:\t\t1.281804\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1127 of 5000 took 0.022s\n",
      "  training loss:\t\t1.249948\n",
      "  validation loss:\t\t1.276667\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1128 of 5000 took 0.022s\n",
      "  training loss:\t\t1.302182\n",
      "  validation loss:\t\t1.271866\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1129 of 5000 took 0.022s\n",
      "  training loss:\t\t1.301163\n",
      "  validation loss:\t\t1.267877\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1130 of 5000 took 0.022s\n",
      "  training loss:\t\t1.264203\n",
      "  validation loss:\t\t1.248727\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1131 of 5000 took 0.022s\n",
      "  training loss:\t\t1.257080\n",
      "  validation loss:\t\t1.247328\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1132 of 5000 took 0.024s\n",
      "  training loss:\t\t1.250706\n",
      "  validation loss:\t\t1.242778\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1133 of 5000 took 0.023s\n",
      "  training loss:\t\t1.249044\n",
      "  validation loss:\t\t1.371342\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1134 of 5000 took 0.023s\n",
      "  training loss:\t\t1.331170\n",
      "  validation loss:\t\t1.584699\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1135 of 5000 took 0.024s\n",
      "  training loss:\t\t1.707651\n",
      "  validation loss:\t\t1.216832\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1136 of 5000 took 0.022s\n",
      "  training loss:\t\t1.221415\n",
      "  validation loss:\t\t1.327560\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1137 of 5000 took 0.022s\n",
      "  training loss:\t\t1.300888\n",
      "  validation loss:\t\t1.292021\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1138 of 5000 took 0.022s\n",
      "  training loss:\t\t1.308173\n",
      "  validation loss:\t\t1.200824\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1139 of 5000 took 0.022s\n",
      "  training loss:\t\t1.250951\n",
      "  validation loss:\t\t1.210572\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1140 of 5000 took 0.021s\n",
      "  training loss:\t\t1.206076\n",
      "  validation loss:\t\t1.187305\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1141 of 5000 took 0.022s\n",
      "  training loss:\t\t1.195773\n",
      "  validation loss:\t\t1.181638\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1142 of 5000 took 0.022s\n",
      "  training loss:\t\t1.164222\n",
      "  validation loss:\t\t1.169102\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1143 of 5000 took 0.022s\n",
      "  training loss:\t\t1.235094\n",
      "  validation loss:\t\t1.163016\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1144 of 5000 took 0.022s\n",
      "  training loss:\t\t1.175393\n",
      "  validation loss:\t\t1.183692\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1145 of 5000 took 0.025s\n",
      "  training loss:\t\t1.196483\n",
      "  validation loss:\t\t1.193610\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1146 of 5000 took 0.022s\n",
      "  training loss:\t\t1.205178\n",
      "  validation loss:\t\t1.165772\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1147 of 5000 took 0.022s\n",
      "  training loss:\t\t1.171699\n",
      "  validation loss:\t\t1.132766\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1148 of 5000 took 0.021s\n",
      "  training loss:\t\t1.187186\n",
      "  validation loss:\t\t1.215442\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1149 of 5000 took 0.021s\n",
      "  training loss:\t\t1.203943\n",
      "  validation loss:\t\t1.327686\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1150 of 5000 took 0.023s\n",
      "  training loss:\t\t1.367640\n",
      "  validation loss:\t\t1.263451\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1151 of 5000 took 0.022s\n",
      "  training loss:\t\t1.278638\n",
      "  validation loss:\t\t1.369997\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1152 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408597\n",
      "  validation loss:\t\t1.155987\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1153 of 5000 took 0.022s\n",
      "  training loss:\t\t1.223800\n",
      "  validation loss:\t\t1.118037\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1154 of 5000 took 0.022s\n",
      "  training loss:\t\t1.174728\n",
      "  validation loss:\t\t1.128550\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1155 of 5000 took 0.024s\n",
      "  training loss:\t\t1.115993\n",
      "  validation loss:\t\t1.125484\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1156 of 5000 took 0.023s\n",
      "  training loss:\t\t1.150089\n",
      "  validation loss:\t\t1.149334\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1157 of 5000 took 0.023s\n",
      "  training loss:\t\t1.170436\n",
      "  validation loss:\t\t1.165781\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1158 of 5000 took 0.022s\n",
      "  training loss:\t\t1.179997\n",
      "  validation loss:\t\t1.114003\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1159 of 5000 took 0.022s\n",
      "  training loss:\t\t1.156066\n",
      "  validation loss:\t\t1.118238\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1160 of 5000 took 0.022s\n",
      "  training loss:\t\t1.136777\n",
      "  validation loss:\t\t1.224611\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1161 of 5000 took 0.022s\n",
      "  training loss:\t\t1.208899\n",
      "  validation loss:\t\t1.276026\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 1162 of 5000 took 0.022s\n",
      "  training loss:\t\t1.443645\n",
      "  validation loss:\t\t1.200846\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1163 of 5000 took 0.023s\n",
      "  training loss:\t\t1.194196\n",
      "  validation loss:\t\t1.168267\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1164 of 5000 took 0.022s\n",
      "  training loss:\t\t1.150268\n",
      "  validation loss:\t\t1.074863\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1165 of 5000 took 0.024s\n",
      "  training loss:\t\t1.059851\n",
      "  validation loss:\t\t1.099154\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1166 of 5000 took 0.023s\n",
      "  training loss:\t\t1.105274\n",
      "  validation loss:\t\t1.068820\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1167 of 5000 took 0.023s\n",
      "  training loss:\t\t1.075589\n",
      "  validation loss:\t\t1.067030\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1168 of 5000 took 0.022s\n",
      "  training loss:\t\t1.056782\n",
      "  validation loss:\t\t1.060122\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1169 of 5000 took 0.022s\n",
      "  training loss:\t\t1.119180\n",
      "  validation loss:\t\t1.056291\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1170 of 5000 took 0.023s\n",
      "  training loss:\t\t1.065268\n",
      "  validation loss:\t\t1.060645\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1171 of 5000 took 0.023s\n",
      "  training loss:\t\t1.074902\n",
      "  validation loss:\t\t1.137465\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1172 of 5000 took 0.023s\n",
      "  training loss:\t\t1.081330\n",
      "  validation loss:\t\t1.129752\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1173 of 5000 took 0.022s\n",
      "  training loss:\t\t1.101714\n",
      "  validation loss:\t\t1.057201\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1174 of 5000 took 0.024s\n",
      "  training loss:\t\t0.992523\n",
      "  validation loss:\t\t1.059374\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1175 of 5000 took 0.022s\n",
      "  training loss:\t\t1.128177\n",
      "  validation loss:\t\t1.076574\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1176 of 5000 took 0.022s\n",
      "  training loss:\t\t1.054380\n",
      "  validation loss:\t\t1.065377\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1177 of 5000 took 0.023s\n",
      "  training loss:\t\t1.100849\n",
      "  validation loss:\t\t1.084963\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1178 of 5000 took 0.023s\n",
      "  training loss:\t\t1.064373\n",
      "  validation loss:\t\t1.107339\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1179 of 5000 took 0.023s\n",
      "  training loss:\t\t1.126150\n",
      "  validation loss:\t\t1.048432\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1180 of 5000 took 0.022s\n",
      "  training loss:\t\t1.036410\n",
      "  validation loss:\t\t1.113791\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1181 of 5000 took 0.024s\n",
      "  training loss:\t\t1.073339\n",
      "  validation loss:\t\t1.074912\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1182 of 5000 took 0.023s\n",
      "  training loss:\t\t1.007457\n",
      "  validation loss:\t\t1.227749\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1183 of 5000 took 0.024s\n",
      "  training loss:\t\t1.183132\n",
      "  validation loss:\t\t1.200035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1184 of 5000 took 0.022s\n",
      "  training loss:\t\t1.187590\n",
      "  validation loss:\t\t1.626845\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1185 of 5000 took 0.022s\n",
      "  training loss:\t\t1.562792\n",
      "  validation loss:\t\t1.080695\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1186 of 5000 took 0.021s\n",
      "  training loss:\t\t1.109870\n",
      "  validation loss:\t\t1.079401\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1187 of 5000 took 0.022s\n",
      "  training loss:\t\t1.014912\n",
      "  validation loss:\t\t1.061653\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1188 of 5000 took 0.022s\n",
      "  training loss:\t\t1.130474\n",
      "  validation loss:\t\t1.105539\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1189 of 5000 took 0.023s\n",
      "  training loss:\t\t1.039250\n",
      "  validation loss:\t\t1.045442\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1190 of 5000 took 0.023s\n",
      "  training loss:\t\t1.117501\n",
      "  validation loss:\t\t1.032548\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1191 of 5000 took 0.023s\n",
      "  training loss:\t\t1.087503\n",
      "  validation loss:\t\t1.014529\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1192 of 5000 took 0.024s\n",
      "  training loss:\t\t1.148595\n",
      "  validation loss:\t\t1.006434\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1193 of 5000 took 0.022s\n",
      "  training loss:\t\t1.042270\n",
      "  validation loss:\t\t1.005922\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1194 of 5000 took 0.022s\n",
      "  training loss:\t\t0.966035\n",
      "  validation loss:\t\t1.001539\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1195 of 5000 took 0.022s\n",
      "  training loss:\t\t1.010710\n",
      "  validation loss:\t\t1.001287\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1196 of 5000 took 0.022s\n",
      "  training loss:\t\t0.992990\n",
      "  validation loss:\t\t1.003157\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1197 of 5000 took 0.022s\n",
      "  training loss:\t\t1.073104\n",
      "  validation loss:\t\t1.082064\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1198 of 5000 took 0.022s\n",
      "  training loss:\t\t1.086573\n",
      "  validation loss:\t\t1.191141\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1199 of 5000 took 0.023s\n",
      "  training loss:\t\t1.143000\n",
      "  validation loss:\t\t1.615621\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1200 of 5000 took 0.022s\n",
      "  training loss:\t\t1.439920\n",
      "  validation loss:\t\t1.895787\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1201 of 5000 took 0.024s\n",
      "  training loss:\t\t1.658955\n",
      "  validation loss:\t\t1.061126\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1202 of 5000 took 0.023s\n",
      "  training loss:\t\t0.992664\n",
      "  validation loss:\t\t1.074840\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1203 of 5000 took 0.022s\n",
      "  training loss:\t\t1.132110\n",
      "  validation loss:\t\t1.067795\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1204 of 5000 took 0.022s\n",
      "  training loss:\t\t1.049233\n",
      "  validation loss:\t\t1.060139\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1205 of 5000 took 0.022s\n",
      "  training loss:\t\t0.999900\n",
      "  validation loss:\t\t1.041584\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1206 of 5000 took 0.023s\n",
      "  training loss:\t\t1.021504\n",
      "  validation loss:\t\t1.015649\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1207 of 5000 took 0.023s\n",
      "  training loss:\t\t0.984608\n",
      "  validation loss:\t\t1.035369\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1208 of 5000 took 0.022s\n",
      "  training loss:\t\t1.033499\n",
      "  validation loss:\t\t1.018284\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1209 of 5000 took 0.023s\n",
      "  training loss:\t\t0.953372\n",
      "  validation loss:\t\t1.007494\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1210 of 5000 took 0.023s\n",
      "  training loss:\t\t1.058812\n",
      "  validation loss:\t\t1.012450\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1211 of 5000 took 0.023s\n",
      "  training loss:\t\t0.990771\n",
      "  validation loss:\t\t1.075685\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1212 of 5000 took 0.022s\n",
      "  training loss:\t\t1.132377\n",
      "  validation loss:\t\t1.213489\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 1213 of 5000 took 0.022s\n",
      "  training loss:\t\t1.161398\n",
      "  validation loss:\t\t1.938074\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1214 of 5000 took 0.022s\n",
      "  training loss:\t\t1.746377\n",
      "  validation loss:\t\t1.365040\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1215 of 5000 took 0.021s\n",
      "  training loss:\t\t1.278242\n",
      "  validation loss:\t\t1.643502\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1216 of 5000 took 0.021s\n",
      "  training loss:\t\t1.416131\n",
      "  validation loss:\t\t1.065265\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1217 of 5000 took 0.022s\n",
      "  training loss:\t\t1.030070\n",
      "  validation loss:\t\t1.114864\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1218 of 5000 took 0.023s\n",
      "  training loss:\t\t0.998209\n",
      "  validation loss:\t\t1.064718\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1219 of 5000 took 0.022s\n",
      "  training loss:\t\t1.026424\n",
      "  validation loss:\t\t1.026525\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1220 of 5000 took 0.023s\n",
      "  training loss:\t\t1.027583\n",
      "  validation loss:\t\t1.012864\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1221 of 5000 took 0.023s\n",
      "  training loss:\t\t1.024637\n",
      "  validation loss:\t\t1.005035\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1222 of 5000 took 0.022s\n",
      "  training loss:\t\t0.993844\n",
      "  validation loss:\t\t0.999267\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1223 of 5000 took 0.021s\n",
      "  training loss:\t\t1.127547\n",
      "  validation loss:\t\t0.997876\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1224 of 5000 took 0.021s\n",
      "  training loss:\t\t0.980253\n",
      "  validation loss:\t\t1.007717\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1225 of 5000 took 0.021s\n",
      "  training loss:\t\t1.017554\n",
      "  validation loss:\t\t1.006145\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1226 of 5000 took 0.022s\n",
      "  training loss:\t\t0.973551\n",
      "  validation loss:\t\t1.002603\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1227 of 5000 took 0.023s\n",
      "  training loss:\t\t0.998088\n",
      "  validation loss:\t\t1.037532\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1228 of 5000 took 0.023s\n",
      "  training loss:\t\t0.987415\n",
      "  validation loss:\t\t1.030626\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1229 of 5000 took 0.022s\n",
      "  training loss:\t\t0.962299\n",
      "  validation loss:\t\t1.032333\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1230 of 5000 took 0.023s\n",
      "  training loss:\t\t1.024913\n",
      "  validation loss:\t\t1.042776\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1231 of 5000 took 0.024s\n",
      "  training loss:\t\t1.085428\n",
      "  validation loss:\t\t0.990144\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1232 of 5000 took 0.022s\n",
      "  training loss:\t\t0.982289\n",
      "  validation loss:\t\t0.995558\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1233 of 5000 took 0.022s\n",
      "  training loss:\t\t1.052031\n",
      "  validation loss:\t\t0.990559\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1234 of 5000 took 0.021s\n",
      "  training loss:\t\t0.970648\n",
      "  validation loss:\t\t0.991442\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1235 of 5000 took 0.022s\n",
      "  training loss:\t\t0.979883\n",
      "  validation loss:\t\t0.991659\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1236 of 5000 took 0.022s\n",
      "  training loss:\t\t1.002234\n",
      "  validation loss:\t\t0.991687\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1237 of 5000 took 0.023s\n",
      "  training loss:\t\t0.915075\n",
      "  validation loss:\t\t1.003322\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1238 of 5000 took 0.022s\n",
      "  training loss:\t\t0.946975\n",
      "  validation loss:\t\t0.985858\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1239 of 5000 took 0.022s\n",
      "  training loss:\t\t1.003684\n",
      "  validation loss:\t\t1.004546\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1240 of 5000 took 0.023s\n",
      "  training loss:\t\t0.917725\n",
      "  validation loss:\t\t1.063404\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1241 of 5000 took 0.024s\n",
      "  training loss:\t\t1.098841\n",
      "  validation loss:\t\t1.255130\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1242 of 5000 took 0.023s\n",
      "  training loss:\t\t1.126669\n",
      "  validation loss:\t\t1.588629\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1243 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381276\n",
      "  validation loss:\t\t1.752760\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1244 of 5000 took 0.023s\n",
      "  training loss:\t\t1.613468\n",
      "  validation loss:\t\t1.745191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1245 of 5000 took 0.023s\n",
      "  training loss:\t\t1.559814\n",
      "  validation loss:\t\t1.055005\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1246 of 5000 took 0.024s\n",
      "  training loss:\t\t1.060561\n",
      "  validation loss:\t\t1.047616\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1247 of 5000 took 0.022s\n",
      "  training loss:\t\t1.020518\n",
      "  validation loss:\t\t1.053832\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1248 of 5000 took 0.022s\n",
      "  training loss:\t\t1.003167\n",
      "  validation loss:\t\t1.017275\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1249 of 5000 took 0.023s\n",
      "  training loss:\t\t0.971852\n",
      "  validation loss:\t\t1.015932\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1250 of 5000 took 0.023s\n",
      "  training loss:\t\t0.866783\n",
      "  validation loss:\t\t1.018520\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1251 of 5000 took 0.023s\n",
      "  training loss:\t\t0.976287\n",
      "  validation loss:\t\t1.065347\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1252 of 5000 took 0.022s\n",
      "  training loss:\t\t0.957247\n",
      "  validation loss:\t\t1.030872\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1253 of 5000 took 0.022s\n",
      "  training loss:\t\t1.029418\n",
      "  validation loss:\t\t1.124555\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1254 of 5000 took 0.022s\n",
      "  training loss:\t\t1.057364\n",
      "  validation loss:\t\t1.241915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1255 of 5000 took 0.021s\n",
      "  training loss:\t\t1.077205\n",
      "  validation loss:\t\t1.873224\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1256 of 5000 took 0.022s\n",
      "  training loss:\t\t1.757010\n",
      "  validation loss:\t\t1.739046\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1257 of 5000 took 0.022s\n",
      "  training loss:\t\t1.673574\n",
      "  validation loss:\t\t1.129063\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1258 of 5000 took 0.022s\n",
      "  training loss:\t\t0.992024\n",
      "  validation loss:\t\t1.571707\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1259 of 5000 took 0.022s\n",
      "  training loss:\t\t1.280898\n",
      "  validation loss:\t\t1.135083\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1260 of 5000 took 0.024s\n",
      "  training loss:\t\t1.055601\n",
      "  validation loss:\t\t1.126051\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1261 of 5000 took 0.022s\n",
      "  training loss:\t\t0.982011\n",
      "  validation loss:\t\t1.039476\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1262 of 5000 took 0.022s\n",
      "  training loss:\t\t0.975244\n",
      "  validation loss:\t\t1.076029\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1263 of 5000 took 0.023s\n",
      "  training loss:\t\t1.002674\n",
      "  validation loss:\t\t1.056664\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1264 of 5000 took 0.022s\n",
      "  training loss:\t\t0.988045\n",
      "  validation loss:\t\t1.086541\n",
      "  validation accuracy:\t\t47.50 %\n",
      "Epoch 1265 of 5000 took 0.023s\n",
      "  training loss:\t\t0.986395\n",
      "  validation loss:\t\t1.009359\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1266 of 5000 took 0.023s\n",
      "  training loss:\t\t0.888418\n",
      "  validation loss:\t\t1.015534\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1267 of 5000 took 0.022s\n",
      "  training loss:\t\t1.028845\n",
      "  validation loss:\t\t1.039969\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1268 of 5000 took 0.022s\n",
      "  training loss:\t\t0.955382\n",
      "  validation loss:\t\t1.139271\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1269 of 5000 took 0.023s\n",
      "  training loss:\t\t1.032174\n",
      "  validation loss:\t\t1.056352\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1270 of 5000 took 0.023s\n",
      "  training loss:\t\t1.095289\n",
      "  validation loss:\t\t1.062900\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1271 of 5000 took 0.023s\n",
      "  training loss:\t\t0.986527\n",
      "  validation loss:\t\t1.111854\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1272 of 5000 took 0.023s\n",
      "  training loss:\t\t1.004451\n",
      "  validation loss:\t\t1.381371\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1273 of 5000 took 0.022s\n",
      "  training loss:\t\t1.125648\n",
      "  validation loss:\t\t1.175981\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1274 of 5000 took 0.022s\n",
      "  training loss:\t\t1.093879\n",
      "  validation loss:\t\t1.256879\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1275 of 5000 took 0.023s\n",
      "  training loss:\t\t1.064140\n",
      "  validation loss:\t\t1.177060\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1276 of 5000 took 0.023s\n",
      "  training loss:\t\t1.149020\n",
      "  validation loss:\t\t1.208586\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1277 of 5000 took 0.023s\n",
      "  training loss:\t\t0.973849\n",
      "  validation loss:\t\t1.040591\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1278 of 5000 took 0.024s\n",
      "  training loss:\t\t0.958866\n",
      "  validation loss:\t\t1.014042\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1279 of 5000 took 0.023s\n",
      "  training loss:\t\t0.865084\n",
      "  validation loss:\t\t0.998709\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1280 of 5000 took 0.021s\n",
      "  training loss:\t\t0.963750\n",
      "  validation loss:\t\t0.985199\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1281 of 5000 took 0.021s\n",
      "  training loss:\t\t0.895568\n",
      "  validation loss:\t\t0.979855\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1282 of 5000 took 0.021s\n",
      "  training loss:\t\t0.890835\n",
      "  validation loss:\t\t0.955079\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1283 of 5000 took 0.022s\n",
      "  training loss:\t\t0.933970\n",
      "  validation loss:\t\t0.950253\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1284 of 5000 took 0.022s\n",
      "  training loss:\t\t0.951194\n",
      "  validation loss:\t\t0.948032\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1285 of 5000 took 0.023s\n",
      "  training loss:\t\t0.900811\n",
      "  validation loss:\t\t0.946524\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1286 of 5000 took 0.022s\n",
      "  training loss:\t\t1.033080\n",
      "  validation loss:\t\t0.973761\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1287 of 5000 took 0.022s\n",
      "  training loss:\t\t0.969789\n",
      "  validation loss:\t\t1.190284\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1288 of 5000 took 0.024s\n",
      "  training loss:\t\t1.057208\n",
      "  validation loss:\t\t1.309365\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1289 of 5000 took 0.024s\n",
      "  training loss:\t\t1.197928\n",
      "  validation loss:\t\t1.845451\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1290 of 5000 took 0.022s\n",
      "  training loss:\t\t1.580112\n",
      "  validation loss:\t\t1.512881\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1291 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409196\n",
      "  validation loss:\t\t1.663782\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1292 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393306\n",
      "  validation loss:\t\t1.410450\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1293 of 5000 took 0.022s\n",
      "  training loss:\t\t1.292359\n",
      "  validation loss:\t\t1.439435\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1294 of 5000 took 0.022s\n",
      "  training loss:\t\t1.212841\n",
      "  validation loss:\t\t1.101162\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1295 of 5000 took 0.022s\n",
      "  training loss:\t\t0.973799\n",
      "  validation loss:\t\t1.082916\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1296 of 5000 took 0.022s\n",
      "  training loss:\t\t0.913155\n",
      "  validation loss:\t\t1.031484\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1297 of 5000 took 0.022s\n",
      "  training loss:\t\t0.883020\n",
      "  validation loss:\t\t1.012160\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1298 of 5000 took 0.023s\n",
      "  training loss:\t\t0.923296\n",
      "  validation loss:\t\t0.989627\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1299 of 5000 took 0.023s\n",
      "  training loss:\t\t0.868224\n",
      "  validation loss:\t\t0.983860\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1300 of 5000 took 0.022s\n",
      "  training loss:\t\t0.943005\n",
      "  validation loss:\t\t0.960481\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1301 of 5000 took 0.022s\n",
      "  training loss:\t\t0.826268\n",
      "  validation loss:\t\t0.949752\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1302 of 5000 took 0.022s\n",
      "  training loss:\t\t0.904168\n",
      "  validation loss:\t\t0.962446\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1303 of 5000 took 0.022s\n",
      "  training loss:\t\t0.922907\n",
      "  validation loss:\t\t1.008163\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1304 of 5000 took 0.021s\n",
      "  training loss:\t\t1.006621\n",
      "  validation loss:\t\t0.997368\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1305 of 5000 took 0.023s\n",
      "  training loss:\t\t0.910666\n",
      "  validation loss:\t\t1.123708\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1306 of 5000 took 0.022s\n",
      "  training loss:\t\t0.956057\n",
      "  validation loss:\t\t1.023620\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1307 of 5000 took 0.022s\n",
      "  training loss:\t\t0.929834\n",
      "  validation loss:\t\t1.002397\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1308 of 5000 took 0.023s\n",
      "  training loss:\t\t0.954196\n",
      "  validation loss:\t\t0.965343\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1309 of 5000 took 0.024s\n",
      "  training loss:\t\t0.835141\n",
      "  validation loss:\t\t0.987888\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1310 of 5000 took 0.022s\n",
      "  training loss:\t\t0.883036\n",
      "  validation loss:\t\t0.994514\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1311 of 5000 took 0.021s\n",
      "  training loss:\t\t0.954587\n",
      "  validation loss:\t\t1.069570\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1312 of 5000 took 0.021s\n",
      "  training loss:\t\t0.922388\n",
      "  validation loss:\t\t0.956025\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1313 of 5000 took 0.022s\n",
      "  training loss:\t\t0.859197\n",
      "  validation loss:\t\t0.973732\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1314 of 5000 took 0.022s\n",
      "  training loss:\t\t0.958572\n",
      "  validation loss:\t\t0.965446\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1315 of 5000 took 0.022s\n",
      "  training loss:\t\t0.820316\n",
      "  validation loss:\t\t0.965697\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1316 of 5000 took 0.023s\n",
      "  training loss:\t\t0.950105\n",
      "  validation loss:\t\t0.956519\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1317 of 5000 took 0.023s\n",
      "  training loss:\t\t0.918401\n",
      "  validation loss:\t\t0.968210\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1318 of 5000 took 0.024s\n",
      "  training loss:\t\t0.853451\n",
      "  validation loss:\t\t1.065429\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1319 of 5000 took 0.023s\n",
      "  training loss:\t\t0.955029\n",
      "  validation loss:\t\t1.816711\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1320 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405625\n",
      "  validation loss:\t\t1.583373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1321 of 5000 took 0.022s\n",
      "  training loss:\t\t1.484443\n",
      "  validation loss:\t\t1.173499\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1322 of 5000 took 0.023s\n",
      "  training loss:\t\t0.891619\n",
      "  validation loss:\t\t1.028170\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1323 of 5000 took 0.023s\n",
      "  training loss:\t\t0.872349\n",
      "  validation loss:\t\t1.007866\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1324 of 5000 took 0.022s\n",
      "  training loss:\t\t0.796802\n",
      "  validation loss:\t\t1.038746\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1325 of 5000 took 0.022s\n",
      "  training loss:\t\t0.960343\n",
      "  validation loss:\t\t1.014662\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1326 of 5000 took 0.023s\n",
      "  training loss:\t\t1.006693\n",
      "  validation loss:\t\t1.144941\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1327 of 5000 took 0.024s\n",
      "  training loss:\t\t1.089804\n",
      "  validation loss:\t\t1.127973\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1328 of 5000 took 0.022s\n",
      "  training loss:\t\t0.961621\n",
      "  validation loss:\t\t1.276600\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1329 of 5000 took 0.022s\n",
      "  training loss:\t\t1.023843\n",
      "  validation loss:\t\t1.005008\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1330 of 5000 took 0.021s\n",
      "  training loss:\t\t0.909997\n",
      "  validation loss:\t\t1.053131\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1331 of 5000 took 0.022s\n",
      "  training loss:\t\t0.944614\n",
      "  validation loss:\t\t0.966875\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1332 of 5000 took 0.022s\n",
      "  training loss:\t\t0.818886\n",
      "  validation loss:\t\t1.017489\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1333 of 5000 took 0.022s\n",
      "  training loss:\t\t0.828605\n",
      "  validation loss:\t\t1.052775\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1334 of 5000 took 0.022s\n",
      "  training loss:\t\t0.908350\n",
      "  validation loss:\t\t1.545642\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1335 of 5000 took 0.022s\n",
      "  training loss:\t\t1.317359\n",
      "  validation loss:\t\t1.873562\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1336 of 5000 took 0.024s\n",
      "  training loss:\t\t1.762141\n",
      "  validation loss:\t\t1.009806\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1337 of 5000 took 0.022s\n",
      "  training loss:\t\t1.031901\n",
      "  validation loss:\t\t1.466531\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1338 of 5000 took 0.022s\n",
      "  training loss:\t\t1.099801\n",
      "  validation loss:\t\t1.257901\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 1339 of 5000 took 0.022s\n",
      "  training loss:\t\t1.106319\n",
      "  validation loss:\t\t1.836959\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1340 of 5000 took 0.022s\n",
      "  training loss:\t\t1.535616\n",
      "  validation loss:\t\t1.554077\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1341 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377697\n",
      "  validation loss:\t\t1.122537\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1342 of 5000 took 0.024s\n",
      "  training loss:\t\t0.919506\n",
      "  validation loss:\t\t1.069028\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1343 of 5000 took 0.022s\n",
      "  training loss:\t\t0.898240\n",
      "  validation loss:\t\t1.031715\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1344 of 5000 took 0.022s\n",
      "  training loss:\t\t0.850311\n",
      "  validation loss:\t\t0.999591\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1345 of 5000 took 0.023s\n",
      "  training loss:\t\t0.861051\n",
      "  validation loss:\t\t0.973304\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1346 of 5000 took 0.024s\n",
      "  training loss:\t\t0.881377\n",
      "  validation loss:\t\t0.986522\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1347 of 5000 took 0.022s\n",
      "  training loss:\t\t0.784858\n",
      "  validation loss:\t\t1.133736\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1348 of 5000 took 0.022s\n",
      "  training loss:\t\t1.053705\n",
      "  validation loss:\t\t1.161259\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1349 of 5000 took 0.022s\n",
      "  training loss:\t\t1.012679\n",
      "  validation loss:\t\t1.428632\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1350 of 5000 took 0.022s\n",
      "  training loss:\t\t1.159027\n",
      "  validation loss:\t\t1.126080\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1351 of 5000 took 0.022s\n",
      "  training loss:\t\t0.981407\n",
      "  validation loss:\t\t1.405336\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1352 of 5000 took 0.022s\n",
      "  training loss:\t\t1.254437\n",
      "  validation loss:\t\t1.009552\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1353 of 5000 took 0.022s\n",
      "  training loss:\t\t0.949894\n",
      "  validation loss:\t\t1.001590\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1354 of 5000 took 0.022s\n",
      "  training loss:\t\t0.913890\n",
      "  validation loss:\t\t0.990816\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1355 of 5000 took 0.023s\n",
      "  training loss:\t\t0.868730\n",
      "  validation loss:\t\t0.986781\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1356 of 5000 took 0.023s\n",
      "  training loss:\t\t0.854240\n",
      "  validation loss:\t\t1.021128\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1357 of 5000 took 0.023s\n",
      "  training loss:\t\t0.965546\n",
      "  validation loss:\t\t0.964565\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1358 of 5000 took 0.023s\n",
      "  training loss:\t\t0.866197\n",
      "  validation loss:\t\t0.956508\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1359 of 5000 took 0.022s\n",
      "  training loss:\t\t0.911641\n",
      "  validation loss:\t\t0.948099\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1360 of 5000 took 0.022s\n",
      "  training loss:\t\t0.935304\n",
      "  validation loss:\t\t0.943550\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1361 of 5000 took 0.023s\n",
      "  training loss:\t\t0.845127\n",
      "  validation loss:\t\t0.952975\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1362 of 5000 took 0.022s\n",
      "  training loss:\t\t0.867391\n",
      "  validation loss:\t\t0.977242\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1363 of 5000 took 0.023s\n",
      "  training loss:\t\t0.947378\n",
      "  validation loss:\t\t1.024174\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1364 of 5000 took 0.024s\n",
      "  training loss:\t\t0.880466\n",
      "  validation loss:\t\t1.186427\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1365 of 5000 took 0.023s\n",
      "  training loss:\t\t0.996629\n",
      "  validation loss:\t\t1.920850\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1366 of 5000 took 0.022s\n",
      "  training loss:\t\t1.634423\n",
      "  validation loss:\t\t1.735193\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1367 of 5000 took 0.022s\n",
      "  training loss:\t\t1.535134\n",
      "  validation loss:\t\t1.199753\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1368 of 5000 took 0.022s\n",
      "  training loss:\t\t1.034398\n",
      "  validation loss:\t\t1.535281\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1369 of 5000 took 0.022s\n",
      "  training loss:\t\t1.119566\n",
      "  validation loss:\t\t1.029382\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1370 of 5000 took 0.022s\n",
      "  training loss:\t\t0.845709\n",
      "  validation loss:\t\t1.015636\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1371 of 5000 took 0.023s\n",
      "  training loss:\t\t0.820481\n",
      "  validation loss:\t\t1.002052\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1372 of 5000 took 0.022s\n",
      "  training loss:\t\t0.805704\n",
      "  validation loss:\t\t0.998836\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1373 of 5000 took 0.023s\n",
      "  training loss:\t\t0.859092\n",
      "  validation loss:\t\t0.975744\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1374 of 5000 took 0.022s\n",
      "  training loss:\t\t0.824738\n",
      "  validation loss:\t\t0.965910\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1375 of 5000 took 0.024s\n",
      "  training loss:\t\t0.897050\n",
      "  validation loss:\t\t0.971076\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1376 of 5000 took 0.021s\n",
      "  training loss:\t\t0.802956\n",
      "  validation loss:\t\t0.947053\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1377 of 5000 took 0.022s\n",
      "  training loss:\t\t0.804057\n",
      "  validation loss:\t\t0.954932\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1378 of 5000 took 0.022s\n",
      "  training loss:\t\t0.777418\n",
      "  validation loss:\t\t0.949021\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1379 of 5000 took 0.022s\n",
      "  training loss:\t\t0.850868\n",
      "  validation loss:\t\t0.951745\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1380 of 5000 took 0.023s\n",
      "  training loss:\t\t0.842814\n",
      "  validation loss:\t\t0.928678\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1381 of 5000 took 0.022s\n",
      "  training loss:\t\t0.844150\n",
      "  validation loss:\t\t0.941878\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1382 of 5000 took 0.022s\n",
      "  training loss:\t\t0.702944\n",
      "  validation loss:\t\t0.923643\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1383 of 5000 took 0.021s\n",
      "  training loss:\t\t0.750601\n",
      "  validation loss:\t\t0.919900\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1384 of 5000 took 0.023s\n",
      "  training loss:\t\t0.820753\n",
      "  validation loss:\t\t0.916498\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1385 of 5000 took 0.022s\n",
      "  training loss:\t\t0.899945\n",
      "  validation loss:\t\t0.933464\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1386 of 5000 took 0.022s\n",
      "  training loss:\t\t0.833731\n",
      "  validation loss:\t\t1.016961\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1387 of 5000 took 0.022s\n",
      "  training loss:\t\t0.864539\n",
      "  validation loss:\t\t1.596683\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1388 of 5000 took 0.022s\n",
      "  training loss:\t\t1.216098\n",
      "  validation loss:\t\t1.538975\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1389 of 5000 took 0.022s\n",
      "  training loss:\t\t1.334826\n",
      "  validation loss:\t\t1.002044\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1390 of 5000 took 0.022s\n",
      "  training loss:\t\t0.811984\n",
      "  validation loss:\t\t0.990069\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1391 of 5000 took 0.023s\n",
      "  training loss:\t\t0.816733\n",
      "  validation loss:\t\t0.960478\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1392 of 5000 took 0.022s\n",
      "  training loss:\t\t0.798553\n",
      "  validation loss:\t\t0.963380\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1393 of 5000 took 0.023s\n",
      "  training loss:\t\t0.743072\n",
      "  validation loss:\t\t0.962852\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1394 of 5000 took 0.023s\n",
      "  training loss:\t\t0.814768\n",
      "  validation loss:\t\t0.980562\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1395 of 5000 took 0.022s\n",
      "  training loss:\t\t0.738137\n",
      "  validation loss:\t\t1.230510\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1396 of 5000 took 0.021s\n",
      "  training loss:\t\t1.001888\n",
      "  validation loss:\t\t1.537410\n",
      "  validation accuracy:\t\t22.50 %\n",
      "Epoch 1397 of 5000 took 0.021s\n",
      "  training loss:\t\t1.368478\n",
      "  validation loss:\t\t1.079078\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1398 of 5000 took 0.022s\n",
      "  training loss:\t\t0.820172\n",
      "  validation loss:\t\t1.051646\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1399 of 5000 took 0.022s\n",
      "  training loss:\t\t0.944502\n",
      "  validation loss:\t\t1.619298\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 1400 of 5000 took 0.022s\n",
      "  training loss:\t\t1.275841\n",
      "  validation loss:\t\t1.517622\n",
      "  validation accuracy:\t\t22.50 %\n",
      "Epoch 1401 of 5000 took 0.022s\n",
      "  training loss:\t\t1.295038\n",
      "  validation loss:\t\t1.218968\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1402 of 5000 took 0.023s\n",
      "  training loss:\t\t0.893372\n",
      "  validation loss:\t\t1.116984\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1403 of 5000 took 0.024s\n",
      "  training loss:\t\t0.903391\n",
      "  validation loss:\t\t1.343364\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1404 of 5000 took 0.023s\n",
      "  training loss:\t\t0.951476\n",
      "  validation loss:\t\t1.131402\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 1405 of 5000 took 0.021s\n",
      "  training loss:\t\t0.922452\n",
      "  validation loss:\t\t1.630968\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1406 of 5000 took 0.022s\n",
      "  training loss:\t\t1.242553\n",
      "  validation loss:\t\t1.641402\n",
      "  validation accuracy:\t\t22.50 %\n",
      "Epoch 1407 of 5000 took 0.021s\n",
      "  training loss:\t\t1.457672\n",
      "  validation loss:\t\t1.029090\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1408 of 5000 took 0.022s\n",
      "  training loss:\t\t0.848894\n",
      "  validation loss:\t\t1.219862\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1409 of 5000 took 0.022s\n",
      "  training loss:\t\t1.019606\n",
      "  validation loss:\t\t0.993663\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1410 of 5000 took 0.023s\n",
      "  training loss:\t\t0.734488\n",
      "  validation loss:\t\t0.974262\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1411 of 5000 took 0.023s\n",
      "  training loss:\t\t0.754169\n",
      "  validation loss:\t\t0.967023\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1412 of 5000 took 0.022s\n",
      "  training loss:\t\t0.744389\n",
      "  validation loss:\t\t1.131177\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1413 of 5000 took 0.023s\n",
      "  training loss:\t\t1.035598\n",
      "  validation loss:\t\t1.476671\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1414 of 5000 took 0.024s\n",
      "  training loss:\t\t1.278484\n",
      "  validation loss:\t\t0.954265\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1415 of 5000 took 0.022s\n",
      "  training loss:\t\t0.792310\n",
      "  validation loss:\t\t1.022496\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1416 of 5000 took 0.024s\n",
      "  training loss:\t\t0.796373\n",
      "  validation loss:\t\t0.990725\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1417 of 5000 took 0.023s\n",
      "  training loss:\t\t0.753990\n",
      "  validation loss:\t\t1.077671\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1418 of 5000 took 0.023s\n",
      "  training loss:\t\t0.960941\n",
      "  validation loss:\t\t1.193904\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1419 of 5000 took 0.023s\n",
      "  training loss:\t\t0.992532\n",
      "  validation loss:\t\t1.633228\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1420 of 5000 took 0.022s\n",
      "  training loss:\t\t1.252274\n",
      "  validation loss:\t\t1.396091\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1421 of 5000 took 0.022s\n",
      "  training loss:\t\t1.185471\n",
      "  validation loss:\t\t1.044601\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1422 of 5000 took 0.022s\n",
      "  training loss:\t\t0.800272\n",
      "  validation loss:\t\t0.972435\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1423 of 5000 took 0.024s\n",
      "  training loss:\t\t0.684391\n",
      "  validation loss:\t\t1.041913\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1424 of 5000 took 0.022s\n",
      "  training loss:\t\t0.821036\n",
      "  validation loss:\t\t0.966999\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1425 of 5000 took 0.022s\n",
      "  training loss:\t\t0.725095\n",
      "  validation loss:\t\t0.956360\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1426 of 5000 took 0.022s\n",
      "  training loss:\t\t0.831059\n",
      "  validation loss:\t\t0.951269\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1427 of 5000 took 0.022s\n",
      "  training loss:\t\t0.772664\n",
      "  validation loss:\t\t0.964092\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1428 of 5000 took 0.023s\n",
      "  training loss:\t\t0.741026\n",
      "  validation loss:\t\t0.942371\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1429 of 5000 took 0.022s\n",
      "  training loss:\t\t0.730542\n",
      "  validation loss:\t\t0.932231\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1430 of 5000 took 0.022s\n",
      "  training loss:\t\t0.720526\n",
      "  validation loss:\t\t0.949296\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1431 of 5000 took 0.022s\n",
      "  training loss:\t\t0.904961\n",
      "  validation loss:\t\t1.157235\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1432 of 5000 took 0.022s\n",
      "  training loss:\t\t0.981311\n",
      "  validation loss:\t\t1.780140\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1433 of 5000 took 0.024s\n",
      "  training loss:\t\t1.356061\n",
      "  validation loss:\t\t1.719732\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1434 of 5000 took 0.022s\n",
      "  training loss:\t\t1.527438\n",
      "  validation loss:\t\t1.376632\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1435 of 5000 took 0.022s\n",
      "  training loss:\t\t1.104966\n",
      "  validation loss:\t\t1.684599\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1436 of 5000 took 0.021s\n",
      "  training loss:\t\t1.291794\n",
      "  validation loss:\t\t1.384759\n",
      "  validation accuracy:\t\t26.25 %\n",
      "Epoch 1437 of 5000 took 0.021s\n",
      "  training loss:\t\t1.303748\n",
      "  validation loss:\t\t1.233823\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1438 of 5000 took 0.023s\n",
      "  training loss:\t\t0.764991\n",
      "  validation loss:\t\t1.016908\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1439 of 5000 took 0.022s\n",
      "  training loss:\t\t0.684520\n",
      "  validation loss:\t\t1.021878\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1440 of 5000 took 0.022s\n",
      "  training loss:\t\t0.657512\n",
      "  validation loss:\t\t0.994687\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1441 of 5000 took 0.022s\n",
      "  training loss:\t\t0.820336\n",
      "  validation loss:\t\t0.981268\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1442 of 5000 took 0.023s\n",
      "  training loss:\t\t0.808782\n",
      "  validation loss:\t\t0.969159\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1443 of 5000 took 0.022s\n",
      "  training loss:\t\t0.753222\n",
      "  validation loss:\t\t0.961894\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1444 of 5000 took 0.021s\n",
      "  training loss:\t\t0.768523\n",
      "  validation loss:\t\t0.949209\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1445 of 5000 took 0.021s\n",
      "  training loss:\t\t0.906557\n",
      "  validation loss:\t\t0.948284\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1446 of 5000 took 0.021s\n",
      "  training loss:\t\t0.776000\n",
      "  validation loss:\t\t0.952737\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1447 of 5000 took 0.022s\n",
      "  training loss:\t\t0.773869\n",
      "  validation loss:\t\t0.949127\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1448 of 5000 took 0.022s\n",
      "  training loss:\t\t0.811242\n",
      "  validation loss:\t\t0.936861\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1449 of 5000 took 0.022s\n",
      "  training loss:\t\t0.762438\n",
      "  validation loss:\t\t0.936465\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1450 of 5000 took 0.023s\n",
      "  training loss:\t\t0.811617\n",
      "  validation loss:\t\t0.991583\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1451 of 5000 took 0.022s\n",
      "  training loss:\t\t0.817248\n",
      "  validation loss:\t\t1.602550\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1452 of 5000 took 0.024s\n",
      "  training loss:\t\t1.158381\n",
      "  validation loss:\t\t1.621264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1453 of 5000 took 0.021s\n",
      "  training loss:\t\t1.417174\n",
      "  validation loss:\t\t1.082948\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1454 of 5000 took 0.021s\n",
      "  training loss:\t\t0.810356\n",
      "  validation loss:\t\t2.048918\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1455 of 5000 took 0.021s\n",
      "  training loss:\t\t1.641377\n",
      "  validation loss:\t\t1.883814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1456 of 5000 took 0.021s\n",
      "  training loss:\t\t1.809394\n",
      "  validation loss:\t\t1.643413\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1457 of 5000 took 0.021s\n",
      "  training loss:\t\t1.495449\n",
      "  validation loss:\t\t1.018122\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1458 of 5000 took 0.023s\n",
      "  training loss:\t\t0.723750\n",
      "  validation loss:\t\t1.354218\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1459 of 5000 took 0.022s\n",
      "  training loss:\t\t1.011700\n",
      "  validation loss:\t\t1.061867\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1460 of 5000 took 0.022s\n",
      "  training loss:\t\t0.780887\n",
      "  validation loss:\t\t1.158150\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1461 of 5000 took 0.023s\n",
      "  training loss:\t\t0.836285\n",
      "  validation loss:\t\t1.003806\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1462 of 5000 took 0.024s\n",
      "  training loss:\t\t0.821582\n",
      "  validation loss:\t\t1.002115\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1463 of 5000 took 0.022s\n",
      "  training loss:\t\t0.685333\n",
      "  validation loss:\t\t0.968822\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1464 of 5000 took 0.021s\n",
      "  training loss:\t\t0.728083\n",
      "  validation loss:\t\t0.953931\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1465 of 5000 took 0.022s\n",
      "  training loss:\t\t0.774727\n",
      "  validation loss:\t\t0.985926\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1466 of 5000 took 0.022s\n",
      "  training loss:\t\t0.745964\n",
      "  validation loss:\t\t1.026433\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1467 of 5000 took 0.023s\n",
      "  training loss:\t\t0.746629\n",
      "  validation loss:\t\t1.475811\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1468 of 5000 took 0.023s\n",
      "  training loss:\t\t1.072842\n",
      "  validation loss:\t\t1.419229\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1469 of 5000 took 0.022s\n",
      "  training loss:\t\t1.164791\n",
      "  validation loss:\t\t0.952874\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1470 of 5000 took 0.022s\n",
      "  training loss:\t\t0.760297\n",
      "  validation loss:\t\t1.053218\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1471 of 5000 took 0.022s\n",
      "  training loss:\t\t0.757573\n",
      "  validation loss:\t\t0.981238\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1472 of 5000 took 0.023s\n",
      "  training loss:\t\t0.762312\n",
      "  validation loss:\t\t1.133700\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1473 of 5000 took 0.022s\n",
      "  training loss:\t\t0.821765\n",
      "  validation loss:\t\t1.076937\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1474 of 5000 took 0.021s\n",
      "  training loss:\t\t0.791924\n",
      "  validation loss:\t\t2.076010\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1475 of 5000 took 0.021s\n",
      "  training loss:\t\t1.428377\n",
      "  validation loss:\t\t1.759052\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1476 of 5000 took 0.022s\n",
      "  training loss:\t\t1.605226\n",
      "  validation loss:\t\t1.529549\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1477 of 5000 took 0.024s\n",
      "  training loss:\t\t1.333410\n",
      "  validation loss:\t\t1.225958\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1478 of 5000 took 0.023s\n",
      "  training loss:\t\t0.910273\n",
      "  validation loss:\t\t1.146161\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1479 of 5000 took 0.022s\n",
      "  training loss:\t\t0.884723\n",
      "  validation loss:\t\t1.039877\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1480 of 5000 took 0.022s\n",
      "  training loss:\t\t0.731129\n",
      "  validation loss:\t\t1.007280\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1481 of 5000 took 0.024s\n",
      "  training loss:\t\t0.778554\n",
      "  validation loss:\t\t1.078957\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1482 of 5000 took 0.023s\n",
      "  training loss:\t\t0.813429\n",
      "  validation loss:\t\t1.015405\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1483 of 5000 took 0.023s\n",
      "  training loss:\t\t0.865870\n",
      "  validation loss:\t\t1.063384\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1484 of 5000 took 0.023s\n",
      "  training loss:\t\t0.766418\n",
      "  validation loss:\t\t1.011003\n",
      "  validation accuracy:\t\t56.25 %\n",
      "Epoch 1485 of 5000 took 0.022s\n",
      "  training loss:\t\t0.831014\n",
      "  validation loss:\t\t1.111651\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1486 of 5000 took 0.022s\n",
      "  training loss:\t\t0.792926\n",
      "  validation loss:\t\t0.999353\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1487 of 5000 took 0.023s\n",
      "  training loss:\t\t0.766660\n",
      "  validation loss:\t\t1.070439\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1488 of 5000 took 0.022s\n",
      "  training loss:\t\t0.806516\n",
      "  validation loss:\t\t1.002698\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1489 of 5000 took 0.022s\n",
      "  training loss:\t\t0.793892\n",
      "  validation loss:\t\t1.070798\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1490 of 5000 took 0.022s\n",
      "  training loss:\t\t0.868951\n",
      "  validation loss:\t\t1.034495\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1491 of 5000 took 0.024s\n",
      "  training loss:\t\t0.867228\n",
      "  validation loss:\t\t1.112761\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1492 of 5000 took 0.023s\n",
      "  training loss:\t\t0.790655\n",
      "  validation loss:\t\t1.095558\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1493 of 5000 took 0.022s\n",
      "  training loss:\t\t0.891436\n",
      "  validation loss:\t\t1.760715\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 1494 of 5000 took 0.021s\n",
      "  training loss:\t\t1.454581\n",
      "  validation loss:\t\t1.471304\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1495 of 5000 took 0.022s\n",
      "  training loss:\t\t1.256799\n",
      "  validation loss:\t\t1.092841\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1496 of 5000 took 0.023s\n",
      "  training loss:\t\t0.795773\n",
      "  validation loss:\t\t0.995804\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1497 of 5000 took 0.022s\n",
      "  training loss:\t\t0.715749\n",
      "  validation loss:\t\t0.962300\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1498 of 5000 took 0.023s\n",
      "  training loss:\t\t0.773860\n",
      "  validation loss:\t\t0.939687\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1499 of 5000 took 0.022s\n",
      "  training loss:\t\t0.700904\n",
      "  validation loss:\t\t0.936719\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1500 of 5000 took 0.024s\n",
      "  training loss:\t\t0.715193\n",
      "  validation loss:\t\t0.942177\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1501 of 5000 took 0.022s\n",
      "  training loss:\t\t0.766305\n",
      "  validation loss:\t\t1.162186\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1502 of 5000 took 0.023s\n",
      "  training loss:\t\t0.813551\n",
      "  validation loss:\t\t1.274821\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1503 of 5000 took 0.023s\n",
      "  training loss:\t\t1.109266\n",
      "  validation loss:\t\t1.403195\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1504 of 5000 took 0.022s\n",
      "  training loss:\t\t1.031385\n",
      "  validation loss:\t\t1.202750\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1505 of 5000 took 0.023s\n",
      "  training loss:\t\t0.974432\n",
      "  validation loss:\t\t1.324688\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1506 of 5000 took 0.023s\n",
      "  training loss:\t\t0.844141\n",
      "  validation loss:\t\t0.992100\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1507 of 5000 took 0.022s\n",
      "  training loss:\t\t0.727124\n",
      "  validation loss:\t\t0.969830\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1508 of 5000 took 0.024s\n",
      "  training loss:\t\t0.640754\n",
      "  validation loss:\t\t0.952917\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1509 of 5000 took 0.023s\n",
      "  training loss:\t\t0.701003\n",
      "  validation loss:\t\t0.945926\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1510 of 5000 took 0.022s\n",
      "  training loss:\t\t0.719588\n",
      "  validation loss:\t\t0.938952\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1511 of 5000 took 0.022s\n",
      "  training loss:\t\t0.775869\n",
      "  validation loss:\t\t0.940001\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1512 of 5000 took 0.022s\n",
      "  training loss:\t\t0.849929\n",
      "  validation loss:\t\t1.122289\n",
      "  validation accuracy:\t\t48.75 %\n",
      "Epoch 1513 of 5000 took 0.022s\n",
      "  training loss:\t\t0.880848\n",
      "  validation loss:\t\t1.294360\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1514 of 5000 took 0.022s\n",
      "  training loss:\t\t0.926374\n",
      "  validation loss:\t\t1.255592\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1515 of 5000 took 0.022s\n",
      "  training loss:\t\t1.019201\n",
      "  validation loss:\t\t1.287775\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1516 of 5000 took 0.022s\n",
      "  training loss:\t\t0.986960\n",
      "  validation loss:\t\t1.525918\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1517 of 5000 took 0.022s\n",
      "  training loss:\t\t1.187282\n",
      "  validation loss:\t\t1.385074\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1518 of 5000 took 0.023s\n",
      "  training loss:\t\t1.146589\n",
      "  validation loss:\t\t1.680617\n",
      "  validation accuracy:\t\t33.75 %\n",
      "Epoch 1519 of 5000 took 0.022s\n",
      "  training loss:\t\t1.281487\n",
      "  validation loss:\t\t1.621340\n",
      "  validation accuracy:\t\t23.75 %\n",
      "Epoch 1520 of 5000 took 0.021s\n",
      "  training loss:\t\t1.482935\n",
      "  validation loss:\t\t1.067929\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1521 of 5000 took 0.022s\n",
      "  training loss:\t\t0.812423\n",
      "  validation loss:\t\t1.156795\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1522 of 5000 took 0.022s\n",
      "  training loss:\t\t0.912755\n",
      "  validation loss:\t\t1.038962\n",
      "  validation accuracy:\t\t51.25 %\n",
      "Epoch 1523 of 5000 took 0.022s\n",
      "  training loss:\t\t0.842280\n",
      "  validation loss:\t\t1.109772\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1524 of 5000 took 0.022s\n",
      "  training loss:\t\t0.772591\n",
      "  validation loss:\t\t1.065337\n",
      "  validation accuracy:\t\t53.75 %\n",
      "Epoch 1525 of 5000 took 0.022s\n",
      "  training loss:\t\t0.875619\n",
      "  validation loss:\t\t1.112645\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1526 of 5000 took 0.021s\n",
      "  training loss:\t\t0.900348\n",
      "  validation loss:\t\t0.965977\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1527 of 5000 took 0.022s\n",
      "  training loss:\t\t0.751408\n",
      "  validation loss:\t\t0.960078\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1528 of 5000 took 0.024s\n",
      "  training loss:\t\t0.831982\n",
      "  validation loss:\t\t0.956963\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1529 of 5000 took 0.023s\n",
      "  training loss:\t\t0.810175\n",
      "  validation loss:\t\t0.951941\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1530 of 5000 took 0.021s\n",
      "  training loss:\t\t0.664617\n",
      "  validation loss:\t\t0.933241\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1531 of 5000 took 0.022s\n",
      "  training loss:\t\t0.756113\n",
      "  validation loss:\t\t0.937737\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1532 of 5000 took 0.023s\n",
      "  training loss:\t\t0.720291\n",
      "  validation loss:\t\t0.936295\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1533 of 5000 took 0.023s\n",
      "  training loss:\t\t0.645309\n",
      "  validation loss:\t\t0.919407\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1534 of 5000 took 0.022s\n",
      "  training loss:\t\t0.812409\n",
      "  validation loss:\t\t0.944232\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1535 of 5000 took 0.023s\n",
      "  training loss:\t\t0.699863\n",
      "  validation loss:\t\t1.158939\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1536 of 5000 took 0.023s\n",
      "  training loss:\t\t1.041025\n",
      "  validation loss:\t\t1.108661\n",
      "  validation accuracy:\t\t50.00 %\n",
      "Epoch 1537 of 5000 took 0.023s\n",
      "  training loss:\t\t0.834395\n",
      "  validation loss:\t\t1.290593\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1538 of 5000 took 0.023s\n",
      "  training loss:\t\t1.089401\n",
      "  validation loss:\t\t1.145052\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1539 of 5000 took 0.022s\n",
      "  training loss:\t\t0.921688\n",
      "  validation loss:\t\t1.292598\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1540 of 5000 took 0.022s\n",
      "  training loss:\t\t1.137548\n",
      "  validation loss:\t\t1.270085\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1541 of 5000 took 0.022s\n",
      "  training loss:\t\t1.094964\n",
      "  validation loss:\t\t1.349566\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1542 of 5000 took 0.023s\n",
      "  training loss:\t\t0.950401\n",
      "  validation loss:\t\t1.087999\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1543 of 5000 took 0.021s\n",
      "  training loss:\t\t0.867391\n",
      "  validation loss:\t\t1.147196\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1544 of 5000 took 0.022s\n",
      "  training loss:\t\t0.719884\n",
      "  validation loss:\t\t1.021525\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1545 of 5000 took 0.023s\n",
      "  training loss:\t\t0.713096\n",
      "  validation loss:\t\t1.003371\n",
      "  validation accuracy:\t\t62.50 %\n",
      "Epoch 1546 of 5000 took 0.022s\n",
      "  training loss:\t\t0.719505\n",
      "  validation loss:\t\t0.970111\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1547 of 5000 took 0.022s\n",
      "  training loss:\t\t0.622517\n",
      "  validation loss:\t\t0.955027\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1548 of 5000 took 0.021s\n",
      "  training loss:\t\t0.731474\n",
      "  validation loss:\t\t0.956096\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1549 of 5000 took 0.022s\n",
      "  training loss:\t\t0.759581\n",
      "  validation loss:\t\t1.038628\n",
      "  validation accuracy:\t\t57.50 %\n",
      "Epoch 1550 of 5000 took 0.022s\n",
      "  training loss:\t\t0.743156\n",
      "  validation loss:\t\t1.027954\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1551 of 5000 took 0.023s\n",
      "  training loss:\t\t0.773438\n",
      "  validation loss:\t\t1.273219\n",
      "  validation accuracy:\t\t40.00 %\n",
      "Epoch 1552 of 5000 took 0.022s\n",
      "  training loss:\t\t0.859628\n",
      "  validation loss:\t\t1.158644\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1553 of 5000 took 0.022s\n",
      "  training loss:\t\t1.005483\n",
      "  validation loss:\t\t1.753119\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1554 of 5000 took 0.024s\n",
      "  training loss:\t\t1.169071\n",
      "  validation loss:\t\t1.529683\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1555 of 5000 took 0.022s\n",
      "  training loss:\t\t1.276039\n",
      "  validation loss:\t\t1.027100\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1556 of 5000 took 0.021s\n",
      "  training loss:\t\t0.767201\n",
      "  validation loss:\t\t1.064148\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1557 of 5000 took 0.021s\n",
      "  training loss:\t\t0.749841\n",
      "  validation loss:\t\t0.951378\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1558 of 5000 took 0.021s\n",
      "  training loss:\t\t0.760394\n",
      "  validation loss:\t\t0.942537\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1559 of 5000 took 0.023s\n",
      "  training loss:\t\t0.732271\n",
      "  validation loss:\t\t0.931979\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1560 of 5000 took 0.022s\n",
      "  training loss:\t\t0.656984\n",
      "  validation loss:\t\t0.919948\n",
      "  validation accuracy:\t\t70.00 %\n",
      "Epoch 1561 of 5000 took 0.022s\n",
      "  training loss:\t\t0.751911\n",
      "  validation loss:\t\t0.917743\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1562 of 5000 took 0.022s\n",
      "  training loss:\t\t0.643630\n",
      "  validation loss:\t\t0.917555\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1563 of 5000 took 0.023s\n",
      "  training loss:\t\t0.724305\n",
      "  validation loss:\t\t0.990446\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1564 of 5000 took 0.022s\n",
      "  training loss:\t\t0.680572\n",
      "  validation loss:\t\t1.656254\n",
      "  validation accuracy:\t\t31.25 %\n",
      "Epoch 1565 of 5000 took 0.022s\n",
      "  training loss:\t\t1.193791\n",
      "  validation loss:\t\t1.413902\n",
      "  validation accuracy:\t\t27.50 %\n",
      "Epoch 1566 of 5000 took 0.022s\n",
      "  training loss:\t\t1.167397\n",
      "  validation loss:\t\t1.482274\n",
      "  validation accuracy:\t\t42.50 %\n",
      "Epoch 1567 of 5000 took 0.023s\n",
      "  training loss:\t\t1.018842\n",
      "  validation loss:\t\t1.416562\n",
      "  validation accuracy:\t\t30.00 %\n",
      "Epoch 1568 of 5000 took 0.022s\n",
      "  training loss:\t\t1.162720\n",
      "  validation loss:\t\t1.614868\n",
      "  validation accuracy:\t\t32.50 %\n",
      "Epoch 1569 of 5000 took 0.021s\n",
      "  training loss:\t\t1.169733\n",
      "  validation loss:\t\t1.371662\n",
      "  validation accuracy:\t\t37.50 %\n",
      "Epoch 1570 of 5000 took 0.022s\n",
      "  training loss:\t\t1.083801\n",
      "  validation loss:\t\t1.359026\n",
      "  validation accuracy:\t\t52.50 %\n",
      "Epoch 1571 of 5000 took 0.022s\n",
      "  training loss:\t\t0.855235\n",
      "  validation loss:\t\t1.124643\n",
      "  validation accuracy:\t\t55.00 %\n",
      "Epoch 1572 of 5000 took 0.024s\n",
      "  training loss:\t\t0.966152\n",
      "  validation loss:\t\t1.092714\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1573 of 5000 took 0.022s\n",
      "  training loss:\t\t0.717736\n",
      "  validation loss:\t\t1.033439\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1574 of 5000 took 0.022s\n",
      "  training loss:\t\t0.788708\n",
      "  validation loss:\t\t0.998354\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1575 of 5000 took 0.022s\n",
      "  training loss:\t\t0.717024\n",
      "  validation loss:\t\t0.963510\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1576 of 5000 took 0.021s\n",
      "  training loss:\t\t0.732677\n",
      "  validation loss:\t\t0.934138\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1577 of 5000 took 0.022s\n",
      "  training loss:\t\t0.743726\n",
      "  validation loss:\t\t0.918312\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1578 of 5000 took 0.022s\n",
      "  training loss:\t\t0.680775\n",
      "  validation loss:\t\t0.935051\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1579 of 5000 took 0.022s\n",
      "  training loss:\t\t0.673663\n",
      "  validation loss:\t\t0.936636\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1580 of 5000 took 0.022s\n",
      "  training loss:\t\t0.682253\n",
      "  validation loss:\t\t1.051866\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1581 of 5000 took 0.022s\n",
      "  training loss:\t\t0.774098\n",
      "  validation loss:\t\t1.502312\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 1582 of 5000 took 0.024s\n",
      "  training loss:\t\t1.231773\n",
      "  validation loss:\t\t1.521693\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1583 of 5000 took 0.022s\n",
      "  training loss:\t\t1.216010\n",
      "  validation loss:\t\t0.979902\n",
      "  validation accuracy:\t\t61.25 %\n",
      "Epoch 1584 of 5000 took 0.022s\n",
      "  training loss:\t\t0.701745\n",
      "  validation loss:\t\t1.023742\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1585 of 5000 took 0.023s\n",
      "  training loss:\t\t0.844314\n",
      "  validation loss:\t\t1.072446\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1586 of 5000 took 0.022s\n",
      "  training loss:\t\t0.867707\n",
      "  validation loss:\t\t1.144603\n",
      "  validation accuracy:\t\t45.00 %\n",
      "Epoch 1587 of 5000 took 0.022s\n",
      "  training loss:\t\t0.753756\n",
      "  validation loss:\t\t0.980866\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1588 of 5000 took 0.023s\n",
      "  training loss:\t\t0.793928\n",
      "  validation loss:\t\t1.295292\n",
      "  validation accuracy:\t\t38.75 %\n",
      "Epoch 1589 of 5000 took 0.022s\n",
      "  training loss:\t\t0.983692\n",
      "  validation loss:\t\t1.231931\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1590 of 5000 took 0.025s\n",
      "  training loss:\t\t0.927536\n",
      "  validation loss:\t\t1.392753\n",
      "  validation accuracy:\t\t46.25 %\n",
      "Epoch 1591 of 5000 took 0.022s\n",
      "  training loss:\t\t0.965926\n",
      "  validation loss:\t\t1.030966\n",
      "  validation accuracy:\t\t60.00 %\n",
      "Epoch 1592 of 5000 took 0.022s\n",
      "  training loss:\t\t0.743246\n",
      "  validation loss:\t\t0.996031\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1593 of 5000 took 0.022s\n",
      "  training loss:\t\t0.658816\n",
      "  validation loss:\t\t0.973274\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1594 of 5000 took 0.023s\n",
      "  training loss:\t\t0.598527\n",
      "  validation loss:\t\t0.967395\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1595 of 5000 took 0.023s\n",
      "  training loss:\t\t0.585398\n",
      "  validation loss:\t\t0.952734\n",
      "  validation accuracy:\t\t66.25 %\n",
      "Epoch 1596 of 5000 took 0.022s\n",
      "  training loss:\t\t0.627019\n",
      "  validation loss:\t\t0.938387\n",
      "  validation accuracy:\t\t63.75 %\n",
      "Epoch 1597 of 5000 took 0.022s\n",
      "  training loss:\t\t0.728526\n",
      "  validation loss:\t\t0.927225\n",
      "  validation accuracy:\t\t68.75 %\n",
      "Epoch 1598 of 5000 took 0.022s\n",
      "  training loss:\t\t0.664321\n",
      "  validation loss:\t\t0.913490\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1599 of 5000 took 0.023s\n",
      "  training loss:\t\t0.721155\n",
      "  validation loss:\t\t0.918666\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1600 of 5000 took 0.023s\n",
      "  training loss:\t\t0.664025\n",
      "  validation loss:\t\t0.915851\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1601 of 5000 took 0.023s\n",
      "  training loss:\t\t0.665026\n",
      "  validation loss:\t\t0.896322\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1602 of 5000 took 0.023s\n",
      "  training loss:\t\t0.737951\n",
      "  validation loss:\t\t0.896656\n",
      "  validation accuracy:\t\t72.50 %\n",
      "Epoch 1603 of 5000 took 0.023s\n",
      "  training loss:\t\t0.669034\n",
      "  validation loss:\t\t0.892722\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1604 of 5000 took 0.023s\n",
      "  training loss:\t\t0.761764\n",
      "  validation loss:\t\t1.013558\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1605 of 5000 took 0.022s\n",
      "  training loss:\t\t0.744433\n",
      "  validation loss:\t\t1.384010\n",
      "  validation accuracy:\t\t35.00 %\n",
      "Epoch 1606 of 5000 took 0.022s\n",
      "  training loss:\t\t0.893248\n",
      "  validation loss:\t\t1.497397\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1607 of 5000 took 0.023s\n",
      "  training loss:\t\t1.283898\n",
      "  validation loss:\t\t0.979578\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1608 of 5000 took 0.024s\n",
      "  training loss:\t\t0.736127\n",
      "  validation loss:\t\t0.938052\n",
      "  validation accuracy:\t\t73.75 %\n",
      "Epoch 1609 of 5000 took 0.023s\n",
      "  training loss:\t\t0.827888\n",
      "  validation loss:\t\t0.984861\n",
      "  validation accuracy:\t\t65.00 %\n",
      "Epoch 1610 of 5000 took 0.022s\n",
      "  training loss:\t\t0.698193\n",
      "  validation loss:\t\t0.964539\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1611 of 5000 took 0.022s\n",
      "  training loss:\t\t0.592710\n",
      "  validation loss:\t\t0.935898\n",
      "  validation accuracy:\t\t71.25 %\n",
      "Epoch 1612 of 5000 took 0.022s\n",
      "  training loss:\t\t0.571278\n",
      "  validation loss:\t\t0.941407\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 1613 of 5000 took 0.023s\n",
      "  training loss:\t\t0.725605\n",
      "  validation loss:\t\t0.947422\n",
      "  validation accuracy:\t\t75.00 %\n",
      "Epoch 1614 of 5000 took 0.022s\n",
      "  training loss:\t\t0.616029\n",
      "  validation loss:\t\t0.976280\n",
      "  validation accuracy:\t\t67.50 %\n",
      "Epoch 1615 of 5000 took 0.022s\n",
      "  training loss:\t\t0.717866\n",
      "  validation loss:\t\t1.308245\n",
      "  validation accuracy:\t\t41.25 %\n",
      "Epoch 1616 of 5000 took 0.023s\n",
      "  training loss:\t\t0.819798\n",
      "  validation loss:\t\t1.268347\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1617 of 5000 took 0.022s\n",
      "  training loss:\t\t0.837652\n",
      "  validation loss:\t\t1.489847\n",
      "  validation accuracy:\t\t36.25 %\n",
      "Epoch 1618 of 5000 took 0.022s\n",
      "  training loss:\t\t0.994100\n",
      "  validation loss:\t\t1.553644\n",
      "  validation accuracy:\t\t28.75 %\n",
      "Epoch 1619 of 5000 took 0.021s\n",
      "  training loss:\t\t1.245021\n",
      "  validation loss:\t\t1.231372\n",
      "  validation accuracy:\t\t58.75 %\n",
      "Epoch 1620 of 5000 took 0.022s\n",
      "  training loss:\t\t0.731812\n",
      "  validation loss:\t\t1.231095\n",
      "  validation accuracy:\t\t43.75 %\n",
      "Epoch 1621 of 5000 took 0.022s\n",
      "  training loss:\t\t0.870694\n",
      "  validation loss:\t\t2.275604\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1622 of 5000 took 0.021s\n",
      "  training loss:\t\t1.750043\n",
      "  validation loss:\t\t2.094259\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1623 of 5000 took 0.023s\n",
      "  training loss:\t\t2.110653\n",
      "  validation loss:\t\t2.049132\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1624 of 5000 took 0.021s\n",
      "  training loss:\t\t2.045420\n",
      "  validation loss:\t\t1.900003\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1625 of 5000 took 0.022s\n",
      "  training loss:\t\t1.948889\n",
      "  validation loss:\t\t1.795917\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1626 of 5000 took 0.022s\n",
      "  training loss:\t\t1.892851\n",
      "  validation loss:\t\t1.686861\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1627 of 5000 took 0.022s\n",
      "  training loss:\t\t1.707680\n",
      "  validation loss:\t\t1.591387\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1628 of 5000 took 0.023s\n",
      "  training loss:\t\t1.668293\n",
      "  validation loss:\t\t1.512373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1629 of 5000 took 0.023s\n",
      "  training loss:\t\t1.494465\n",
      "  validation loss:\t\t1.456818\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1630 of 5000 took 0.022s\n",
      "  training loss:\t\t1.466718\n",
      "  validation loss:\t\t1.420750\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1631 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410097\n",
      "  validation loss:\t\t1.404352\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1632 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392453\n",
      "  validation loss:\t\t1.403528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1633 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396961\n",
      "  validation loss:\t\t1.412733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1634 of 5000 took 0.021s\n",
      "  training loss:\t\t1.443760\n",
      "  validation loss:\t\t1.424927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1635 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373134\n",
      "  validation loss:\t\t1.440884\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1636 of 5000 took 0.024s\n",
      "  training loss:\t\t1.473072\n",
      "  validation loss:\t\t1.452683\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1637 of 5000 took 0.024s\n",
      "  training loss:\t\t1.496393\n",
      "  validation loss:\t\t1.459389\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1638 of 5000 took 0.022s\n",
      "  training loss:\t\t1.448213\n",
      "  validation loss:\t\t1.462877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1639 of 5000 took 0.021s\n",
      "  training loss:\t\t1.505490\n",
      "  validation loss:\t\t1.458478\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1640 of 5000 took 0.022s\n",
      "  training loss:\t\t1.471196\n",
      "  validation loss:\t\t1.451418\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1641 of 5000 took 0.023s\n",
      "  training loss:\t\t1.454410\n",
      "  validation loss:\t\t1.441926\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1642 of 5000 took 0.023s\n",
      "  training loss:\t\t1.426279\n",
      "  validation loss:\t\t1.430760\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1643 of 5000 took 0.023s\n",
      "  training loss:\t\t1.457316\n",
      "  validation loss:\t\t1.419423\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1644 of 5000 took 0.022s\n",
      "  training loss:\t\t1.443411\n",
      "  validation loss:\t\t1.409140\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1645 of 5000 took 0.022s\n",
      "  training loss:\t\t1.438603\n",
      "  validation loss:\t\t1.398675\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1646 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400383\n",
      "  validation loss:\t\t1.392608\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1647 of 5000 took 0.022s\n",
      "  training loss:\t\t1.421300\n",
      "  validation loss:\t\t1.389385\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1648 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406195\n",
      "  validation loss:\t\t1.388410\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1649 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408695\n",
      "  validation loss:\t\t1.388893\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1650 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390883\n",
      "  validation loss:\t\t1.389942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1651 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389505\n",
      "  validation loss:\t\t1.391005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1652 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410679\n",
      "  validation loss:\t\t1.392728\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1653 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383537\n",
      "  validation loss:\t\t1.393050\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1654 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394813\n",
      "  validation loss:\t\t1.393525\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1655 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392401\n",
      "  validation loss:\t\t1.392733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1656 of 5000 took 0.022s\n",
      "  training loss:\t\t1.428945\n",
      "  validation loss:\t\t1.391942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1657 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396756\n",
      "  validation loss:\t\t1.390696\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1658 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391620\n",
      "  validation loss:\t\t1.389861\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1659 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403011\n",
      "  validation loss:\t\t1.389146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1660 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409056\n",
      "  validation loss:\t\t1.388417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1661 of 5000 took 0.023s\n",
      "  training loss:\t\t1.430776\n",
      "  validation loss:\t\t1.388063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1662 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416482\n",
      "  validation loss:\t\t1.387804\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1663 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383082\n",
      "  validation loss:\t\t1.387859\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1664 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399542\n",
      "  validation loss:\t\t1.387897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1665 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403594\n",
      "  validation loss:\t\t1.387879\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1666 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401406\n",
      "  validation loss:\t\t1.388005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1667 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378196\n",
      "  validation loss:\t\t1.388138\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1668 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402518\n",
      "  validation loss:\t\t1.388146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1669 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377307\n",
      "  validation loss:\t\t1.388142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1670 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379831\n",
      "  validation loss:\t\t1.388244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1671 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413579\n",
      "  validation loss:\t\t1.388122\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1672 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404055\n",
      "  validation loss:\t\t1.388135\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1673 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381911\n",
      "  validation loss:\t\t1.388084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1674 of 5000 took 0.022s\n",
      "  training loss:\t\t1.438817\n",
      "  validation loss:\t\t1.387863\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1675 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391962\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1676 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389627\n",
      "  validation loss:\t\t1.387742\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1677 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392381\n",
      "  validation loss:\t\t1.387629\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1678 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399076\n",
      "  validation loss:\t\t1.387590\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1679 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384869\n",
      "  validation loss:\t\t1.387567\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1680 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401396\n",
      "  validation loss:\t\t1.387598\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1681 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399418\n",
      "  validation loss:\t\t1.387576\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1682 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396822\n",
      "  validation loss:\t\t1.387703\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1683 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397147\n",
      "  validation loss:\t\t1.387918\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1684 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385446\n",
      "  validation loss:\t\t1.388370\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1685 of 5000 took 0.022s\n",
      "  training loss:\t\t1.422794\n",
      "  validation loss:\t\t1.388778\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1686 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411760\n",
      "  validation loss:\t\t1.389178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1687 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392324\n",
      "  validation loss:\t\t1.389167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1688 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381702\n",
      "  validation loss:\t\t1.388983\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1689 of 5000 took 0.021s\n",
      "  training loss:\t\t1.413282\n",
      "  validation loss:\t\t1.388574\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1690 of 5000 took 0.021s\n",
      "  training loss:\t\t1.413718\n",
      "  validation loss:\t\t1.388061\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1691 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385146\n",
      "  validation loss:\t\t1.387931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1692 of 5000 took 0.022s\n",
      "  training loss:\t\t1.372312\n",
      "  validation loss:\t\t1.387779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1693 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398430\n",
      "  validation loss:\t\t1.387592\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1694 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406318\n",
      "  validation loss:\t\t1.387561\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1695 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390375\n",
      "  validation loss:\t\t1.387506\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1696 of 5000 took 0.023s\n",
      "  training loss:\t\t1.419918\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1697 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390627\n",
      "  validation loss:\t\t1.387528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1698 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400237\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1699 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398433\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1700 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401203\n",
      "  validation loss:\t\t1.387603\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1701 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386170\n",
      "  validation loss:\t\t1.387649\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1702 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391348\n",
      "  validation loss:\t\t1.387630\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1703 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393008\n",
      "  validation loss:\t\t1.387641\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1704 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410366\n",
      "  validation loss:\t\t1.387666\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1705 of 5000 took 0.023s\n",
      "  training loss:\t\t1.423525\n",
      "  validation loss:\t\t1.387652\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1706 of 5000 took 0.023s\n",
      "  training loss:\t\t1.428205\n",
      "  validation loss:\t\t1.387642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1707 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389890\n",
      "  validation loss:\t\t1.387734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1708 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382444\n",
      "  validation loss:\t\t1.387830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1709 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409360\n",
      "  validation loss:\t\t1.387836\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1710 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396353\n",
      "  validation loss:\t\t1.387669\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1711 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412879\n",
      "  validation loss:\t\t1.387633\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1712 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398955\n",
      "  validation loss:\t\t1.387581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1713 of 5000 took 0.023s\n",
      "  training loss:\t\t1.415934\n",
      "  validation loss:\t\t1.387605\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1714 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384994\n",
      "  validation loss:\t\t1.387528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1715 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385144\n",
      "  validation loss:\t\t1.387551\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1716 of 5000 took 0.024s\n",
      "  training loss:\t\t1.402651\n",
      "  validation loss:\t\t1.387520\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1717 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410523\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1718 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384600\n",
      "  validation loss:\t\t1.387521\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1719 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402913\n",
      "  validation loss:\t\t1.387585\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1720 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406458\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1721 of 5000 took 0.022s\n",
      "  training loss:\t\t1.357603\n",
      "  validation loss:\t\t1.387826\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1722 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402568\n",
      "  validation loss:\t\t1.387938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1723 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401402\n",
      "  validation loss:\t\t1.388059\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1724 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397385\n",
      "  validation loss:\t\t1.388275\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1725 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397885\n",
      "  validation loss:\t\t1.388470\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1726 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411732\n",
      "  validation loss:\t\t1.388598\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1727 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385005\n",
      "  validation loss:\t\t1.388897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1728 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398670\n",
      "  validation loss:\t\t1.388909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1729 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407069\n",
      "  validation loss:\t\t1.388706\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1730 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400304\n",
      "  validation loss:\t\t1.388467\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1731 of 5000 took 0.023s\n",
      "  training loss:\t\t1.369481\n",
      "  validation loss:\t\t1.388234\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1732 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399826\n",
      "  validation loss:\t\t1.387878\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1733 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395105\n",
      "  validation loss:\t\t1.387689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1734 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394948\n",
      "  validation loss:\t\t1.387615\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1735 of 5000 took 0.024s\n",
      "  training loss:\t\t1.405472\n",
      "  validation loss:\t\t1.387621\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1736 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393311\n",
      "  validation loss:\t\t1.387612\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1737 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383058\n",
      "  validation loss:\t\t1.387786\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1738 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392927\n",
      "  validation loss:\t\t1.387915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1739 of 5000 took 0.022s\n",
      "  training loss:\t\t1.419755\n",
      "  validation loss:\t\t1.388001\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1740 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397681\n",
      "  validation loss:\t\t1.388348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1741 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405754\n",
      "  validation loss:\t\t1.388368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1742 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396570\n",
      "  validation loss:\t\t1.388202\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1743 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403951\n",
      "  validation loss:\t\t1.387967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1744 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388879\n",
      "  validation loss:\t\t1.387723\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1745 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385477\n",
      "  validation loss:\t\t1.387727\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1746 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404219\n",
      "  validation loss:\t\t1.387913\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1747 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395274\n",
      "  validation loss:\t\t1.388049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1748 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395665\n",
      "  validation loss:\t\t1.388133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1749 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398182\n",
      "  validation loss:\t\t1.388325\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1750 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388482\n",
      "  validation loss:\t\t1.388209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1751 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402210\n",
      "  validation loss:\t\t1.388343\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1752 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392089\n",
      "  validation loss:\t\t1.388137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1753 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409197\n",
      "  validation loss:\t\t1.387994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1754 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412220\n",
      "  validation loss:\t\t1.387845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1755 of 5000 took 0.024s\n",
      "  training loss:\t\t1.405916\n",
      "  validation loss:\t\t1.387727\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1756 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402398\n",
      "  validation loss:\t\t1.387679\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1757 of 5000 took 0.021s\n",
      "  training loss:\t\t1.409099\n",
      "  validation loss:\t\t1.387710\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1758 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416711\n",
      "  validation loss:\t\t1.387725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1759 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390252\n",
      "  validation loss:\t\t1.387810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1760 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408243\n",
      "  validation loss:\t\t1.387883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1761 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408294\n",
      "  validation loss:\t\t1.387889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1762 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415786\n",
      "  validation loss:\t\t1.387990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1763 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384450\n",
      "  validation loss:\t\t1.388116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1764 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396844\n",
      "  validation loss:\t\t1.388158\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1765 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406463\n",
      "  validation loss:\t\t1.388207\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1766 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391147\n",
      "  validation loss:\t\t1.388319\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1767 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384612\n",
      "  validation loss:\t\t1.388437\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1768 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414257\n",
      "  validation loss:\t\t1.388533\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1769 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392558\n",
      "  validation loss:\t\t1.388823\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1770 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376074\n",
      "  validation loss:\t\t1.389064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1771 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382416\n",
      "  validation loss:\t\t1.389476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1772 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401139\n",
      "  validation loss:\t\t1.389382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1773 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408734\n",
      "  validation loss:\t\t1.389075\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1774 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393331\n",
      "  validation loss:\t\t1.388698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1775 of 5000 took 0.024s\n",
      "  training loss:\t\t1.401621\n",
      "  validation loss:\t\t1.388553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1776 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393547\n",
      "  validation loss:\t\t1.388397\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1777 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390510\n",
      "  validation loss:\t\t1.388077\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1778 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415394\n",
      "  validation loss:\t\t1.387763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1779 of 5000 took 0.023s\n",
      "  training loss:\t\t1.368657\n",
      "  validation loss:\t\t1.387542\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1780 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404645\n",
      "  validation loss:\t\t1.387458\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1781 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415199\n",
      "  validation loss:\t\t1.387463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1782 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389696\n",
      "  validation loss:\t\t1.387536\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1783 of 5000 took 0.022s\n",
      "  training loss:\t\t1.365324\n",
      "  validation loss:\t\t1.387714\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1784 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389852\n",
      "  validation loss:\t\t1.387957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1785 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405493\n",
      "  validation loss:\t\t1.388110\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1786 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396357\n",
      "  validation loss:\t\t1.388035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1787 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416985\n",
      "  validation loss:\t\t1.387782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1788 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384690\n",
      "  validation loss:\t\t1.387618\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1789 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383292\n",
      "  validation loss:\t\t1.387547\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1790 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370406\n",
      "  validation loss:\t\t1.387533\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1791 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408265\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1792 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412894\n",
      "  validation loss:\t\t1.387559\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1793 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392750\n",
      "  validation loss:\t\t1.387547\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1794 of 5000 took 0.024s\n",
      "  training loss:\t\t1.381680\n",
      "  validation loss:\t\t1.387515\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1795 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393017\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1796 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395880\n",
      "  validation loss:\t\t1.387544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1797 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387720\n",
      "  validation loss:\t\t1.387503\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1798 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393808\n",
      "  validation loss:\t\t1.387474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1799 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401775\n",
      "  validation loss:\t\t1.387476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1800 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411739\n",
      "  validation loss:\t\t1.387463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1801 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396320\n",
      "  validation loss:\t\t1.387454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1802 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370259\n",
      "  validation loss:\t\t1.387459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1803 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407015\n",
      "  validation loss:\t\t1.387556\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1804 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393027\n",
      "  validation loss:\t\t1.387535\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1805 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393299\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1806 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394868\n",
      "  validation loss:\t\t1.387561\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1807 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386009\n",
      "  validation loss:\t\t1.387556\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1808 of 5000 took 0.024s\n",
      "  training loss:\t\t1.360708\n",
      "  validation loss:\t\t1.387644\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1809 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397921\n",
      "  validation loss:\t\t1.387624\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1810 of 5000 took 0.023s\n",
      "  training loss:\t\t1.366077\n",
      "  validation loss:\t\t1.387584\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1811 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407824\n",
      "  validation loss:\t\t1.387439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1812 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382520\n",
      "  validation loss:\t\t1.387489\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1813 of 5000 took 0.024s\n",
      "  training loss:\t\t1.377193\n",
      "  validation loss:\t\t1.387515\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1814 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406678\n",
      "  validation loss:\t\t1.387631\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1815 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405167\n",
      "  validation loss:\t\t1.387759\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1816 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395944\n",
      "  validation loss:\t\t1.387925\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1817 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385137\n",
      "  validation loss:\t\t1.387907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1818 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396853\n",
      "  validation loss:\t\t1.387952\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1819 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375494\n",
      "  validation loss:\t\t1.387881\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1820 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386286\n",
      "  validation loss:\t\t1.387785\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1821 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404362\n",
      "  validation loss:\t\t1.387817\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1822 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406013\n",
      "  validation loss:\t\t1.387888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1823 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390440\n",
      "  validation loss:\t\t1.388061\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1824 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401698\n",
      "  validation loss:\t\t1.387784\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1825 of 5000 took 0.021s\n",
      "  training loss:\t\t1.410978\n",
      "  validation loss:\t\t1.387917\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1826 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373774\n",
      "  validation loss:\t\t1.387874\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1827 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379224\n",
      "  validation loss:\t\t1.388012\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1828 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411244\n",
      "  validation loss:\t\t1.388153\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1829 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405971\n",
      "  validation loss:\t\t1.388270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1830 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400210\n",
      "  validation loss:\t\t1.388072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1831 of 5000 took 0.022s\n",
      "  training loss:\t\t1.420776\n",
      "  validation loss:\t\t1.388032\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1832 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381347\n",
      "  validation loss:\t\t1.387841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1833 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389140\n",
      "  validation loss:\t\t1.387858\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1834 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396176\n",
      "  validation loss:\t\t1.387733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1835 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395697\n",
      "  validation loss:\t\t1.387963\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1836 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408795\n",
      "  validation loss:\t\t1.388077\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1837 of 5000 took 0.023s\n",
      "  training loss:\t\t1.420754\n",
      "  validation loss:\t\t1.388054\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1838 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397999\n",
      "  validation loss:\t\t1.387967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1839 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407853\n",
      "  validation loss:\t\t1.387912\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1840 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382910\n",
      "  validation loss:\t\t1.387770\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1841 of 5000 took 0.022s\n",
      "  training loss:\t\t1.371427\n",
      "  validation loss:\t\t1.387627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1842 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406186\n",
      "  validation loss:\t\t1.387607\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1843 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395727\n",
      "  validation loss:\t\t1.387638\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1844 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382829\n",
      "  validation loss:\t\t1.387720\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1845 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389301\n",
      "  validation loss:\t\t1.387792\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1846 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383804\n",
      "  validation loss:\t\t1.387867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1847 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379041\n",
      "  validation loss:\t\t1.387959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1848 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384051\n",
      "  validation loss:\t\t1.387938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1849 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401105\n",
      "  validation loss:\t\t1.387911\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1850 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377123\n",
      "  validation loss:\t\t1.387644\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1851 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388428\n",
      "  validation loss:\t\t1.387617\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1852 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405465\n",
      "  validation loss:\t\t1.387654\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1853 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399233\n",
      "  validation loss:\t\t1.387582\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1854 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406419\n",
      "  validation loss:\t\t1.387543\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1855 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378868\n",
      "  validation loss:\t\t1.387584\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1856 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405841\n",
      "  validation loss:\t\t1.387626\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1857 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377320\n",
      "  validation loss:\t\t1.387735\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1858 of 5000 took 0.023s\n",
      "  training loss:\t\t1.416458\n",
      "  validation loss:\t\t1.387779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1859 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408898\n",
      "  validation loss:\t\t1.387980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1860 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391250\n",
      "  validation loss:\t\t1.388142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1861 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412883\n",
      "  validation loss:\t\t1.387890\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1862 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389762\n",
      "  validation loss:\t\t1.387909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1863 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373251\n",
      "  validation loss:\t\t1.387733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1864 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389220\n",
      "  validation loss:\t\t1.387589\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1865 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390467\n",
      "  validation loss:\t\t1.387544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1866 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395642\n",
      "  validation loss:\t\t1.387582\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1867 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395058\n",
      "  validation loss:\t\t1.387754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1868 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413867\n",
      "  validation loss:\t\t1.387705\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1869 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409439\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1870 of 5000 took 0.024s\n",
      "  training loss:\t\t1.406980\n",
      "  validation loss:\t\t1.387594\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1871 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400022\n",
      "  validation loss:\t\t1.387578\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1872 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396904\n",
      "  validation loss:\t\t1.387641\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1873 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383650\n",
      "  validation loss:\t\t1.387652\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1874 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401920\n",
      "  validation loss:\t\t1.387702\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1875 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380115\n",
      "  validation loss:\t\t1.387686\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1876 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406762\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1877 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395927\n",
      "  validation loss:\t\t1.387523\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1878 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409169\n",
      "  validation loss:\t\t1.387500\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1879 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394598\n",
      "  validation loss:\t\t1.387510\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1880 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407171\n",
      "  validation loss:\t\t1.387463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1881 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398567\n",
      "  validation loss:\t\t1.387512\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1882 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408589\n",
      "  validation loss:\t\t1.387614\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1883 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397735\n",
      "  validation loss:\t\t1.387591\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1884 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390711\n",
      "  validation loss:\t\t1.387683\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1885 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400184\n",
      "  validation loss:\t\t1.387836\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1886 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383908\n",
      "  validation loss:\t\t1.387830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1887 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394535\n",
      "  validation loss:\t\t1.387712\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1888 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376818\n",
      "  validation loss:\t\t1.387647\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1889 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379883\n",
      "  validation loss:\t\t1.387694\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1890 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391838\n",
      "  validation loss:\t\t1.387730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1891 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402019\n",
      "  validation loss:\t\t1.387758\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1892 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402551\n",
      "  validation loss:\t\t1.387713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1893 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403507\n",
      "  validation loss:\t\t1.387778\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1894 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394929\n",
      "  validation loss:\t\t1.388002\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1895 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413872\n",
      "  validation loss:\t\t1.387932\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1896 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395298\n",
      "  validation loss:\t\t1.387952\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1897 of 5000 took 0.024s\n",
      "  training loss:\t\t1.364385\n",
      "  validation loss:\t\t1.388067\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1898 of 5000 took 0.023s\n",
      "  training loss:\t\t1.410469\n",
      "  validation loss:\t\t1.388102\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1899 of 5000 took 0.022s\n",
      "  training loss:\t\t1.420111\n",
      "  validation loss:\t\t1.387960\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1900 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389280\n",
      "  validation loss:\t\t1.387857\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1901 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406571\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1902 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401715\n",
      "  validation loss:\t\t1.387604\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1903 of 5000 took 0.023s\n",
      "  training loss:\t\t1.420059\n",
      "  validation loss:\t\t1.387777\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1904 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380775\n",
      "  validation loss:\t\t1.387786\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1905 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381123\n",
      "  validation loss:\t\t1.387658\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1906 of 5000 took 0.024s\n",
      "  training loss:\t\t1.414824\n",
      "  validation loss:\t\t1.387679\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1907 of 5000 took 0.024s\n",
      "  training loss:\t\t1.413614\n",
      "  validation loss:\t\t1.387616\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1908 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402552\n",
      "  validation loss:\t\t1.387540\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1909 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383780\n",
      "  validation loss:\t\t1.387579\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1910 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410361\n",
      "  validation loss:\t\t1.387492\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1911 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385239\n",
      "  validation loss:\t\t1.387456\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1912 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384646\n",
      "  validation loss:\t\t1.387673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1913 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383533\n",
      "  validation loss:\t\t1.388150\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1914 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391609\n",
      "  validation loss:\t\t1.388418\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1915 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388280\n",
      "  validation loss:\t\t1.388794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1916 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389535\n",
      "  validation loss:\t\t1.389331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1917 of 5000 took 0.021s\n",
      "  training loss:\t\t1.417420\n",
      "  validation loss:\t\t1.389479\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1918 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405883\n",
      "  validation loss:\t\t1.389304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1919 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403750\n",
      "  validation loss:\t\t1.389032\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1920 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394718\n",
      "  validation loss:\t\t1.388627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1921 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409033\n",
      "  validation loss:\t\t1.388417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1922 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387789\n",
      "  validation loss:\t\t1.388285\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1923 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388799\n",
      "  validation loss:\t\t1.387908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1924 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385818\n",
      "  validation loss:\t\t1.387752\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1925 of 5000 took 0.021s\n",
      "  training loss:\t\t1.411457\n",
      "  validation loss:\t\t1.387532\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1926 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378238\n",
      "  validation loss:\t\t1.387518\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1927 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404304\n",
      "  validation loss:\t\t1.387636\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1928 of 5000 took 0.021s\n",
      "  training loss:\t\t1.404031\n",
      "  validation loss:\t\t1.387758\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1929 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406621\n",
      "  validation loss:\t\t1.387921\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1930 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389083\n",
      "  validation loss:\t\t1.388044\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1931 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398892\n",
      "  validation loss:\t\t1.387903\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1932 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403499\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1933 of 5000 took 0.023s\n",
      "  training loss:\t\t1.417007\n",
      "  validation loss:\t\t1.387376\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1934 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393192\n",
      "  validation loss:\t\t1.387360\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1935 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399317\n",
      "  validation loss:\t\t1.387361\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1936 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390362\n",
      "  validation loss:\t\t1.387416\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1937 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403732\n",
      "  validation loss:\t\t1.387413\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1938 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397168\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1939 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392447\n",
      "  validation loss:\t\t1.387753\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1940 of 5000 took 0.021s\n",
      "  training loss:\t\t1.406973\n",
      "  validation loss:\t\t1.387919\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1941 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387723\n",
      "  validation loss:\t\t1.388124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1942 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407423\n",
      "  validation loss:\t\t1.388117\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1943 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414287\n",
      "  validation loss:\t\t1.388296\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1944 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400891\n",
      "  validation loss:\t\t1.388301\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1945 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400012\n",
      "  validation loss:\t\t1.388062\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1946 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392084\n",
      "  validation loss:\t\t1.387849\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1947 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389882\n",
      "  validation loss:\t\t1.387618\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1948 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388066\n",
      "  validation loss:\t\t1.387431\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1949 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385367\n",
      "  validation loss:\t\t1.387378\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1950 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381122\n",
      "  validation loss:\t\t1.387417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1951 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394828\n",
      "  validation loss:\t\t1.387383\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1952 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388715\n",
      "  validation loss:\t\t1.387438\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1953 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388492\n",
      "  validation loss:\t\t1.387427\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1954 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399999\n",
      "  validation loss:\t\t1.387453\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1955 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389395\n",
      "  validation loss:\t\t1.387465\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1956 of 5000 took 0.024s\n",
      "  training loss:\t\t1.377702\n",
      "  validation loss:\t\t1.387565\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1957 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393217\n",
      "  validation loss:\t\t1.387652\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1958 of 5000 took 0.021s\n",
      "  training loss:\t\t1.410887\n",
      "  validation loss:\t\t1.387651\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1959 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383156\n",
      "  validation loss:\t\t1.387637\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1960 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406215\n",
      "  validation loss:\t\t1.387544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1961 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408559\n",
      "  validation loss:\t\t1.387554\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1962 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401469\n",
      "  validation loss:\t\t1.387530\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1963 of 5000 took 0.022s\n",
      "  training loss:\t\t1.416375\n",
      "  validation loss:\t\t1.387532\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1964 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413771\n",
      "  validation loss:\t\t1.387471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1965 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406514\n",
      "  validation loss:\t\t1.387399\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1966 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388265\n",
      "  validation loss:\t\t1.387406\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1967 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392954\n",
      "  validation loss:\t\t1.387403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1968 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397565\n",
      "  validation loss:\t\t1.387451\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1969 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388776\n",
      "  validation loss:\t\t1.387571\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1970 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391839\n",
      "  validation loss:\t\t1.387627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1971 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401008\n",
      "  validation loss:\t\t1.387600\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1972 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386664\n",
      "  validation loss:\t\t1.387560\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1973 of 5000 took 0.025s\n",
      "  training loss:\t\t1.397774\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1974 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398220\n",
      "  validation loss:\t\t1.387742\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1975 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388341\n",
      "  validation loss:\t\t1.388005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1976 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383300\n",
      "  validation loss:\t\t1.387973\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1977 of 5000 took 0.023s\n",
      "  training loss:\t\t1.370954\n",
      "  validation loss:\t\t1.388163\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1978 of 5000 took 0.022s\n",
      "  training loss:\t\t1.362957\n",
      "  validation loss:\t\t1.387891\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1979 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392760\n",
      "  validation loss:\t\t1.388004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1980 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387009\n",
      "  validation loss:\t\t1.387797\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1981 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374834\n",
      "  validation loss:\t\t1.387732\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1982 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408479\n",
      "  validation loss:\t\t1.387610\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1983 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380979\n",
      "  validation loss:\t\t1.387618\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1984 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405940\n",
      "  validation loss:\t\t1.387629\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1985 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395909\n",
      "  validation loss:\t\t1.387639\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1986 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384572\n",
      "  validation loss:\t\t1.387600\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1987 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392998\n",
      "  validation loss:\t\t1.387885\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1988 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397897\n",
      "  validation loss:\t\t1.388058\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1989 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381930\n",
      "  validation loss:\t\t1.388264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1990 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403947\n",
      "  validation loss:\t\t1.387979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1991 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403976\n",
      "  validation loss:\t\t1.387869\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1992 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392594\n",
      "  validation loss:\t\t1.387896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1993 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397359\n",
      "  validation loss:\t\t1.387969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1994 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400967\n",
      "  validation loss:\t\t1.387775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1995 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398848\n",
      "  validation loss:\t\t1.387627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1996 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388007\n",
      "  validation loss:\t\t1.387787\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1997 of 5000 took 0.024s\n",
      "  training loss:\t\t1.407400\n",
      "  validation loss:\t\t1.387732\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1998 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388000\n",
      "  validation loss:\t\t1.387875\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 1999 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393784\n",
      "  validation loss:\t\t1.388018\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2000 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393146\n",
      "  validation loss:\t\t1.388164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2001 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381623\n",
      "  validation loss:\t\t1.388439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2002 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384639\n",
      "  validation loss:\t\t1.388606\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2003 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384994\n",
      "  validation loss:\t\t1.388624\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2004 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402761\n",
      "  validation loss:\t\t1.388598\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2005 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412293\n",
      "  validation loss:\t\t1.388361\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2006 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380726\n",
      "  validation loss:\t\t1.387984\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2007 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401593\n",
      "  validation loss:\t\t1.387879\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2008 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387830\n",
      "  validation loss:\t\t1.387958\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2009 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388536\n",
      "  validation loss:\t\t1.388162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2010 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394203\n",
      "  validation loss:\t\t1.388207\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2011 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397790\n",
      "  validation loss:\t\t1.388053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2012 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393330\n",
      "  validation loss:\t\t1.388124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2013 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391687\n",
      "  validation loss:\t\t1.387860\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2014 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405365\n",
      "  validation loss:\t\t1.387867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2015 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409529\n",
      "  validation loss:\t\t1.387626\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2016 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398285\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2017 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408327\n",
      "  validation loss:\t\t1.387482\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2018 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393128\n",
      "  validation loss:\t\t1.387514\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2019 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399984\n",
      "  validation loss:\t\t1.387453\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2020 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376421\n",
      "  validation loss:\t\t1.387444\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2021 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385155\n",
      "  validation loss:\t\t1.387439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2022 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398977\n",
      "  validation loss:\t\t1.387474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2023 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392525\n",
      "  validation loss:\t\t1.387671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2024 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398279\n",
      "  validation loss:\t\t1.387850\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2025 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382293\n",
      "  validation loss:\t\t1.388200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2026 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377555\n",
      "  validation loss:\t\t1.388340\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2027 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373638\n",
      "  validation loss:\t\t1.388478\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2028 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401139\n",
      "  validation loss:\t\t1.388260\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2029 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374304\n",
      "  validation loss:\t\t1.388336\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2030 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385857\n",
      "  validation loss:\t\t1.388305\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2031 of 5000 took 0.022s\n",
      "  training loss:\t\t1.417497\n",
      "  validation loss:\t\t1.388104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2032 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381850\n",
      "  validation loss:\t\t1.388095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2033 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382364\n",
      "  validation loss:\t\t1.388090\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2034 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377009\n",
      "  validation loss:\t\t1.387954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2035 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392663\n",
      "  validation loss:\t\t1.387812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2036 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396268\n",
      "  validation loss:\t\t1.387740\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2037 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413219\n",
      "  validation loss:\t\t1.387698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2038 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402528\n",
      "  validation loss:\t\t1.387864\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2039 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402612\n",
      "  validation loss:\t\t1.387933\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2040 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401653\n",
      "  validation loss:\t\t1.387967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2041 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391940\n",
      "  validation loss:\t\t1.388004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2042 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383444\n",
      "  validation loss:\t\t1.388039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2043 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384114\n",
      "  validation loss:\t\t1.388141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2044 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404681\n",
      "  validation loss:\t\t1.388396\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2045 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382088\n",
      "  validation loss:\t\t1.388573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2046 of 5000 took 0.024s\n",
      "  training loss:\t\t1.411162\n",
      "  validation loss:\t\t1.388293\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2047 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413606\n",
      "  validation loss:\t\t1.388078\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2048 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406039\n",
      "  validation loss:\t\t1.387948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2049 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385778\n",
      "  validation loss:\t\t1.387768\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2050 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385704\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2051 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384193\n",
      "  validation loss:\t\t1.387476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2052 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378888\n",
      "  validation loss:\t\t1.387507\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2053 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379658\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2054 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400270\n",
      "  validation loss:\t\t1.387627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2055 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392752\n",
      "  validation loss:\t\t1.387584\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2056 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392274\n",
      "  validation loss:\t\t1.387543\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2057 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383490\n",
      "  validation loss:\t\t1.387394\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2058 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392982\n",
      "  validation loss:\t\t1.387316\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2059 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399408\n",
      "  validation loss:\t\t1.387287\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2060 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391191\n",
      "  validation loss:\t\t1.387336\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2061 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394342\n",
      "  validation loss:\t\t1.387369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2062 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395858\n",
      "  validation loss:\t\t1.387414\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2063 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396418\n",
      "  validation loss:\t\t1.387522\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2064 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407667\n",
      "  validation loss:\t\t1.387681\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2065 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394711\n",
      "  validation loss:\t\t1.387768\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2066 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389710\n",
      "  validation loss:\t\t1.387776\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2067 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397121\n",
      "  validation loss:\t\t1.387708\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2068 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386778\n",
      "  validation loss:\t\t1.387766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2069 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404146\n",
      "  validation loss:\t\t1.387768\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2070 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390556\n",
      "  validation loss:\t\t1.387933\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2071 of 5000 took 0.023s\n",
      "  training loss:\t\t1.422786\n",
      "  validation loss:\t\t1.387918\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2072 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403817\n",
      "  validation loss:\t\t1.387947\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2073 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388227\n",
      "  validation loss:\t\t1.388155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2074 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397120\n",
      "  validation loss:\t\t1.387995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2075 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384025\n",
      "  validation loss:\t\t1.387908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2076 of 5000 took 0.021s\n",
      "  training loss:\t\t1.411547\n",
      "  validation loss:\t\t1.387792\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2077 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403611\n",
      "  validation loss:\t\t1.387557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2078 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392155\n",
      "  validation loss:\t\t1.387355\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2079 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398175\n",
      "  validation loss:\t\t1.387267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2080 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388662\n",
      "  validation loss:\t\t1.387327\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2081 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402792\n",
      "  validation loss:\t\t1.387412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2082 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388070\n",
      "  validation loss:\t\t1.387721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2083 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406119\n",
      "  validation loss:\t\t1.387868\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2084 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389348\n",
      "  validation loss:\t\t1.388064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2085 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386522\n",
      "  validation loss:\t\t1.388152\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2086 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399022\n",
      "  validation loss:\t\t1.387965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2087 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376133\n",
      "  validation loss:\t\t1.387904\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2088 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393261\n",
      "  validation loss:\t\t1.387836\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2089 of 5000 took 0.024s\n",
      "  training loss:\t\t1.410251\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2090 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395825\n",
      "  validation loss:\t\t1.387585\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2091 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387270\n",
      "  validation loss:\t\t1.387568\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2092 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389492\n",
      "  validation loss:\t\t1.387529\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2093 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375025\n",
      "  validation loss:\t\t1.387546\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2094 of 5000 took 0.022s\n",
      "  training loss:\t\t1.415959\n",
      "  validation loss:\t\t1.387473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2095 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377343\n",
      "  validation loss:\t\t1.387492\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2096 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398080\n",
      "  validation loss:\t\t1.387651\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2097 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388239\n",
      "  validation loss:\t\t1.387594\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2098 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394472\n",
      "  validation loss:\t\t1.387547\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2099 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413038\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2100 of 5000 took 0.023s\n",
      "  training loss:\t\t1.414422\n",
      "  validation loss:\t\t1.387488\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2101 of 5000 took 0.023s\n",
      "  training loss:\t\t1.415695\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2102 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412842\n",
      "  validation loss:\t\t1.387819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2103 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388002\n",
      "  validation loss:\t\t1.388266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2104 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407948\n",
      "  validation loss:\t\t1.388545\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2105 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380987\n",
      "  validation loss:\t\t1.389056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2106 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411788\n",
      "  validation loss:\t\t1.388934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2107 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388086\n",
      "  validation loss:\t\t1.388803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2108 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384269\n",
      "  validation loss:\t\t1.388887\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2109 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395030\n",
      "  validation loss:\t\t1.388698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2110 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387190\n",
      "  validation loss:\t\t1.388505\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2111 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402647\n",
      "  validation loss:\t\t1.388159\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2112 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403469\n",
      "  validation loss:\t\t1.388092\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2113 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412558\n",
      "  validation loss:\t\t1.387786\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2114 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388022\n",
      "  validation loss:\t\t1.387658\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2115 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398541\n",
      "  validation loss:\t\t1.387527\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2116 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395198\n",
      "  validation loss:\t\t1.387553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2117 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398866\n",
      "  validation loss:\t\t1.387512\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2118 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386344\n",
      "  validation loss:\t\t1.387567\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2119 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401456\n",
      "  validation loss:\t\t1.387672\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2120 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384721\n",
      "  validation loss:\t\t1.387752\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2121 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397318\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2122 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396348\n",
      "  validation loss:\t\t1.387523\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2123 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388592\n",
      "  validation loss:\t\t1.387417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2124 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395962\n",
      "  validation loss:\t\t1.387311\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2125 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391322\n",
      "  validation loss:\t\t1.387309\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2126 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402360\n",
      "  validation loss:\t\t1.387307\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2127 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391623\n",
      "  validation loss:\t\t1.387349\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2128 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396621\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2129 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397309\n",
      "  validation loss:\t\t1.387384\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2130 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403053\n",
      "  validation loss:\t\t1.387425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2131 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399380\n",
      "  validation loss:\t\t1.387577\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2132 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389362\n",
      "  validation loss:\t\t1.387730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2133 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383817\n",
      "  validation loss:\t\t1.387684\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2134 of 5000 took 0.022s\n",
      "  training loss:\t\t1.369603\n",
      "  validation loss:\t\t1.387923\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2135 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385905\n",
      "  validation loss:\t\t1.387905\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2136 of 5000 took 0.024s\n",
      "  training loss:\t\t1.399085\n",
      "  validation loss:\t\t1.387713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2137 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398034\n",
      "  validation loss:\t\t1.387554\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2138 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393509\n",
      "  validation loss:\t\t1.387430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2139 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390059\n",
      "  validation loss:\t\t1.387401\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2140 of 5000 took 0.023s\n",
      "  training loss:\t\t1.412333\n",
      "  validation loss:\t\t1.387405\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2141 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397587\n",
      "  validation loss:\t\t1.387475\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2142 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402983\n",
      "  validation loss:\t\t1.387494\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2143 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393525\n",
      "  validation loss:\t\t1.387455\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2144 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396327\n",
      "  validation loss:\t\t1.387374\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2145 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397882\n",
      "  validation loss:\t\t1.387372\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2146 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380669\n",
      "  validation loss:\t\t1.387500\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2147 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389343\n",
      "  validation loss:\t\t1.387454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2148 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399308\n",
      "  validation loss:\t\t1.387549\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2149 of 5000 took 0.023s\n",
      "  training loss:\t\t1.423143\n",
      "  validation loss:\t\t1.387493\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2150 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383741\n",
      "  validation loss:\t\t1.387535\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2151 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381750\n",
      "  validation loss:\t\t1.387568\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2152 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388216\n",
      "  validation loss:\t\t1.387745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2153 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387435\n",
      "  validation loss:\t\t1.387936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2154 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392626\n",
      "  validation loss:\t\t1.388061\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2155 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388047\n",
      "  validation loss:\t\t1.388184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2156 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405498\n",
      "  validation loss:\t\t1.388039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2157 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388653\n",
      "  validation loss:\t\t1.387722\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2158 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376577\n",
      "  validation loss:\t\t1.387566\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2159 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403494\n",
      "  validation loss:\t\t1.387385\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2160 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388852\n",
      "  validation loss:\t\t1.387317\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2161 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383279\n",
      "  validation loss:\t\t1.387331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2162 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396913\n",
      "  validation loss:\t\t1.387487\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2163 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391594\n",
      "  validation loss:\t\t1.387531\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2164 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391236\n",
      "  validation loss:\t\t1.387419\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2165 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399533\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2166 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390504\n",
      "  validation loss:\t\t1.387664\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2167 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381872\n",
      "  validation loss:\t\t1.387893\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2168 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391267\n",
      "  validation loss:\t\t1.388051\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2169 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380913\n",
      "  validation loss:\t\t1.388291\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2170 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402559\n",
      "  validation loss:\t\t1.388542\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2171 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384020\n",
      "  validation loss:\t\t1.388660\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2172 of 5000 took 0.024s\n",
      "  training loss:\t\t1.379092\n",
      "  validation loss:\t\t1.388679\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2173 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386830\n",
      "  validation loss:\t\t1.388834\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2174 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400820\n",
      "  validation loss:\t\t1.388801\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2175 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390674\n",
      "  validation loss:\t\t1.388736\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2176 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374895\n",
      "  validation loss:\t\t1.388763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2177 of 5000 took 0.023s\n",
      "  training loss:\t\t1.363054\n",
      "  validation loss:\t\t1.388825\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2178 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414074\n",
      "  validation loss:\t\t1.388273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2179 of 5000 took 0.022s\n",
      "  training loss:\t\t1.412413\n",
      "  validation loss:\t\t1.387989\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2180 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397999\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2181 of 5000 took 0.024s\n",
      "  training loss:\t\t1.402894\n",
      "  validation loss:\t\t1.387370\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2182 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391607\n",
      "  validation loss:\t\t1.387316\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2183 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388530\n",
      "  validation loss:\t\t1.387285\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2184 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393687\n",
      "  validation loss:\t\t1.387372\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2185 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400874\n",
      "  validation loss:\t\t1.387431\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2186 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411560\n",
      "  validation loss:\t\t1.387492\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2187 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389519\n",
      "  validation loss:\t\t1.387639\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2188 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387783\n",
      "  validation loss:\t\t1.387941\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2189 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374971\n",
      "  validation loss:\t\t1.388022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2190 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397314\n",
      "  validation loss:\t\t1.388117\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2191 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397825\n",
      "  validation loss:\t\t1.388020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2192 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395124\n",
      "  validation loss:\t\t1.388039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2193 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408362\n",
      "  validation loss:\t\t1.387947\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2194 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388376\n",
      "  validation loss:\t\t1.387749\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2195 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396490\n",
      "  validation loss:\t\t1.387808\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2196 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393932\n",
      "  validation loss:\t\t1.387833\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2197 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395615\n",
      "  validation loss:\t\t1.387994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2198 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381468\n",
      "  validation loss:\t\t1.388095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2199 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391480\n",
      "  validation loss:\t\t1.388029\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2200 of 5000 took 0.024s\n",
      "  training loss:\t\t1.377392\n",
      "  validation loss:\t\t1.388103\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2201 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399311\n",
      "  validation loss:\t\t1.388054\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2202 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401076\n",
      "  validation loss:\t\t1.387882\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2203 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378485\n",
      "  validation loss:\t\t1.387794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2204 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385507\n",
      "  validation loss:\t\t1.387701\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2205 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411856\n",
      "  validation loss:\t\t1.387576\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2206 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398078\n",
      "  validation loss:\t\t1.387604\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2207 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383563\n",
      "  validation loss:\t\t1.387603\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2208 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384503\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2209 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397327\n",
      "  validation loss:\t\t1.387535\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2210 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394024\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2211 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387447\n",
      "  validation loss:\t\t1.387674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2212 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397914\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2213 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407272\n",
      "  validation loss:\t\t1.387578\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2214 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398480\n",
      "  validation loss:\t\t1.387734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2215 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383175\n",
      "  validation loss:\t\t1.387958\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2216 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383315\n",
      "  validation loss:\t\t1.387986\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2217 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398790\n",
      "  validation loss:\t\t1.387831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2218 of 5000 took 0.024s\n",
      "  training loss:\t\t1.402341\n",
      "  validation loss:\t\t1.387596\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2219 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378973\n",
      "  validation loss:\t\t1.387682\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2220 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379818\n",
      "  validation loss:\t\t1.387714\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2221 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390151\n",
      "  validation loss:\t\t1.387685\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2222 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381846\n",
      "  validation loss:\t\t1.387482\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2223 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388607\n",
      "  validation loss:\t\t1.387476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2224 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403800\n",
      "  validation loss:\t\t1.387454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2225 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394200\n",
      "  validation loss:\t\t1.387411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2226 of 5000 took 0.021s\n",
      "  training loss:\t\t1.404081\n",
      "  validation loss:\t\t1.387349\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2227 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397481\n",
      "  validation loss:\t\t1.387410\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2228 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388197\n",
      "  validation loss:\t\t1.387536\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2229 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382965\n",
      "  validation loss:\t\t1.387686\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2230 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403689\n",
      "  validation loss:\t\t1.387826\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2231 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402496\n",
      "  validation loss:\t\t1.387938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2232 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393567\n",
      "  validation loss:\t\t1.388217\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2233 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387475\n",
      "  validation loss:\t\t1.388179\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2234 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399094\n",
      "  validation loss:\t\t1.387993\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2235 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379572\n",
      "  validation loss:\t\t1.387939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2236 of 5000 took 0.024s\n",
      "  training loss:\t\t1.380494\n",
      "  validation loss:\t\t1.387865\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2237 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384194\n",
      "  validation loss:\t\t1.388033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2238 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383007\n",
      "  validation loss:\t\t1.388471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2239 of 5000 took 0.021s\n",
      "  training loss:\t\t1.368979\n",
      "  validation loss:\t\t1.388986\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2240 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398217\n",
      "  validation loss:\t\t1.389506\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2241 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391097\n",
      "  validation loss:\t\t1.390099\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2242 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378485\n",
      "  validation loss:\t\t1.390397\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2243 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393057\n",
      "  validation loss:\t\t1.390173\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2244 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397451\n",
      "  validation loss:\t\t1.389995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2245 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405287\n",
      "  validation loss:\t\t1.389440\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2246 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400545\n",
      "  validation loss:\t\t1.389161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2247 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410629\n",
      "  validation loss:\t\t1.388698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2248 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385006\n",
      "  validation loss:\t\t1.388425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2249 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407495\n",
      "  validation loss:\t\t1.388109\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2250 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383892\n",
      "  validation loss:\t\t1.388023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2251 of 5000 took 0.023s\n",
      "  training loss:\t\t1.373821\n",
      "  validation loss:\t\t1.387939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2252 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379028\n",
      "  validation loss:\t\t1.387928\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2253 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396281\n",
      "  validation loss:\t\t1.387850\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2254 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377000\n",
      "  validation loss:\t\t1.387965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2255 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396379\n",
      "  validation loss:\t\t1.388034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2256 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395832\n",
      "  validation loss:\t\t1.387972\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2257 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386833\n",
      "  validation loss:\t\t1.388023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2258 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387892\n",
      "  validation loss:\t\t1.388033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2259 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394255\n",
      "  validation loss:\t\t1.387877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2260 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386847\n",
      "  validation loss:\t\t1.387749\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2261 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402861\n",
      "  validation loss:\t\t1.387786\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2262 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389109\n",
      "  validation loss:\t\t1.387944\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2263 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397941\n",
      "  validation loss:\t\t1.387830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2264 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396181\n",
      "  validation loss:\t\t1.387937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2265 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401580\n",
      "  validation loss:\t\t1.388033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2266 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401786\n",
      "  validation loss:\t\t1.387969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2267 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381541\n",
      "  validation loss:\t\t1.387792\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2268 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404867\n",
      "  validation loss:\t\t1.387630\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2269 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401201\n",
      "  validation loss:\t\t1.387625\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2270 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381553\n",
      "  validation loss:\t\t1.387730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2271 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387824\n",
      "  validation loss:\t\t1.387842\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2272 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396467\n",
      "  validation loss:\t\t1.387793\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2273 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396599\n",
      "  validation loss:\t\t1.387725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2274 of 5000 took 0.023s\n",
      "  training loss:\t\t1.408840\n",
      "  validation loss:\t\t1.387775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2275 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370845\n",
      "  validation loss:\t\t1.387949\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2276 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401094\n",
      "  validation loss:\t\t1.387990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2277 of 5000 took 0.021s\n",
      "  training loss:\t\t1.378140\n",
      "  validation loss:\t\t1.387744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2278 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392159\n",
      "  validation loss:\t\t1.387495\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2279 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376061\n",
      "  validation loss:\t\t1.387426\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2280 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399169\n",
      "  validation loss:\t\t1.387501\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2281 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381396\n",
      "  validation loss:\t\t1.387459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2282 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390973\n",
      "  validation loss:\t\t1.387431\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2283 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390896\n",
      "  validation loss:\t\t1.387623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2284 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388292\n",
      "  validation loss:\t\t1.387666\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2285 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376817\n",
      "  validation loss:\t\t1.387907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2286 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378101\n",
      "  validation loss:\t\t1.388061\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2287 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401752\n",
      "  validation loss:\t\t1.388205\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2288 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383981\n",
      "  validation loss:\t\t1.388210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2289 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384596\n",
      "  validation loss:\t\t1.388149\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2290 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378202\n",
      "  validation loss:\t\t1.387704\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2291 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386951\n",
      "  validation loss:\t\t1.387432\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2292 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401916\n",
      "  validation loss:\t\t1.387244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2293 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394140\n",
      "  validation loss:\t\t1.387231\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2294 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394673\n",
      "  validation loss:\t\t1.387440\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2295 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390080\n",
      "  validation loss:\t\t1.387907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2296 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391605\n",
      "  validation loss:\t\t1.388392\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2297 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407403\n",
      "  validation loss:\t\t1.388497\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2298 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389352\n",
      "  validation loss:\t\t1.388863\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2299 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381523\n",
      "  validation loss:\t\t1.388974\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2300 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383187\n",
      "  validation loss:\t\t1.388659\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2301 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380801\n",
      "  validation loss:\t\t1.388581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2302 of 5000 took 0.022s\n",
      "  training loss:\t\t1.411300\n",
      "  validation loss:\t\t1.388191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2303 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393257\n",
      "  validation loss:\t\t1.388033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2304 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397467\n",
      "  validation loss:\t\t1.387821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2305 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395893\n",
      "  validation loss:\t\t1.387845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2306 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392253\n",
      "  validation loss:\t\t1.387787\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2307 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401018\n",
      "  validation loss:\t\t1.387804\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2308 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375300\n",
      "  validation loss:\t\t1.388234\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2309 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399003\n",
      "  validation loss:\t\t1.388124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2310 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397574\n",
      "  validation loss:\t\t1.388266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2311 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400065\n",
      "  validation loss:\t\t1.388116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2312 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400860\n",
      "  validation loss:\t\t1.387807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2313 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404973\n",
      "  validation loss:\t\t1.387534\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2314 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386722\n",
      "  validation loss:\t\t1.387358\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2315 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381232\n",
      "  validation loss:\t\t1.387380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2316 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388045\n",
      "  validation loss:\t\t1.387400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2317 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397264\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2318 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395935\n",
      "  validation loss:\t\t1.387438\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2319 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405724\n",
      "  validation loss:\t\t1.387337\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2320 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379663\n",
      "  validation loss:\t\t1.387320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2321 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382333\n",
      "  validation loss:\t\t1.387369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2322 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393266\n",
      "  validation loss:\t\t1.387393\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2323 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387979\n",
      "  validation loss:\t\t1.387267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2324 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380485\n",
      "  validation loss:\t\t1.387201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2325 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378337\n",
      "  validation loss:\t\t1.387160\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2326 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384226\n",
      "  validation loss:\t\t1.387147\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2327 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378085\n",
      "  validation loss:\t\t1.387162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2328 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383397\n",
      "  validation loss:\t\t1.387256\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2329 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384665\n",
      "  validation loss:\t\t1.387443\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2330 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389010\n",
      "  validation loss:\t\t1.387647\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2331 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386820\n",
      "  validation loss:\t\t1.387969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2332 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377349\n",
      "  validation loss:\t\t1.388369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2333 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398455\n",
      "  validation loss:\t\t1.388444\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2334 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398221\n",
      "  validation loss:\t\t1.388649\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2335 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390784\n",
      "  validation loss:\t\t1.388803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2336 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377731\n",
      "  validation loss:\t\t1.388834\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2337 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394996\n",
      "  validation loss:\t\t1.388914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2338 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395392\n",
      "  validation loss:\t\t1.388405\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2339 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391173\n",
      "  validation loss:\t\t1.388131\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2340 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385458\n",
      "  validation loss:\t\t1.387999\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2341 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383865\n",
      "  validation loss:\t\t1.388122\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2342 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401119\n",
      "  validation loss:\t\t1.388111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2343 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389218\n",
      "  validation loss:\t\t1.388026\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2344 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390087\n",
      "  validation loss:\t\t1.387843\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2345 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379357\n",
      "  validation loss:\t\t1.387830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2346 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390310\n",
      "  validation loss:\t\t1.387735\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2347 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389552\n",
      "  validation loss:\t\t1.387468\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2348 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401188\n",
      "  validation loss:\t\t1.387304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2349 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385625\n",
      "  validation loss:\t\t1.387183\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2350 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395528\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2351 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378036\n",
      "  validation loss:\t\t1.387206\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2352 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395834\n",
      "  validation loss:\t\t1.387241\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2353 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390335\n",
      "  validation loss:\t\t1.387196\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2354 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380411\n",
      "  validation loss:\t\t1.387178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2355 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407096\n",
      "  validation loss:\t\t1.387207\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2356 of 5000 took 0.023s\n",
      "  training loss:\t\t1.413590\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2357 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393680\n",
      "  validation loss:\t\t1.387342\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2358 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389372\n",
      "  validation loss:\t\t1.387435\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2359 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392666\n",
      "  validation loss:\t\t1.387361\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2360 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373218\n",
      "  validation loss:\t\t1.387365\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2361 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403991\n",
      "  validation loss:\t\t1.387476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2362 of 5000 took 0.022s\n",
      "  training loss:\t\t1.372255\n",
      "  validation loss:\t\t1.387680\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2363 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393134\n",
      "  validation loss:\t\t1.387857\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2364 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402121\n",
      "  validation loss:\t\t1.388089\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2365 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390394\n",
      "  validation loss:\t\t1.388513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2366 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394335\n",
      "  validation loss:\t\t1.389072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2367 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393199\n",
      "  validation loss:\t\t1.389054\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2368 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395905\n",
      "  validation loss:\t\t1.388944\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2369 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390119\n",
      "  validation loss:\t\t1.388544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2370 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396232\n",
      "  validation loss:\t\t1.388076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2371 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381210\n",
      "  validation loss:\t\t1.387633\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2372 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388356\n",
      "  validation loss:\t\t1.387552\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2373 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377895\n",
      "  validation loss:\t\t1.387456\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2374 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387262\n",
      "  validation loss:\t\t1.387495\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2375 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375015\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2376 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398851\n",
      "  validation loss:\t\t1.387638\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2377 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398567\n",
      "  validation loss:\t\t1.387765\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2378 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392759\n",
      "  validation loss:\t\t1.387888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2379 of 5000 took 0.022s\n",
      "  training loss:\t\t1.360818\n",
      "  validation loss:\t\t1.388412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2380 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399443\n",
      "  validation loss:\t\t1.388555\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2381 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390405\n",
      "  validation loss:\t\t1.388638\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2382 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377470\n",
      "  validation loss:\t\t1.388806\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2383 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370185\n",
      "  validation loss:\t\t1.389057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2384 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393684\n",
      "  validation loss:\t\t1.389012\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2385 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399290\n",
      "  validation loss:\t\t1.388846\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2386 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392090\n",
      "  validation loss:\t\t1.388267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2387 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377733\n",
      "  validation loss:\t\t1.387853\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2388 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408782\n",
      "  validation loss:\t\t1.387563\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2389 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373384\n",
      "  validation loss:\t\t1.387501\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2390 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379758\n",
      "  validation loss:\t\t1.387559\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2391 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388005\n",
      "  validation loss:\t\t1.387697\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2392 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392195\n",
      "  validation loss:\t\t1.387800\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2393 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395957\n",
      "  validation loss:\t\t1.387829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2394 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377531\n",
      "  validation loss:\t\t1.388034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2395 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376892\n",
      "  validation loss:\t\t1.388454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2396 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389120\n",
      "  validation loss:\t\t1.388496\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2397 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392133\n",
      "  validation loss:\t\t1.388474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2398 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394976\n",
      "  validation loss:\t\t1.388352\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2399 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391047\n",
      "  validation loss:\t\t1.387888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2400 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396512\n",
      "  validation loss:\t\t1.387841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2401 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387799\n",
      "  validation loss:\t\t1.387806\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2402 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398668\n",
      "  validation loss:\t\t1.387775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2403 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400184\n",
      "  validation loss:\t\t1.387389\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2404 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397388\n",
      "  validation loss:\t\t1.387224\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2405 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398683\n",
      "  validation loss:\t\t1.387280\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2406 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386841\n",
      "  validation loss:\t\t1.387229\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2407 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394864\n",
      "  validation loss:\t\t1.387260\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2408 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409035\n",
      "  validation loss:\t\t1.387246\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2409 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394773\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2410 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374886\n",
      "  validation loss:\t\t1.387222\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2411 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386794\n",
      "  validation loss:\t\t1.387143\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2412 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403349\n",
      "  validation loss:\t\t1.387111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2413 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392800\n",
      "  validation loss:\t\t1.387180\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2414 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406927\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2415 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401911\n",
      "  validation loss:\t\t1.387285\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2416 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381621\n",
      "  validation loss:\t\t1.387482\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2417 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392956\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2418 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389888\n",
      "  validation loss:\t\t1.387748\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2419 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382694\n",
      "  validation loss:\t\t1.387900\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2420 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389336\n",
      "  validation loss:\t\t1.388062\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2421 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405552\n",
      "  validation loss:\t\t1.388029\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2422 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386581\n",
      "  validation loss:\t\t1.388165\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2423 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390977\n",
      "  validation loss:\t\t1.388024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2424 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388826\n",
      "  validation loss:\t\t1.388131\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2425 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381761\n",
      "  validation loss:\t\t1.388187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2426 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399419\n",
      "  validation loss:\t\t1.388078\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2427 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389901\n",
      "  validation loss:\t\t1.388198\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2428 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401241\n",
      "  validation loss:\t\t1.387966\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2429 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394290\n",
      "  validation loss:\t\t1.387882\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2430 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378231\n",
      "  validation loss:\t\t1.388015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2431 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379819\n",
      "  validation loss:\t\t1.387959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2432 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398036\n",
      "  validation loss:\t\t1.388112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2433 of 5000 took 0.024s\n",
      "  training loss:\t\t1.403076\n",
      "  validation loss:\t\t1.388064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2434 of 5000 took 0.024s\n",
      "  training loss:\t\t1.408254\n",
      "  validation loss:\t\t1.387894\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2435 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407363\n",
      "  validation loss:\t\t1.387716\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2436 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375831\n",
      "  validation loss:\t\t1.387671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2437 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403687\n",
      "  validation loss:\t\t1.387395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2438 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377026\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2439 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400818\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2440 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399356\n",
      "  validation loss:\t\t1.387250\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2441 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401780\n",
      "  validation loss:\t\t1.387443\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2442 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387450\n",
      "  validation loss:\t\t1.387703\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2443 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392072\n",
      "  validation loss:\t\t1.388030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2444 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377026\n",
      "  validation loss:\t\t1.388199\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2445 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379530\n",
      "  validation loss:\t\t1.388559\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2446 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392752\n",
      "  validation loss:\t\t1.388457\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2447 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386726\n",
      "  validation loss:\t\t1.388462\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2448 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389974\n",
      "  validation loss:\t\t1.388394\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2449 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380936\n",
      "  validation loss:\t\t1.388335\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2450 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407441\n",
      "  validation loss:\t\t1.388320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2451 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385170\n",
      "  validation loss:\t\t1.388072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2452 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404370\n",
      "  validation loss:\t\t1.387993\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2453 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385971\n",
      "  validation loss:\t\t1.387625\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2454 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383101\n",
      "  validation loss:\t\t1.387629\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2455 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385872\n",
      "  validation loss:\t\t1.387790\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2456 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401718\n",
      "  validation loss:\t\t1.387842\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2457 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383012\n",
      "  validation loss:\t\t1.388098\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2458 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400016\n",
      "  validation loss:\t\t1.388037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2459 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387857\n",
      "  validation loss:\t\t1.388085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2460 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396085\n",
      "  validation loss:\t\t1.388208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2461 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397075\n",
      "  validation loss:\t\t1.388273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2462 of 5000 took 0.023s\n",
      "  training loss:\t\t1.414390\n",
      "  validation loss:\t\t1.388074\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2463 of 5000 took 0.024s\n",
      "  training loss:\t\t1.377236\n",
      "  validation loss:\t\t1.388011\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2464 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400250\n",
      "  validation loss:\t\t1.387804\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2465 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381230\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2466 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386937\n",
      "  validation loss:\t\t1.387648\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2467 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380902\n",
      "  validation loss:\t\t1.387455\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2468 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400903\n",
      "  validation loss:\t\t1.387339\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2469 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397561\n",
      "  validation loss:\t\t1.387253\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2470 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377461\n",
      "  validation loss:\t\t1.387281\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2471 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383206\n",
      "  validation loss:\t\t1.387491\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2472 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383841\n",
      "  validation loss:\t\t1.387827\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2473 of 5000 took 0.025s\n",
      "  training loss:\t\t1.399561\n",
      "  validation loss:\t\t1.388037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2474 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396765\n",
      "  validation loss:\t\t1.388087\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2475 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404757\n",
      "  validation loss:\t\t1.388096\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2476 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397272\n",
      "  validation loss:\t\t1.387840\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2477 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384748\n",
      "  validation loss:\t\t1.387655\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2478 of 5000 took 0.024s\n",
      "  training loss:\t\t1.379757\n",
      "  validation loss:\t\t1.387685\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2479 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377338\n",
      "  validation loss:\t\t1.387671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2480 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390774\n",
      "  validation loss:\t\t1.387463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2481 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384435\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2482 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377292\n",
      "  validation loss:\t\t1.387200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2483 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394152\n",
      "  validation loss:\t\t1.387084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2484 of 5000 took 0.023s\n",
      "  training loss:\t\t1.373351\n",
      "  validation loss:\t\t1.387064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2485 of 5000 took 0.023s\n",
      "  training loss:\t\t1.372242\n",
      "  validation loss:\t\t1.387111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2486 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379520\n",
      "  validation loss:\t\t1.387230\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2487 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398291\n",
      "  validation loss:\t\t1.387365\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2488 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396232\n",
      "  validation loss:\t\t1.387403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2489 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393522\n",
      "  validation loss:\t\t1.387438\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2490 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397468\n",
      "  validation loss:\t\t1.387436\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2491 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391312\n",
      "  validation loss:\t\t1.387181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2492 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379562\n",
      "  validation loss:\t\t1.387202\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2493 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388079\n",
      "  validation loss:\t\t1.387220\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2494 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377828\n",
      "  validation loss:\t\t1.387353\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2495 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385419\n",
      "  validation loss:\t\t1.387611\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2496 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385161\n",
      "  validation loss:\t\t1.387711\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2497 of 5000 took 0.023s\n",
      "  training loss:\t\t1.407686\n",
      "  validation loss:\t\t1.387635\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2498 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404300\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2499 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378547\n",
      "  validation loss:\t\t1.387357\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2500 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383507\n",
      "  validation loss:\t\t1.387270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2501 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392214\n",
      "  validation loss:\t\t1.387164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2502 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395136\n",
      "  validation loss:\t\t1.387150\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2503 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386599\n",
      "  validation loss:\t\t1.387091\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2504 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389845\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2505 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382984\n",
      "  validation loss:\t\t1.387291\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2506 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408934\n",
      "  validation loss:\t\t1.387345\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2507 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402048\n",
      "  validation loss:\t\t1.387417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2508 of 5000 took 0.024s\n",
      "  training loss:\t\t1.381486\n",
      "  validation loss:\t\t1.387528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2509 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386750\n",
      "  validation loss:\t\t1.387606\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2510 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398842\n",
      "  validation loss:\t\t1.387678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2511 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400020\n",
      "  validation loss:\t\t1.387772\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2512 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391130\n",
      "  validation loss:\t\t1.387932\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2513 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386682\n",
      "  validation loss:\t\t1.388272\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2514 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380768\n",
      "  validation loss:\t\t1.388214\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2515 of 5000 took 0.025s\n",
      "  training loss:\t\t1.387345\n",
      "  validation loss:\t\t1.387821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2516 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382520\n",
      "  validation loss:\t\t1.387829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2517 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384561\n",
      "  validation loss:\t\t1.387920\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2518 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398812\n",
      "  validation loss:\t\t1.387803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2519 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383109\n",
      "  validation loss:\t\t1.388058\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2520 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380875\n",
      "  validation loss:\t\t1.388034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2521 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390136\n",
      "  validation loss:\t\t1.388167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2522 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397963\n",
      "  validation loss:\t\t1.388208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2523 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385384\n",
      "  validation loss:\t\t1.388199\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2524 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378242\n",
      "  validation loss:\t\t1.388307\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2525 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391297\n",
      "  validation loss:\t\t1.388642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2526 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394251\n",
      "  validation loss:\t\t1.388952\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2527 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411532\n",
      "  validation loss:\t\t1.389045\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2528 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398785\n",
      "  validation loss:\t\t1.389264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2529 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395707\n",
      "  validation loss:\t\t1.389277\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2530 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409240\n",
      "  validation loss:\t\t1.389162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2531 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400293\n",
      "  validation loss:\t\t1.389107\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2532 of 5000 took 0.021s\n",
      "  training loss:\t\t1.408617\n",
      "  validation loss:\t\t1.388796\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2533 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393776\n",
      "  validation loss:\t\t1.388276\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2534 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396759\n",
      "  validation loss:\t\t1.387814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2535 of 5000 took 0.024s\n",
      "  training loss:\t\t1.407565\n",
      "  validation loss:\t\t1.387366\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2536 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387106\n",
      "  validation loss:\t\t1.387227\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2537 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395013\n",
      "  validation loss:\t\t1.387139\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2538 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376653\n",
      "  validation loss:\t\t1.387144\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2539 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389256\n",
      "  validation loss:\t\t1.387181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2540 of 5000 took 0.021s\n",
      "  training loss:\t\t1.403074\n",
      "  validation loss:\t\t1.387263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2541 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393424\n",
      "  validation loss:\t\t1.387423\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2542 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388243\n",
      "  validation loss:\t\t1.387589\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2543 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381570\n",
      "  validation loss:\t\t1.387832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2544 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389007\n",
      "  validation loss:\t\t1.387845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2545 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378330\n",
      "  validation loss:\t\t1.387627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2546 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379403\n",
      "  validation loss:\t\t1.387591\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2547 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382728\n",
      "  validation loss:\t\t1.387444\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2548 of 5000 took 0.025s\n",
      "  training loss:\t\t1.399652\n",
      "  validation loss:\t\t1.387267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2549 of 5000 took 0.026s\n",
      "  training loss:\t\t1.405800\n",
      "  validation loss:\t\t1.387193\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2550 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392028\n",
      "  validation loss:\t\t1.387257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2551 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379892\n",
      "  validation loss:\t\t1.387445\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2552 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407972\n",
      "  validation loss:\t\t1.387416\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2553 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379597\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2554 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399147\n",
      "  validation loss:\t\t1.387766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2555 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407205\n",
      "  validation loss:\t\t1.387870\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2556 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385761\n",
      "  validation loss:\t\t1.387744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2557 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398586\n",
      "  validation loss:\t\t1.387568\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2558 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379291\n",
      "  validation loss:\t\t1.387357\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2559 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381025\n",
      "  validation loss:\t\t1.387179\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2560 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391719\n",
      "  validation loss:\t\t1.387175\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2561 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401515\n",
      "  validation loss:\t\t1.387259\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2562 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389802\n",
      "  validation loss:\t\t1.387331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2563 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394049\n",
      "  validation loss:\t\t1.387663\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2564 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399089\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2565 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385954\n",
      "  validation loss:\t\t1.387597\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2566 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384848\n",
      "  validation loss:\t\t1.387532\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2567 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380924\n",
      "  validation loss:\t\t1.387584\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2568 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381944\n",
      "  validation loss:\t\t1.387387\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2569 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392032\n",
      "  validation loss:\t\t1.387381\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2570 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380058\n",
      "  validation loss:\t\t1.387487\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2571 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392306\n",
      "  validation loss:\t\t1.387677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2572 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395299\n",
      "  validation loss:\t\t1.387794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2573 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402111\n",
      "  validation loss:\t\t1.387601\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2574 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384808\n",
      "  validation loss:\t\t1.387694\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2575 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384565\n",
      "  validation loss:\t\t1.387949\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2576 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387964\n",
      "  validation loss:\t\t1.387959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2577 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401060\n",
      "  validation loss:\t\t1.388037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2578 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379451\n",
      "  validation loss:\t\t1.387909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2579 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383399\n",
      "  validation loss:\t\t1.387910\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2580 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387894\n",
      "  validation loss:\t\t1.388084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2581 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384547\n",
      "  validation loss:\t\t1.387929\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2582 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374466\n",
      "  validation loss:\t\t1.388072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2583 of 5000 took 0.024s\n",
      "  training loss:\t\t1.403791\n",
      "  validation loss:\t\t1.388110\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2584 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390012\n",
      "  validation loss:\t\t1.388020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2585 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378256\n",
      "  validation loss:\t\t1.387782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2586 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382711\n",
      "  validation loss:\t\t1.387664\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2587 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389316\n",
      "  validation loss:\t\t1.387260\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2588 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383585\n",
      "  validation loss:\t\t1.387077\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2589 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397316\n",
      "  validation loss:\t\t1.387067\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2590 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390345\n",
      "  validation loss:\t\t1.387160\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2591 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387543\n",
      "  validation loss:\t\t1.387238\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2592 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377612\n",
      "  validation loss:\t\t1.387343\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2593 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382598\n",
      "  validation loss:\t\t1.387516\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2594 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389920\n",
      "  validation loss:\t\t1.387730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2595 of 5000 took 0.023s\n",
      "  training loss:\t\t1.410074\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2596 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394403\n",
      "  validation loss:\t\t1.387313\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2597 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382006\n",
      "  validation loss:\t\t1.387262\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2598 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385752\n",
      "  validation loss:\t\t1.387238\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2599 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399310\n",
      "  validation loss:\t\t1.387167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2600 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384320\n",
      "  validation loss:\t\t1.387184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2601 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392222\n",
      "  validation loss:\t\t1.387130\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2602 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393367\n",
      "  validation loss:\t\t1.387191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2603 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388607\n",
      "  validation loss:\t\t1.387034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2604 of 5000 took 0.023s\n",
      "  training loss:\t\t1.404184\n",
      "  validation loss:\t\t1.387036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2605 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403921\n",
      "  validation loss:\t\t1.387059\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2606 of 5000 took 0.022s\n",
      "  training loss:\t\t1.371192\n",
      "  validation loss:\t\t1.387118\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2607 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400482\n",
      "  validation loss:\t\t1.387289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2608 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390417\n",
      "  validation loss:\t\t1.387398\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2609 of 5000 took 0.022s\n",
      "  training loss:\t\t1.369315\n",
      "  validation loss:\t\t1.387505\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2610 of 5000 took 0.022s\n",
      "  training loss:\t\t1.413850\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2611 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383459\n",
      "  validation loss:\t\t1.387540\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2612 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384720\n",
      "  validation loss:\t\t1.387641\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2613 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398484\n",
      "  validation loss:\t\t1.387481\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2614 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384558\n",
      "  validation loss:\t\t1.387199\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2615 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387030\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2616 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396734\n",
      "  validation loss:\t\t1.387156\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2617 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396288\n",
      "  validation loss:\t\t1.387128\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2618 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390209\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2619 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383064\n",
      "  validation loss:\t\t1.387151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2620 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386465\n",
      "  validation loss:\t\t1.387328\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2621 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387523\n",
      "  validation loss:\t\t1.387537\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2622 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385305\n",
      "  validation loss:\t\t1.387755\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2623 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380652\n",
      "  validation loss:\t\t1.387782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2624 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389786\n",
      "  validation loss:\t\t1.387800\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2625 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396736\n",
      "  validation loss:\t\t1.388006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2626 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403993\n",
      "  validation loss:\t\t1.387907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2627 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397207\n",
      "  validation loss:\t\t1.387514\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2628 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400430\n",
      "  validation loss:\t\t1.387230\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2629 of 5000 took 0.024s\n",
      "  training loss:\t\t1.371818\n",
      "  validation loss:\t\t1.387140\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2630 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402291\n",
      "  validation loss:\t\t1.386998\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2631 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394948\n",
      "  validation loss:\t\t1.387020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2632 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397524\n",
      "  validation loss:\t\t1.387193\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2633 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405436\n",
      "  validation loss:\t\t1.387410\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2634 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390609\n",
      "  validation loss:\t\t1.387614\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2635 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391546\n",
      "  validation loss:\t\t1.387647\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2636 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391045\n",
      "  validation loss:\t\t1.387697\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2637 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398515\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2638 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393493\n",
      "  validation loss:\t\t1.387805\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2639 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410051\n",
      "  validation loss:\t\t1.387766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2640 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389218\n",
      "  validation loss:\t\t1.387721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2641 of 5000 took 0.024s\n",
      "  training loss:\t\t1.401206\n",
      "  validation loss:\t\t1.387552\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2642 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374905\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2643 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394801\n",
      "  validation loss:\t\t1.387660\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2644 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399575\n",
      "  validation loss:\t\t1.387569\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2645 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399974\n",
      "  validation loss:\t\t1.387486\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2646 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385187\n",
      "  validation loss:\t\t1.387304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2647 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391730\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2648 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391048\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2649 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382018\n",
      "  validation loss:\t\t1.387088\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2650 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384550\n",
      "  validation loss:\t\t1.387128\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2651 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410210\n",
      "  validation loss:\t\t1.387106\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2652 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393743\n",
      "  validation loss:\t\t1.387178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2653 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385882\n",
      "  validation loss:\t\t1.387169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2654 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395242\n",
      "  validation loss:\t\t1.387223\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2655 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389612\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2656 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379775\n",
      "  validation loss:\t\t1.387690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2657 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397554\n",
      "  validation loss:\t\t1.387851\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2658 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397497\n",
      "  validation loss:\t\t1.387809\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2659 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394672\n",
      "  validation loss:\t\t1.387670\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2660 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393090\n",
      "  validation loss:\t\t1.387690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2661 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396470\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2662 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399619\n",
      "  validation loss:\t\t1.387586\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2663 of 5000 took 0.023s\n",
      "  training loss:\t\t1.405666\n",
      "  validation loss:\t\t1.387459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2664 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392679\n",
      "  validation loss:\t\t1.387377\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2665 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393765\n",
      "  validation loss:\t\t1.387380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2666 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392521\n",
      "  validation loss:\t\t1.387548\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2667 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389053\n",
      "  validation loss:\t\t1.387677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2668 of 5000 took 0.023s\n",
      "  training loss:\t\t1.372087\n",
      "  validation loss:\t\t1.388069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2669 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395537\n",
      "  validation loss:\t\t1.388290\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2670 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389475\n",
      "  validation loss:\t\t1.388414\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2671 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385808\n",
      "  validation loss:\t\t1.388396\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2672 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380530\n",
      "  validation loss:\t\t1.388373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2673 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383987\n",
      "  validation loss:\t\t1.388437\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2674 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381435\n",
      "  validation loss:\t\t1.388447\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2675 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391730\n",
      "  validation loss:\t\t1.388155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2676 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397569\n",
      "  validation loss:\t\t1.388056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2677 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381261\n",
      "  validation loss:\t\t1.387988\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2678 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400006\n",
      "  validation loss:\t\t1.387705\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2679 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389242\n",
      "  validation loss:\t\t1.387507\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2680 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402681\n",
      "  validation loss:\t\t1.387518\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2681 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393032\n",
      "  validation loss:\t\t1.387547\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2682 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385926\n",
      "  validation loss:\t\t1.387784\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2683 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395957\n",
      "  validation loss:\t\t1.387920\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2684 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403185\n",
      "  validation loss:\t\t1.388222\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2685 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392390\n",
      "  validation loss:\t\t1.388329\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2686 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400232\n",
      "  validation loss:\t\t1.388328\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2687 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396536\n",
      "  validation loss:\t\t1.388260\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2688 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395470\n",
      "  validation loss:\t\t1.387936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2689 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392028\n",
      "  validation loss:\t\t1.387831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2690 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386693\n",
      "  validation loss:\t\t1.387543\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2691 of 5000 took 0.022s\n",
      "  training loss:\t\t1.408928\n",
      "  validation loss:\t\t1.387625\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2692 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391650\n",
      "  validation loss:\t\t1.387545\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2693 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397307\n",
      "  validation loss:\t\t1.387334\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2694 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387854\n",
      "  validation loss:\t\t1.387211\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2695 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385730\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2696 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381990\n",
      "  validation loss:\t\t1.386967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2697 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390588\n",
      "  validation loss:\t\t1.387081\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2698 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397745\n",
      "  validation loss:\t\t1.387133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2699 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389664\n",
      "  validation loss:\t\t1.387272\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2700 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389581\n",
      "  validation loss:\t\t1.387301\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2701 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398010\n",
      "  validation loss:\t\t1.387329\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2702 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391683\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2703 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387222\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2704 of 5000 took 0.023s\n",
      "  training loss:\t\t1.411670\n",
      "  validation loss:\t\t1.387069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2705 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393169\n",
      "  validation loss:\t\t1.387057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2706 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397000\n",
      "  validation loss:\t\t1.387030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2707 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393667\n",
      "  validation loss:\t\t1.387100\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2708 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394430\n",
      "  validation loss:\t\t1.387093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2709 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393470\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2710 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386085\n",
      "  validation loss:\t\t1.387249\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2711 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387534\n",
      "  validation loss:\t\t1.387544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2712 of 5000 took 0.023s\n",
      "  training loss:\t\t1.364254\n",
      "  validation loss:\t\t1.388016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2713 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391215\n",
      "  validation loss:\t\t1.388293\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2714 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391494\n",
      "  validation loss:\t\t1.388412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2715 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397917\n",
      "  validation loss:\t\t1.388086\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2716 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396559\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2717 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382800\n",
      "  validation loss:\t\t1.387471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2718 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391053\n",
      "  validation loss:\t\t1.387660\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2719 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406604\n",
      "  validation loss:\t\t1.387586\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2720 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386425\n",
      "  validation loss:\t\t1.387568\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2721 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401063\n",
      "  validation loss:\t\t1.387790\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2722 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377112\n",
      "  validation loss:\t\t1.387745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2723 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378432\n",
      "  validation loss:\t\t1.387791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2724 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380954\n",
      "  validation loss:\t\t1.387796\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2725 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379129\n",
      "  validation loss:\t\t1.387794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2726 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388726\n",
      "  validation loss:\t\t1.387651\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2727 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396939\n",
      "  validation loss:\t\t1.387599\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2728 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389775\n",
      "  validation loss:\t\t1.387656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2729 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388874\n",
      "  validation loss:\t\t1.387382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2730 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390763\n",
      "  validation loss:\t\t1.387471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2731 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388769\n",
      "  validation loss:\t\t1.387271\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2732 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387983\n",
      "  validation loss:\t\t1.387216\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2733 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394565\n",
      "  validation loss:\t\t1.387269\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2734 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375263\n",
      "  validation loss:\t\t1.387688\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2735 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383905\n",
      "  validation loss:\t\t1.388341\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2736 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399341\n",
      "  validation loss:\t\t1.388623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2737 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389898\n",
      "  validation loss:\t\t1.389060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2738 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390629\n",
      "  validation loss:\t\t1.389119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2739 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386250\n",
      "  validation loss:\t\t1.389395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2740 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395887\n",
      "  validation loss:\t\t1.389571\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2741 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403366\n",
      "  validation loss:\t\t1.389037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2742 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391619\n",
      "  validation loss:\t\t1.388677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2743 of 5000 took 0.022s\n",
      "  training loss:\t\t1.414560\n",
      "  validation loss:\t\t1.388052\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2744 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390331\n",
      "  validation loss:\t\t1.387835\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2745 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400186\n",
      "  validation loss:\t\t1.387397\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2746 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385250\n",
      "  validation loss:\t\t1.387112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2747 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407339\n",
      "  validation loss:\t\t1.386952\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2748 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392413\n",
      "  validation loss:\t\t1.387096\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2749 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390589\n",
      "  validation loss:\t\t1.387677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2750 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382329\n",
      "  validation loss:\t\t1.388824\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2751 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380260\n",
      "  validation loss:\t\t1.390172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2752 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389502\n",
      "  validation loss:\t\t1.391757\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2753 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393874\n",
      "  validation loss:\t\t1.392997\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2754 of 5000 took 0.024s\n",
      "  training loss:\t\t1.379155\n",
      "  validation loss:\t\t1.394242\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2755 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405825\n",
      "  validation loss:\t\t1.394333\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2756 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402100\n",
      "  validation loss:\t\t1.394190\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2757 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406664\n",
      "  validation loss:\t\t1.392640\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2758 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400841\n",
      "  validation loss:\t\t1.390716\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2759 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391243\n",
      "  validation loss:\t\t1.389120\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2760 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375201\n",
      "  validation loss:\t\t1.388053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2761 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380742\n",
      "  validation loss:\t\t1.387541\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2762 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386718\n",
      "  validation loss:\t\t1.387315\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2763 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396073\n",
      "  validation loss:\t\t1.387286\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2764 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386819\n",
      "  validation loss:\t\t1.387503\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2765 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383173\n",
      "  validation loss:\t\t1.388012\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2766 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393475\n",
      "  validation loss:\t\t1.388419\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2767 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386132\n",
      "  validation loss:\t\t1.388848\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2768 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402462\n",
      "  validation loss:\t\t1.389087\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2769 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402208\n",
      "  validation loss:\t\t1.388701\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2770 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390887\n",
      "  validation loss:\t\t1.388357\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2771 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385229\n",
      "  validation loss:\t\t1.388383\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2772 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387912\n",
      "  validation loss:\t\t1.388698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2773 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390236\n",
      "  validation loss:\t\t1.388409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2774 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383291\n",
      "  validation loss:\t\t1.388216\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2775 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387596\n",
      "  validation loss:\t\t1.388046\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2776 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384206\n",
      "  validation loss:\t\t1.387857\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2777 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396265\n",
      "  validation loss:\t\t1.388146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2778 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392551\n",
      "  validation loss:\t\t1.388397\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2779 of 5000 took 0.023s\n",
      "  training loss:\t\t1.372692\n",
      "  validation loss:\t\t1.388627\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2780 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400259\n",
      "  validation loss:\t\t1.388622\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2781 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388491\n",
      "  validation loss:\t\t1.388664\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2782 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397810\n",
      "  validation loss:\t\t1.388390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2783 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387617\n",
      "  validation loss:\t\t1.388248\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2784 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383923\n",
      "  validation loss:\t\t1.388544\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2785 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397763\n",
      "  validation loss:\t\t1.388473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2786 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403627\n",
      "  validation loss:\t\t1.388403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2787 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387279\n",
      "  validation loss:\t\t1.388354\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2788 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384056\n",
      "  validation loss:\t\t1.388466\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2789 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384971\n",
      "  validation loss:\t\t1.388497\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2790 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387325\n",
      "  validation loss:\t\t1.388333\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2791 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398459\n",
      "  validation loss:\t\t1.387896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2792 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387406\n",
      "  validation loss:\t\t1.387829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2793 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389768\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2794 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381483\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2795 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386209\n",
      "  validation loss:\t\t1.387058\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2796 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389703\n",
      "  validation loss:\t\t1.387049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2797 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395838\n",
      "  validation loss:\t\t1.387168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2798 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398666\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2799 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379884\n",
      "  validation loss:\t\t1.387446\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2800 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386759\n",
      "  validation loss:\t\t1.387684\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2801 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403476\n",
      "  validation loss:\t\t1.387733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2802 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382672\n",
      "  validation loss:\t\t1.387860\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2803 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392366\n",
      "  validation loss:\t\t1.388101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2804 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390054\n",
      "  validation loss:\t\t1.388049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2805 of 5000 took 0.024s\n",
      "  training loss:\t\t1.374201\n",
      "  validation loss:\t\t1.388366\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2806 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388137\n",
      "  validation loss:\t\t1.388908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2807 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382546\n",
      "  validation loss:\t\t1.389447\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2808 of 5000 took 0.022s\n",
      "  training loss:\t\t1.409320\n",
      "  validation loss:\t\t1.389232\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2809 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397540\n",
      "  validation loss:\t\t1.389044\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2810 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401441\n",
      "  validation loss:\t\t1.388674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2811 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397885\n",
      "  validation loss:\t\t1.388229\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2812 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389938\n",
      "  validation loss:\t\t1.388116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2813 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398805\n",
      "  validation loss:\t\t1.387973\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2814 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387256\n",
      "  validation loss:\t\t1.387717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2815 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395416\n",
      "  validation loss:\t\t1.387416\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2816 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381097\n",
      "  validation loss:\t\t1.387494\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2817 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391227\n",
      "  validation loss:\t\t1.387516\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2818 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384842\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2819 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389018\n",
      "  validation loss:\t\t1.387313\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2820 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396529\n",
      "  validation loss:\t\t1.387324\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2821 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401397\n",
      "  validation loss:\t\t1.387056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2822 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388389\n",
      "  validation loss:\t\t1.386959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2823 of 5000 took 0.022s\n",
      "  training loss:\t\t1.369868\n",
      "  validation loss:\t\t1.386994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2824 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388469\n",
      "  validation loss:\t\t1.386968\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2825 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378392\n",
      "  validation loss:\t\t1.387036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2826 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389147\n",
      "  validation loss:\t\t1.387049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2827 of 5000 took 0.021s\n",
      "  training loss:\t\t1.377316\n",
      "  validation loss:\t\t1.387167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2828 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388791\n",
      "  validation loss:\t\t1.387279\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2829 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395069\n",
      "  validation loss:\t\t1.387491\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2830 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379636\n",
      "  validation loss:\t\t1.387453\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2831 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388931\n",
      "  validation loss:\t\t1.387318\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2832 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384891\n",
      "  validation loss:\t\t1.387350\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2833 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393616\n",
      "  validation loss:\t\t1.387198\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2834 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383439\n",
      "  validation loss:\t\t1.387166\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2835 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391544\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2836 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399213\n",
      "  validation loss:\t\t1.387235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2837 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391208\n",
      "  validation loss:\t\t1.387236\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2838 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379190\n",
      "  validation loss:\t\t1.387273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2839 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391809\n",
      "  validation loss:\t\t1.387466\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2840 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394883\n",
      "  validation loss:\t\t1.387445\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2841 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388101\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2842 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385424\n",
      "  validation loss:\t\t1.387861\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2843 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403636\n",
      "  validation loss:\t\t1.388030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2844 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379408\n",
      "  validation loss:\t\t1.388315\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2845 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391008\n",
      "  validation loss:\t\t1.388478\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2846 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389358\n",
      "  validation loss:\t\t1.388423\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2847 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378516\n",
      "  validation loss:\t\t1.388489\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2848 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390381\n",
      "  validation loss:\t\t1.388272\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2849 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387504\n",
      "  validation loss:\t\t1.388508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2850 of 5000 took 0.023s\n",
      "  training loss:\t\t1.369866\n",
      "  validation loss:\t\t1.388528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2851 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403959\n",
      "  validation loss:\t\t1.388175\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2852 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383893\n",
      "  validation loss:\t\t1.388169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2853 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393599\n",
      "  validation loss:\t\t1.387603\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2854 of 5000 took 0.021s\n",
      "  training loss:\t\t1.378115\n",
      "  validation loss:\t\t1.387422\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2855 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388604\n",
      "  validation loss:\t\t1.387303\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2856 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397990\n",
      "  validation loss:\t\t1.387170\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2857 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393077\n",
      "  validation loss:\t\t1.387124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2858 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397006\n",
      "  validation loss:\t\t1.387083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2859 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389610\n",
      "  validation loss:\t\t1.387114\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2860 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379562\n",
      "  validation loss:\t\t1.387255\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2861 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389872\n",
      "  validation loss:\t\t1.387222\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2862 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402890\n",
      "  validation loss:\t\t1.387273\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2863 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390866\n",
      "  validation loss:\t\t1.387278\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2864 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384368\n",
      "  validation loss:\t\t1.387278\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2865 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384142\n",
      "  validation loss:\t\t1.387420\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2866 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395475\n",
      "  validation loss:\t\t1.387801\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2867 of 5000 took 0.024s\n",
      "  training loss:\t\t1.374546\n",
      "  validation loss:\t\t1.388166\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2868 of 5000 took 0.025s\n",
      "  training loss:\t\t1.379125\n",
      "  validation loss:\t\t1.388472\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2869 of 5000 took 0.026s\n",
      "  training loss:\t\t1.381821\n",
      "  validation loss:\t\t1.388800\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2870 of 5000 took 0.026s\n",
      "  training loss:\t\t1.388643\n",
      "  validation loss:\t\t1.389119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2871 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400718\n",
      "  validation loss:\t\t1.388889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2872 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400172\n",
      "  validation loss:\t\t1.388907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2873 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392048\n",
      "  validation loss:\t\t1.388744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2874 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400733\n",
      "  validation loss:\t\t1.388229\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2875 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390433\n",
      "  validation loss:\t\t1.387493\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2876 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396114\n",
      "  validation loss:\t\t1.387156\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2877 of 5000 took 0.025s\n",
      "  training loss:\t\t1.387620\n",
      "  validation loss:\t\t1.387028\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2878 of 5000 took 0.026s\n",
      "  training loss:\t\t1.385021\n",
      "  validation loss:\t\t1.386957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2879 of 5000 took 0.053s\n",
      "  training loss:\t\t1.383932\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2880 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390516\n",
      "  validation loss:\t\t1.386932\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2881 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377487\n",
      "  validation loss:\t\t1.386987\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2882 of 5000 took 0.043s\n",
      "  training loss:\t\t1.394860\n",
      "  validation loss:\t\t1.387065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2883 of 5000 took 0.043s\n",
      "  training loss:\t\t1.381315\n",
      "  validation loss:\t\t1.387186\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2884 of 5000 took 0.025s\n",
      "  training loss:\t\t1.401833\n",
      "  validation loss:\t\t1.387101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2885 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402229\n",
      "  validation loss:\t\t1.387159\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2886 of 5000 took 0.030s\n",
      "  training loss:\t\t1.381304\n",
      "  validation loss:\t\t1.387245\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2887 of 5000 took 0.027s\n",
      "  training loss:\t\t1.392789\n",
      "  validation loss:\t\t1.387221\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2888 of 5000 took 0.025s\n",
      "  training loss:\t\t1.397710\n",
      "  validation loss:\t\t1.387115\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2889 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374863\n",
      "  validation loss:\t\t1.387195\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2890 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382948\n",
      "  validation loss:\t\t1.387288\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2891 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389637\n",
      "  validation loss:\t\t1.387336\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2892 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394957\n",
      "  validation loss:\t\t1.387449\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2893 of 5000 took 0.029s\n",
      "  training loss:\t\t1.388750\n",
      "  validation loss:\t\t1.387689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2894 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378832\n",
      "  validation loss:\t\t1.387774\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2895 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404629\n",
      "  validation loss:\t\t1.387524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2896 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378353\n",
      "  validation loss:\t\t1.387376\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2897 of 5000 took 0.021s\n",
      "  training loss:\t\t1.403545\n",
      "  validation loss:\t\t1.387234\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2898 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393788\n",
      "  validation loss:\t\t1.387082\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2899 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391363\n",
      "  validation loss:\t\t1.387103\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2900 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388385\n",
      "  validation loss:\t\t1.387112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2901 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399230\n",
      "  validation loss:\t\t1.387107\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2902 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398257\n",
      "  validation loss:\t\t1.387255\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2903 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384978\n",
      "  validation loss:\t\t1.387249\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2904 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392893\n",
      "  validation loss:\t\t1.387151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2905 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391005\n",
      "  validation loss:\t\t1.387094\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2906 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400984\n",
      "  validation loss:\t\t1.387044\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2907 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388069\n",
      "  validation loss:\t\t1.386987\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2908 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393690\n",
      "  validation loss:\t\t1.386966\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2909 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384916\n",
      "  validation loss:\t\t1.386960\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2910 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394182\n",
      "  validation loss:\t\t1.386979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2911 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389503\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2912 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392803\n",
      "  validation loss:\t\t1.387185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2913 of 5000 took 0.025s\n",
      "  training loss:\t\t1.396300\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2914 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398367\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2915 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389938\n",
      "  validation loss:\t\t1.387292\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2916 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380529\n",
      "  validation loss:\t\t1.387454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2917 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387722\n",
      "  validation loss:\t\t1.387673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2918 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393963\n",
      "  validation loss:\t\t1.387729\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2919 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386882\n",
      "  validation loss:\t\t1.387883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2920 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384267\n",
      "  validation loss:\t\t1.387971\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2921 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393833\n",
      "  validation loss:\t\t1.387987\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2922 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388076\n",
      "  validation loss:\t\t1.387734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2923 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388067\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2924 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390013\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2925 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390052\n",
      "  validation loss:\t\t1.387619\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2926 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394379\n",
      "  validation loss:\t\t1.387948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2927 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399789\n",
      "  validation loss:\t\t1.388153\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2928 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388562\n",
      "  validation loss:\t\t1.388231\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2929 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394668\n",
      "  validation loss:\t\t1.388537\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2930 of 5000 took 0.023s\n",
      "  training loss:\t\t1.406744\n",
      "  validation loss:\t\t1.388475\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2931 of 5000 took 0.024s\n",
      "  training loss:\t\t1.383638\n",
      "  validation loss:\t\t1.388378\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2932 of 5000 took 0.024s\n",
      "  training loss:\t\t1.369782\n",
      "  validation loss:\t\t1.388207\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2933 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380720\n",
      "  validation loss:\t\t1.387868\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2934 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387915\n",
      "  validation loss:\t\t1.387488\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2935 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402569\n",
      "  validation loss:\t\t1.387256\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2936 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395798\n",
      "  validation loss:\t\t1.387116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2937 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395886\n",
      "  validation loss:\t\t1.387041\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2938 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395021\n",
      "  validation loss:\t\t1.387137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2939 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392386\n",
      "  validation loss:\t\t1.387173\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2940 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392147\n",
      "  validation loss:\t\t1.387324\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2941 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390022\n",
      "  validation loss:\t\t1.387334\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2942 of 5000 took 0.024s\n",
      "  training loss:\t\t1.383271\n",
      "  validation loss:\t\t1.387407\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2943 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401838\n",
      "  validation loss:\t\t1.387320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2944 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377492\n",
      "  validation loss:\t\t1.387189\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2945 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398080\n",
      "  validation loss:\t\t1.387235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2946 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396777\n",
      "  validation loss:\t\t1.387389\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2947 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384054\n",
      "  validation loss:\t\t1.387245\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2948 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379879\n",
      "  validation loss:\t\t1.387188\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2949 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391857\n",
      "  validation loss:\t\t1.387111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2950 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388115\n",
      "  validation loss:\t\t1.387097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2951 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393994\n",
      "  validation loss:\t\t1.387057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2952 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385099\n",
      "  validation loss:\t\t1.387120\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2953 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387589\n",
      "  validation loss:\t\t1.387439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2954 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399341\n",
      "  validation loss:\t\t1.387734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2955 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378736\n",
      "  validation loss:\t\t1.387967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2956 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375546\n",
      "  validation loss:\t\t1.388225\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2957 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386894\n",
      "  validation loss:\t\t1.388430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2958 of 5000 took 0.023s\n",
      "  training loss:\t\t1.409594\n",
      "  validation loss:\t\t1.388382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2959 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385598\n",
      "  validation loss:\t\t1.388320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2960 of 5000 took 0.025s\n",
      "  training loss:\t\t1.390044\n",
      "  validation loss:\t\t1.388372\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2961 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392105\n",
      "  validation loss:\t\t1.387963\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2962 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403024\n",
      "  validation loss:\t\t1.387620\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2963 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394602\n",
      "  validation loss:\t\t1.387373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2964 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402113\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2965 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393567\n",
      "  validation loss:\t\t1.387001\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2966 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399770\n",
      "  validation loss:\t\t1.386918\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2967 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390367\n",
      "  validation loss:\t\t1.387027\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2968 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382157\n",
      "  validation loss:\t\t1.387026\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2969 of 5000 took 0.025s\n",
      "  training loss:\t\t1.376489\n",
      "  validation loss:\t\t1.387108\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2970 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388417\n",
      "  validation loss:\t\t1.387058\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2971 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400112\n",
      "  validation loss:\t\t1.387126\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2972 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394389\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2973 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381674\n",
      "  validation loss:\t\t1.387518\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2974 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391277\n",
      "  validation loss:\t\t1.387553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2975 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390346\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2976 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392477\n",
      "  validation loss:\t\t1.387453\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2977 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388471\n",
      "  validation loss:\t\t1.387481\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2978 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384253\n",
      "  validation loss:\t\t1.387459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2979 of 5000 took 0.025s\n",
      "  training loss:\t\t1.389923\n",
      "  validation loss:\t\t1.387315\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2980 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387660\n",
      "  validation loss:\t\t1.387334\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2981 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395632\n",
      "  validation loss:\t\t1.387263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2982 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400679\n",
      "  validation loss:\t\t1.387004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2983 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395520\n",
      "  validation loss:\t\t1.386959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2984 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388800\n",
      "  validation loss:\t\t1.387011\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2985 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389835\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2986 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395349\n",
      "  validation loss:\t\t1.387558\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2987 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392877\n",
      "  validation loss:\t\t1.387590\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2988 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379794\n",
      "  validation loss:\t\t1.387446\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2989 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392950\n",
      "  validation loss:\t\t1.387331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2990 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378566\n",
      "  validation loss:\t\t1.387229\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2991 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390708\n",
      "  validation loss:\t\t1.387187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2992 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402502\n",
      "  validation loss:\t\t1.387128\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2993 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390773\n",
      "  validation loss:\t\t1.387072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2994 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383732\n",
      "  validation loss:\t\t1.387019\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2995 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390151\n",
      "  validation loss:\t\t1.387022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2996 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392123\n",
      "  validation loss:\t\t1.387016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2997 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390288\n",
      "  validation loss:\t\t1.387230\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2998 of 5000 took 0.025s\n",
      "  training loss:\t\t1.389473\n",
      "  validation loss:\t\t1.387470\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 2999 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393316\n",
      "  validation loss:\t\t1.387608\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3000 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390344\n",
      "  validation loss:\t\t1.387595\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3001 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406247\n",
      "  validation loss:\t\t1.387645\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3002 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387179\n",
      "  validation loss:\t\t1.387682\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3003 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381726\n",
      "  validation loss:\t\t1.387637\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3004 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389461\n",
      "  validation loss:\t\t1.387766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3005 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387392\n",
      "  validation loss:\t\t1.387686\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3006 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394781\n",
      "  validation loss:\t\t1.387738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3007 of 5000 took 0.024s\n",
      "  training loss:\t\t1.399160\n",
      "  validation loss:\t\t1.387807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3008 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377180\n",
      "  validation loss:\t\t1.387783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3009 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389823\n",
      "  validation loss:\t\t1.387769\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3010 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393680\n",
      "  validation loss:\t\t1.387647\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3011 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395782\n",
      "  validation loss:\t\t1.387368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3012 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386696\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3013 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392133\n",
      "  validation loss:\t\t1.387081\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3014 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389894\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3015 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393562\n",
      "  validation loss:\t\t1.386991\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3016 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386587\n",
      "  validation loss:\t\t1.387125\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3017 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386933\n",
      "  validation loss:\t\t1.387267\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3018 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385884\n",
      "  validation loss:\t\t1.387319\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3019 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385643\n",
      "  validation loss:\t\t1.387350\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3020 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387050\n",
      "  validation loss:\t\t1.387366\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3021 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390326\n",
      "  validation loss:\t\t1.387322\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3022 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386648\n",
      "  validation loss:\t\t1.387335\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3023 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389851\n",
      "  validation loss:\t\t1.387226\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3024 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387682\n",
      "  validation loss:\t\t1.387117\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3025 of 5000 took 0.025s\n",
      "  training loss:\t\t1.387324\n",
      "  validation loss:\t\t1.387001\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3026 of 5000 took 0.026s\n",
      "  training loss:\t\t1.390015\n",
      "  validation loss:\t\t1.386996\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3027 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396564\n",
      "  validation loss:\t\t1.387022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3028 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382687\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3029 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385405\n",
      "  validation loss:\t\t1.387243\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3030 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394208\n",
      "  validation loss:\t\t1.387368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3031 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388517\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3032 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395334\n",
      "  validation loss:\t\t1.387558\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3033 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387218\n",
      "  validation loss:\t\t1.387464\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3034 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399483\n",
      "  validation loss:\t\t1.387428\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3035 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374584\n",
      "  validation loss:\t\t1.387509\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3036 of 5000 took 0.025s\n",
      "  training loss:\t\t1.390490\n",
      "  validation loss:\t\t1.387689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3037 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386958\n",
      "  validation loss:\t\t1.388118\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3038 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395139\n",
      "  validation loss:\t\t1.388181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3039 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389181\n",
      "  validation loss:\t\t1.388104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3040 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382686\n",
      "  validation loss:\t\t1.388301\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3041 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394327\n",
      "  validation loss:\t\t1.388421\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3042 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384764\n",
      "  validation loss:\t\t1.388455\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3043 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375506\n",
      "  validation loss:\t\t1.388569\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3044 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396685\n",
      "  validation loss:\t\t1.388411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3045 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389080\n",
      "  validation loss:\t\t1.388107\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3046 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377329\n",
      "  validation loss:\t\t1.387907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3047 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390315\n",
      "  validation loss:\t\t1.387632\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3048 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402484\n",
      "  validation loss:\t\t1.387362\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3049 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377596\n",
      "  validation loss:\t\t1.387119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3050 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383425\n",
      "  validation loss:\t\t1.387003\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3051 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385426\n",
      "  validation loss:\t\t1.387169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3052 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393708\n",
      "  validation loss:\t\t1.387375\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3053 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391136\n",
      "  validation loss:\t\t1.387746\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3054 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387711\n",
      "  validation loss:\t\t1.387877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3055 of 5000 took 0.024s\n",
      "  training loss:\t\t1.375596\n",
      "  validation loss:\t\t1.387875\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3056 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391498\n",
      "  validation loss:\t\t1.387942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3057 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390932\n",
      "  validation loss:\t\t1.387737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3058 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394090\n",
      "  validation loss:\t\t1.387408\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3059 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395239\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3060 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393938\n",
      "  validation loss:\t\t1.387029\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3061 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384393\n",
      "  validation loss:\t\t1.386913\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3062 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387884\n",
      "  validation loss:\t\t1.387085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3063 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387103\n",
      "  validation loss:\t\t1.387209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3064 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386040\n",
      "  validation loss:\t\t1.387253\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3065 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393387\n",
      "  validation loss:\t\t1.387529\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3066 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389538\n",
      "  validation loss:\t\t1.387928\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3067 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385946\n",
      "  validation loss:\t\t1.388330\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3068 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388313\n",
      "  validation loss:\t\t1.388223\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3069 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382989\n",
      "  validation loss:\t\t1.388250\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3070 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383679\n",
      "  validation loss:\t\t1.388711\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3071 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392438\n",
      "  validation loss:\t\t1.388819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3072 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397763\n",
      "  validation loss:\t\t1.388846\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3073 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390340\n",
      "  validation loss:\t\t1.388937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3074 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387895\n",
      "  validation loss:\t\t1.388560\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3075 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386086\n",
      "  validation loss:\t\t1.388347\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3076 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384593\n",
      "  validation loss:\t\t1.388123\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3077 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400989\n",
      "  validation loss:\t\t1.387874\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3078 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386531\n",
      "  validation loss:\t\t1.387657\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3079 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394733\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3080 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381292\n",
      "  validation loss:\t\t1.387348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3081 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378883\n",
      "  validation loss:\t\t1.387486\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3082 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390300\n",
      "  validation loss:\t\t1.387430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3083 of 5000 took 0.025s\n",
      "  training loss:\t\t1.391736\n",
      "  validation loss:\t\t1.387595\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3084 of 5000 took 0.024s\n",
      "  training loss:\t\t1.376691\n",
      "  validation loss:\t\t1.387884\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3085 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382547\n",
      "  validation loss:\t\t1.388027\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3086 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385357\n",
      "  validation loss:\t\t1.388266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3087 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388381\n",
      "  validation loss:\t\t1.388417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3088 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389474\n",
      "  validation loss:\t\t1.388345\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3089 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378368\n",
      "  validation loss:\t\t1.388184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3090 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394273\n",
      "  validation loss:\t\t1.388069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3091 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386222\n",
      "  validation loss:\t\t1.388201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3092 of 5000 took 0.025s\n",
      "  training loss:\t\t1.389231\n",
      "  validation loss:\t\t1.388263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3093 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387434\n",
      "  validation loss:\t\t1.388395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3094 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383206\n",
      "  validation loss:\t\t1.388412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3095 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378669\n",
      "  validation loss:\t\t1.388360\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3096 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380579\n",
      "  validation loss:\t\t1.388200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3097 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388733\n",
      "  validation loss:\t\t1.388184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3098 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377106\n",
      "  validation loss:\t\t1.388145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3099 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395755\n",
      "  validation loss:\t\t1.387950\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3100 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393748\n",
      "  validation loss:\t\t1.387390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3101 of 5000 took 0.025s\n",
      "  training loss:\t\t1.372980\n",
      "  validation loss:\t\t1.387082\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3102 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398017\n",
      "  validation loss:\t\t1.386894\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3103 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393606\n",
      "  validation loss:\t\t1.386868\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3104 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385601\n",
      "  validation loss:\t\t1.387051\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3105 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380614\n",
      "  validation loss:\t\t1.387160\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3106 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395077\n",
      "  validation loss:\t\t1.387290\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3107 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390999\n",
      "  validation loss:\t\t1.387373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3108 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394081\n",
      "  validation loss:\t\t1.387492\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3109 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393454\n",
      "  validation loss:\t\t1.387576\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3110 of 5000 took 0.025s\n",
      "  training loss:\t\t1.388824\n",
      "  validation loss:\t\t1.387582\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3111 of 5000 took 0.076s\n",
      "  training loss:\t\t1.386021\n",
      "  validation loss:\t\t1.387581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3112 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375351\n",
      "  validation loss:\t\t1.387931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3113 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389227\n",
      "  validation loss:\t\t1.387791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3114 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382923\n",
      "  validation loss:\t\t1.387594\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3115 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387205\n",
      "  validation loss:\t\t1.387670\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3116 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380854\n",
      "  validation loss:\t\t1.387994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3117 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384897\n",
      "  validation loss:\t\t1.388017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3118 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384958\n",
      "  validation loss:\t\t1.387934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3119 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388214\n",
      "  validation loss:\t\t1.387929\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3120 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394817\n",
      "  validation loss:\t\t1.387908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3121 of 5000 took 0.021s\n",
      "  training loss:\t\t1.372097\n",
      "  validation loss:\t\t1.388083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3122 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394676\n",
      "  validation loss:\t\t1.388178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3123 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398973\n",
      "  validation loss:\t\t1.388257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3124 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392368\n",
      "  validation loss:\t\t1.387960\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3125 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393323\n",
      "  validation loss:\t\t1.387751\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3126 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392957\n",
      "  validation loss:\t\t1.387847\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3127 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389217\n",
      "  validation loss:\t\t1.387849\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3128 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392552\n",
      "  validation loss:\t\t1.387570\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3129 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384612\n",
      "  validation loss:\t\t1.387425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3130 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389791\n",
      "  validation loss:\t\t1.387427\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3131 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391049\n",
      "  validation loss:\t\t1.387341\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3132 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401540\n",
      "  validation loss:\t\t1.387185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3133 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397457\n",
      "  validation loss:\t\t1.387031\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3134 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384498\n",
      "  validation loss:\t\t1.387009\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3135 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397705\n",
      "  validation loss:\t\t1.387015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3136 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391301\n",
      "  validation loss:\t\t1.386988\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3137 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397081\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3138 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388815\n",
      "  validation loss:\t\t1.386988\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3139 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377205\n",
      "  validation loss:\t\t1.386911\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3140 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388134\n",
      "  validation loss:\t\t1.387045\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3141 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386615\n",
      "  validation loss:\t\t1.387101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3142 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391077\n",
      "  validation loss:\t\t1.387228\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3143 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397083\n",
      "  validation loss:\t\t1.387319\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3144 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393164\n",
      "  validation loss:\t\t1.387239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3145 of 5000 took 0.025s\n",
      "  training loss:\t\t1.386924\n",
      "  validation loss:\t\t1.387209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3146 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391258\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3147 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391237\n",
      "  validation loss:\t\t1.387179\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3148 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397878\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3149 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384481\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3150 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398680\n",
      "  validation loss:\t\t1.387004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3151 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383518\n",
      "  validation loss:\t\t1.386943\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3152 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385687\n",
      "  validation loss:\t\t1.387010\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3153 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384627\n",
      "  validation loss:\t\t1.387057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3154 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382183\n",
      "  validation loss:\t\t1.387203\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3155 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393664\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3156 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377095\n",
      "  validation loss:\t\t1.387218\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3157 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385389\n",
      "  validation loss:\t\t1.387330\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3158 of 5000 took 0.021s\n",
      "  training loss:\t\t1.372044\n",
      "  validation loss:\t\t1.387301\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3159 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394268\n",
      "  validation loss:\t\t1.387121\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3160 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392257\n",
      "  validation loss:\t\t1.387132\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3161 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387475\n",
      "  validation loss:\t\t1.387111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3162 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390687\n",
      "  validation loss:\t\t1.387205\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3163 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377020\n",
      "  validation loss:\t\t1.387304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3164 of 5000 took 0.025s\n",
      "  training loss:\t\t1.395498\n",
      "  validation loss:\t\t1.387219\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3165 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385360\n",
      "  validation loss:\t\t1.387385\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3166 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397382\n",
      "  validation loss:\t\t1.387382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3167 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378555\n",
      "  validation loss:\t\t1.387613\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3168 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393569\n",
      "  validation loss:\t\t1.387535\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3169 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382820\n",
      "  validation loss:\t\t1.387465\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3170 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392813\n",
      "  validation loss:\t\t1.387304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3171 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385696\n",
      "  validation loss:\t\t1.387029\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3172 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390661\n",
      "  validation loss:\t\t1.386929\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3173 of 5000 took 0.024s\n",
      "  training loss:\t\t1.374371\n",
      "  validation loss:\t\t1.386958\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3174 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398526\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3175 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398537\n",
      "  validation loss:\t\t1.387338\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3176 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389109\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3177 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396025\n",
      "  validation loss:\t\t1.387469\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3178 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384927\n",
      "  validation loss:\t\t1.387735\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3179 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398663\n",
      "  validation loss:\t\t1.387901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3180 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397063\n",
      "  validation loss:\t\t1.387783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3181 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401706\n",
      "  validation loss:\t\t1.387728\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3182 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392220\n",
      "  validation loss:\t\t1.387446\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3183 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392232\n",
      "  validation loss:\t\t1.387294\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3184 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380284\n",
      "  validation loss:\t\t1.387168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3185 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391959\n",
      "  validation loss:\t\t1.387194\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3186 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388644\n",
      "  validation loss:\t\t1.387145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3187 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392513\n",
      "  validation loss:\t\t1.387243\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3188 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382691\n",
      "  validation loss:\t\t1.387293\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3189 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389445\n",
      "  validation loss:\t\t1.387524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3190 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388314\n",
      "  validation loss:\t\t1.387661\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3191 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376007\n",
      "  validation loss:\t\t1.387649\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3192 of 5000 took 0.024s\n",
      "  training loss:\t\t1.403263\n",
      "  validation loss:\t\t1.387484\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3193 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396647\n",
      "  validation loss:\t\t1.387161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3194 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390690\n",
      "  validation loss:\t\t1.387024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3195 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384675\n",
      "  validation loss:\t\t1.386889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3196 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378564\n",
      "  validation loss:\t\t1.386849\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3197 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388180\n",
      "  validation loss:\t\t1.386853\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3198 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384090\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3199 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388919\n",
      "  validation loss:\t\t1.386955\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3200 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384353\n",
      "  validation loss:\t\t1.386963\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3201 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393222\n",
      "  validation loss:\t\t1.387060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3202 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394922\n",
      "  validation loss:\t\t1.387270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3203 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391495\n",
      "  validation loss:\t\t1.387393\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3204 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397929\n",
      "  validation loss:\t\t1.387523\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3205 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383552\n",
      "  validation loss:\t\t1.387714\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3206 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396700\n",
      "  validation loss:\t\t1.387723\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3207 of 5000 took 0.021s\n",
      "  training loss:\t\t1.407379\n",
      "  validation loss:\t\t1.387644\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3208 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388862\n",
      "  validation loss:\t\t1.387723\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3209 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388947\n",
      "  validation loss:\t\t1.387696\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3210 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382402\n",
      "  validation loss:\t\t1.387729\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3211 of 5000 took 0.024s\n",
      "  training loss:\t\t1.367297\n",
      "  validation loss:\t\t1.387858\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3212 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396449\n",
      "  validation loss:\t\t1.387639\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3213 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393135\n",
      "  validation loss:\t\t1.387713\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3214 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385074\n",
      "  validation loss:\t\t1.387742\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3215 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394892\n",
      "  validation loss:\t\t1.387665\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3216 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387989\n",
      "  validation loss:\t\t1.387645\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3217 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399618\n",
      "  validation loss:\t\t1.387863\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3218 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394461\n",
      "  validation loss:\t\t1.387956\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3219 of 5000 took 0.022s\n",
      "  training loss:\t\t1.373451\n",
      "  validation loss:\t\t1.388248\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3220 of 5000 took 0.024s\n",
      "  training loss:\t\t1.403135\n",
      "  validation loss:\t\t1.387864\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3221 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397670\n",
      "  validation loss:\t\t1.387571\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3222 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407115\n",
      "  validation loss:\t\t1.387528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3223 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392520\n",
      "  validation loss:\t\t1.387286\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3224 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396019\n",
      "  validation loss:\t\t1.387079\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3225 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379638\n",
      "  validation loss:\t\t1.387121\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3226 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398393\n",
      "  validation loss:\t\t1.387467\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3227 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386968\n",
      "  validation loss:\t\t1.387771\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3228 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383636\n",
      "  validation loss:\t\t1.388013\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3229 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388597\n",
      "  validation loss:\t\t1.388181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3230 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395121\n",
      "  validation loss:\t\t1.388149\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3231 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381209\n",
      "  validation loss:\t\t1.388301\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3232 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392365\n",
      "  validation loss:\t\t1.387998\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3233 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380305\n",
      "  validation loss:\t\t1.387810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3234 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389698\n",
      "  validation loss:\t\t1.387597\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3235 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395548\n",
      "  validation loss:\t\t1.387490\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3236 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387169\n",
      "  validation loss:\t\t1.387419\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3237 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385742\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3238 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395042\n",
      "  validation loss:\t\t1.387777\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3239 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394637\n",
      "  validation loss:\t\t1.387806\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3240 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391414\n",
      "  validation loss:\t\t1.387633\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3241 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385256\n",
      "  validation loss:\t\t1.387770\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3242 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394276\n",
      "  validation loss:\t\t1.387545\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3243 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400114\n",
      "  validation loss:\t\t1.387286\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3244 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383188\n",
      "  validation loss:\t\t1.387145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3245 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381615\n",
      "  validation loss:\t\t1.387006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3246 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387294\n",
      "  validation loss:\t\t1.386985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3247 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391204\n",
      "  validation loss:\t\t1.387103\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3248 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394372\n",
      "  validation loss:\t\t1.387036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3249 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385098\n",
      "  validation loss:\t\t1.387074\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3250 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375894\n",
      "  validation loss:\t\t1.387173\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3251 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381082\n",
      "  validation loss:\t\t1.387420\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3252 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391713\n",
      "  validation loss:\t\t1.387814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3253 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390382\n",
      "  validation loss:\t\t1.388043\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3254 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397344\n",
      "  validation loss:\t\t1.387963\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3255 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401810\n",
      "  validation loss:\t\t1.387874\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3256 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374422\n",
      "  validation loss:\t\t1.387727\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3257 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390946\n",
      "  validation loss:\t\t1.387581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3258 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395541\n",
      "  validation loss:\t\t1.387389\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3259 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384009\n",
      "  validation loss:\t\t1.387159\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3260 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379215\n",
      "  validation loss:\t\t1.387084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3261 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394119\n",
      "  validation loss:\t\t1.387009\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3262 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382535\n",
      "  validation loss:\t\t1.386947\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3263 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389513\n",
      "  validation loss:\t\t1.386848\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3264 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390849\n",
      "  validation loss:\t\t1.386842\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3265 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390531\n",
      "  validation loss:\t\t1.386835\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3266 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387543\n",
      "  validation loss:\t\t1.386830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3267 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383951\n",
      "  validation loss:\t\t1.386897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3268 of 5000 took 0.025s\n",
      "  training loss:\t\t1.381023\n",
      "  validation loss:\t\t1.387010\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3269 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385115\n",
      "  validation loss:\t\t1.387125\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3270 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374561\n",
      "  validation loss:\t\t1.387226\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3271 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383061\n",
      "  validation loss:\t\t1.387432\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3272 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393618\n",
      "  validation loss:\t\t1.387439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3273 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385232\n",
      "  validation loss:\t\t1.387557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3274 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390314\n",
      "  validation loss:\t\t1.387692\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3275 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398121\n",
      "  validation loss:\t\t1.387687\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3276 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389714\n",
      "  validation loss:\t\t1.387687\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3277 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384220\n",
      "  validation loss:\t\t1.387617\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3278 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378994\n",
      "  validation loss:\t\t1.387399\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3279 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395527\n",
      "  validation loss:\t\t1.387699\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3280 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403462\n",
      "  validation loss:\t\t1.387542\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3281 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384017\n",
      "  validation loss:\t\t1.387598\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3282 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387548\n",
      "  validation loss:\t\t1.387565\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3283 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394800\n",
      "  validation loss:\t\t1.387247\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3284 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398909\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3285 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384371\n",
      "  validation loss:\t\t1.387095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3286 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385379\n",
      "  validation loss:\t\t1.387123\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3287 of 5000 took 0.028s\n",
      "  training loss:\t\t1.395121\n",
      "  validation loss:\t\t1.387103\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3288 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385549\n",
      "  validation loss:\t\t1.387207\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3289 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393957\n",
      "  validation loss:\t\t1.387448\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3290 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391083\n",
      "  validation loss:\t\t1.387637\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3291 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381043\n",
      "  validation loss:\t\t1.387596\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3292 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394527\n",
      "  validation loss:\t\t1.387377\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3293 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394055\n",
      "  validation loss:\t\t1.387264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3294 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391658\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3295 of 5000 took 0.021s\n",
      "  training loss:\t\t1.371113\n",
      "  validation loss:\t\t1.387187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3296 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384406\n",
      "  validation loss:\t\t1.387173\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3297 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383399\n",
      "  validation loss:\t\t1.387168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3298 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391308\n",
      "  validation loss:\t\t1.387049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3299 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382512\n",
      "  validation loss:\t\t1.387058\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3300 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387040\n",
      "  validation loss:\t\t1.387060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3301 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393416\n",
      "  validation loss:\t\t1.386986\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3302 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381845\n",
      "  validation loss:\t\t1.386957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3303 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392786\n",
      "  validation loss:\t\t1.386891\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3304 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396304\n",
      "  validation loss:\t\t1.386920\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3305 of 5000 took 0.022s\n",
      "  training loss:\t\t1.370501\n",
      "  validation loss:\t\t1.386956\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3306 of 5000 took 0.025s\n",
      "  training loss:\t\t1.380695\n",
      "  validation loss:\t\t1.387130\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3307 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376144\n",
      "  validation loss:\t\t1.387419\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3308 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380276\n",
      "  validation loss:\t\t1.387684\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3309 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383074\n",
      "  validation loss:\t\t1.387674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3310 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386700\n",
      "  validation loss:\t\t1.387959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3311 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404234\n",
      "  validation loss:\t\t1.387655\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3312 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382519\n",
      "  validation loss:\t\t1.387602\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3313 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394364\n",
      "  validation loss:\t\t1.387619\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3314 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397118\n",
      "  validation loss:\t\t1.387562\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3315 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398195\n",
      "  validation loss:\t\t1.387568\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3316 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390975\n",
      "  validation loss:\t\t1.387737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3317 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386093\n",
      "  validation loss:\t\t1.387733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3318 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387258\n",
      "  validation loss:\t\t1.387802\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3319 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387216\n",
      "  validation loss:\t\t1.387872\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3320 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391893\n",
      "  validation loss:\t\t1.387620\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3321 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386873\n",
      "  validation loss:\t\t1.387392\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3322 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395472\n",
      "  validation loss:\t\t1.387257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3323 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386329\n",
      "  validation loss:\t\t1.387009\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3324 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388775\n",
      "  validation loss:\t\t1.387031\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3325 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385355\n",
      "  validation loss:\t\t1.387026\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3326 of 5000 took 0.025s\n",
      "  training loss:\t\t1.389207\n",
      "  validation loss:\t\t1.386953\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3327 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395007\n",
      "  validation loss:\t\t1.387009\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3328 of 5000 took 0.025s\n",
      "  training loss:\t\t1.391791\n",
      "  validation loss:\t\t1.387116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3329 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391259\n",
      "  validation loss:\t\t1.387112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3330 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392561\n",
      "  validation loss:\t\t1.386991\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3331 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383729\n",
      "  validation loss:\t\t1.387022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3332 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389423\n",
      "  validation loss:\t\t1.387086\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3333 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386025\n",
      "  validation loss:\t\t1.387124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3334 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402934\n",
      "  validation loss:\t\t1.387007\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3335 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394188\n",
      "  validation loss:\t\t1.387068\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3336 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394626\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3337 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381608\n",
      "  validation loss:\t\t1.387130\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3338 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387091\n",
      "  validation loss:\t\t1.387360\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3339 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396619\n",
      "  validation loss:\t\t1.387633\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3340 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388197\n",
      "  validation loss:\t\t1.387691\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3341 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388869\n",
      "  validation loss:\t\t1.387557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3342 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391414\n",
      "  validation loss:\t\t1.387438\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3343 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381693\n",
      "  validation loss:\t\t1.387313\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3344 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385761\n",
      "  validation loss:\t\t1.387195\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3345 of 5000 took 0.025s\n",
      "  training loss:\t\t1.403553\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3346 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390585\n",
      "  validation loss:\t\t1.387131\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3347 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392122\n",
      "  validation loss:\t\t1.387096\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3348 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403351\n",
      "  validation loss:\t\t1.386968\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3349 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388510\n",
      "  validation loss:\t\t1.386972\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3350 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382902\n",
      "  validation loss:\t\t1.386948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3351 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393144\n",
      "  validation loss:\t\t1.386932\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3352 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384494\n",
      "  validation loss:\t\t1.387121\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3353 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390241\n",
      "  validation loss:\t\t1.387234\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3354 of 5000 took 0.023s\n",
      "  training loss:\t\t1.372023\n",
      "  validation loss:\t\t1.387348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3355 of 5000 took 0.025s\n",
      "  training loss:\t\t1.394274\n",
      "  validation loss:\t\t1.387417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3356 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400689\n",
      "  validation loss:\t\t1.387239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3357 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393280\n",
      "  validation loss:\t\t1.387250\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3358 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384907\n",
      "  validation loss:\t\t1.387378\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3359 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401972\n",
      "  validation loss:\t\t1.387284\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3360 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397706\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3361 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386587\n",
      "  validation loss:\t\t1.387123\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3362 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390518\n",
      "  validation loss:\t\t1.387057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3363 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398746\n",
      "  validation loss:\t\t1.386975\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3364 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392540\n",
      "  validation loss:\t\t1.386914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3365 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399098\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3366 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388108\n",
      "  validation loss:\t\t1.387232\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3367 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380592\n",
      "  validation loss:\t\t1.387667\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3368 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389859\n",
      "  validation loss:\t\t1.388113\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3369 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396933\n",
      "  validation loss:\t\t1.388262\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3370 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394890\n",
      "  validation loss:\t\t1.388473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3371 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382955\n",
      "  validation loss:\t\t1.389051\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3372 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378967\n",
      "  validation loss:\t\t1.389636\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3373 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392866\n",
      "  validation loss:\t\t1.389754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3374 of 5000 took 0.024s\n",
      "  training loss:\t\t1.376778\n",
      "  validation loss:\t\t1.389538\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3375 of 5000 took 0.026s\n",
      "  training loss:\t\t1.392452\n",
      "  validation loss:\t\t1.389370\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3376 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391548\n",
      "  validation loss:\t\t1.389039\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3377 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381973\n",
      "  validation loss:\t\t1.388816\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3378 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391045\n",
      "  validation loss:\t\t1.388450\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3379 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376291\n",
      "  validation loss:\t\t1.388186\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3380 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398409\n",
      "  validation loss:\t\t1.387912\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3381 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396319\n",
      "  validation loss:\t\t1.387718\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3382 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385831\n",
      "  validation loss:\t\t1.387964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3383 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394848\n",
      "  validation loss:\t\t1.387918\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3384 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390617\n",
      "  validation loss:\t\t1.387671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3385 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386692\n",
      "  validation loss:\t\t1.387573\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3386 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396890\n",
      "  validation loss:\t\t1.387484\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3387 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392377\n",
      "  validation loss:\t\t1.387338\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3388 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392011\n",
      "  validation loss:\t\t1.387205\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3389 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384638\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3390 of 5000 took 0.022s\n",
      "  training loss:\t\t1.407017\n",
      "  validation loss:\t\t1.386993\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3391 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393813\n",
      "  validation loss:\t\t1.386923\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3392 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390717\n",
      "  validation loss:\t\t1.386873\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3393 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380984\n",
      "  validation loss:\t\t1.386983\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3394 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392367\n",
      "  validation loss:\t\t1.387279\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3395 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394547\n",
      "  validation loss:\t\t1.387538\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3396 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379957\n",
      "  validation loss:\t\t1.387867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3397 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393334\n",
      "  validation loss:\t\t1.387744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3398 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389303\n",
      "  validation loss:\t\t1.387451\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3399 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399398\n",
      "  validation loss:\t\t1.387132\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3400 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384760\n",
      "  validation loss:\t\t1.386945\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3401 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384052\n",
      "  validation loss:\t\t1.386861\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3402 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385727\n",
      "  validation loss:\t\t1.386831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3403 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391103\n",
      "  validation loss:\t\t1.386880\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3404 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384935\n",
      "  validation loss:\t\t1.386938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3405 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391485\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3406 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392128\n",
      "  validation loss:\t\t1.387055\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3407 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389692\n",
      "  validation loss:\t\t1.387148\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3408 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384726\n",
      "  validation loss:\t\t1.387186\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3409 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395712\n",
      "  validation loss:\t\t1.387065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3410 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394630\n",
      "  validation loss:\t\t1.387056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3411 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388275\n",
      "  validation loss:\t\t1.387017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3412 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385153\n",
      "  validation loss:\t\t1.386998\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3413 of 5000 took 0.025s\n",
      "  training loss:\t\t1.395471\n",
      "  validation loss:\t\t1.387053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3414 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398044\n",
      "  validation loss:\t\t1.386967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3415 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379969\n",
      "  validation loss:\t\t1.386990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3416 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400690\n",
      "  validation loss:\t\t1.387047\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3417 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386752\n",
      "  validation loss:\t\t1.386991\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3418 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377876\n",
      "  validation loss:\t\t1.387093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3419 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383674\n",
      "  validation loss:\t\t1.387184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3420 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392535\n",
      "  validation loss:\t\t1.387354\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3421 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401078\n",
      "  validation loss:\t\t1.387412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3422 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404922\n",
      "  validation loss:\t\t1.387305\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3423 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388149\n",
      "  validation loss:\t\t1.387272\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3424 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379407\n",
      "  validation loss:\t\t1.387198\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3425 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380585\n",
      "  validation loss:\t\t1.387072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3426 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380788\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3427 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387130\n",
      "  validation loss:\t\t1.387063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3428 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398027\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3429 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387340\n",
      "  validation loss:\t\t1.387001\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3430 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393625\n",
      "  validation loss:\t\t1.387003\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3431 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392807\n",
      "  validation loss:\t\t1.387093\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3432 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391094\n",
      "  validation loss:\t\t1.387292\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3433 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388189\n",
      "  validation loss:\t\t1.387340\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3434 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388517\n",
      "  validation loss:\t\t1.387176\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3435 of 5000 took 0.021s\n",
      "  training loss:\t\t1.403636\n",
      "  validation loss:\t\t1.387080\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3436 of 5000 took 0.021s\n",
      "  training loss:\t\t1.373313\n",
      "  validation loss:\t\t1.387137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3437 of 5000 took 0.023s\n",
      "  training loss:\t\t1.373372\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3438 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386815\n",
      "  validation loss:\t\t1.387095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3439 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394702\n",
      "  validation loss:\t\t1.387146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3440 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395554\n",
      "  validation loss:\t\t1.386980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3441 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396266\n",
      "  validation loss:\t\t1.386939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3442 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383156\n",
      "  validation loss:\t\t1.386978\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3443 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388995\n",
      "  validation loss:\t\t1.386949\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3444 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391923\n",
      "  validation loss:\t\t1.386989\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3445 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388474\n",
      "  validation loss:\t\t1.386974\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3446 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381744\n",
      "  validation loss:\t\t1.386893\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3447 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386010\n",
      "  validation loss:\t\t1.386934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3448 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383732\n",
      "  validation loss:\t\t1.386845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3449 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386511\n",
      "  validation loss:\t\t1.386811\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3450 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387844\n",
      "  validation loss:\t\t1.386900\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3451 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388865\n",
      "  validation loss:\t\t1.387125\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3452 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392539\n",
      "  validation loss:\t\t1.387418\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3453 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385558\n",
      "  validation loss:\t\t1.387689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3454 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395307\n",
      "  validation loss:\t\t1.387872\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3455 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395996\n",
      "  validation loss:\t\t1.387875\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3456 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392411\n",
      "  validation loss:\t\t1.387463\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3457 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383398\n",
      "  validation loss:\t\t1.387181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3458 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382103\n",
      "  validation loss:\t\t1.387116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3459 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382329\n",
      "  validation loss:\t\t1.387236\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3460 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390231\n",
      "  validation loss:\t\t1.387513\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3461 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389502\n",
      "  validation loss:\t\t1.387540\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3462 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383052\n",
      "  validation loss:\t\t1.387511\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3463 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399924\n",
      "  validation loss:\t\t1.387567\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3464 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398056\n",
      "  validation loss:\t\t1.387251\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3465 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402417\n",
      "  validation loss:\t\t1.387020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3466 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388233\n",
      "  validation loss:\t\t1.386942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3467 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388632\n",
      "  validation loss:\t\t1.387053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3468 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380545\n",
      "  validation loss:\t\t1.387228\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3469 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401893\n",
      "  validation loss:\t\t1.387314\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3470 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394773\n",
      "  validation loss:\t\t1.387348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3471 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394111\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3472 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393288\n",
      "  validation loss:\t\t1.387375\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3473 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385125\n",
      "  validation loss:\t\t1.387308\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3474 of 5000 took 0.024s\n",
      "  training loss:\t\t1.381458\n",
      "  validation loss:\t\t1.387566\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3475 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397325\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3476 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387107\n",
      "  validation loss:\t\t1.387588\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3477 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402460\n",
      "  validation loss:\t\t1.387512\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3478 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389624\n",
      "  validation loss:\t\t1.387574\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3479 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393105\n",
      "  validation loss:\t\t1.387501\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3480 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382199\n",
      "  validation loss:\t\t1.387457\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3481 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380416\n",
      "  validation loss:\t\t1.387408\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3482 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391447\n",
      "  validation loss:\t\t1.387250\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3483 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393231\n",
      "  validation loss:\t\t1.387164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3484 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390096\n",
      "  validation loss:\t\t1.387197\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3485 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388861\n",
      "  validation loss:\t\t1.387323\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3486 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397524\n",
      "  validation loss:\t\t1.387430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3487 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385987\n",
      "  validation loss:\t\t1.387505\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3488 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391345\n",
      "  validation loss:\t\t1.387680\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3489 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399284\n",
      "  validation loss:\t\t1.387689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3490 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390261\n",
      "  validation loss:\t\t1.387662\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3491 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393329\n",
      "  validation loss:\t\t1.387575\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3492 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385208\n",
      "  validation loss:\t\t1.387560\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3493 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394212\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3494 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387251\n",
      "  validation loss:\t\t1.387874\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3495 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382603\n",
      "  validation loss:\t\t1.388412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3496 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386862\n",
      "  validation loss:\t\t1.388678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3497 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391747\n",
      "  validation loss:\t\t1.388982\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3498 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387951\n",
      "  validation loss:\t\t1.388939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3499 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401918\n",
      "  validation loss:\t\t1.388485\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3500 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381321\n",
      "  validation loss:\t\t1.388546\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3501 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381261\n",
      "  validation loss:\t\t1.388678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3502 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403151\n",
      "  validation loss:\t\t1.388313\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3503 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388258\n",
      "  validation loss:\t\t1.388037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3504 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385768\n",
      "  validation loss:\t\t1.387774\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3505 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384743\n",
      "  validation loss:\t\t1.387411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3506 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391859\n",
      "  validation loss:\t\t1.387191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3507 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388102\n",
      "  validation loss:\t\t1.387035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3508 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400157\n",
      "  validation loss:\t\t1.386848\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3509 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391328\n",
      "  validation loss:\t\t1.386847\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3510 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386480\n",
      "  validation loss:\t\t1.386980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3511 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383387\n",
      "  validation loss:\t\t1.387185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3512 of 5000 took 0.024s\n",
      "  training loss:\t\t1.393206\n",
      "  validation loss:\t\t1.387290\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3513 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385524\n",
      "  validation loss:\t\t1.387373\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3514 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380894\n",
      "  validation loss:\t\t1.387277\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3515 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395249\n",
      "  validation loss:\t\t1.387269\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3516 of 5000 took 0.025s\n",
      "  training loss:\t\t1.393700\n",
      "  validation loss:\t\t1.387240\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3517 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391188\n",
      "  validation loss:\t\t1.387189\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3518 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385856\n",
      "  validation loss:\t\t1.387251\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3519 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383283\n",
      "  validation loss:\t\t1.387209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3520 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384145\n",
      "  validation loss:\t\t1.387196\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3521 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387356\n",
      "  validation loss:\t\t1.387287\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3522 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389946\n",
      "  validation loss:\t\t1.387197\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3523 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391876\n",
      "  validation loss:\t\t1.387061\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3524 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385974\n",
      "  validation loss:\t\t1.386971\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3525 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394664\n",
      "  validation loss:\t\t1.387040\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3526 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382470\n",
      "  validation loss:\t\t1.387066\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3527 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382386\n",
      "  validation loss:\t\t1.387053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3528 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378622\n",
      "  validation loss:\t\t1.387026\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3529 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391256\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3530 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384786\n",
      "  validation loss:\t\t1.387013\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3531 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405607\n",
      "  validation loss:\t\t1.386922\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3532 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391475\n",
      "  validation loss:\t\t1.386886\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3533 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396845\n",
      "  validation loss:\t\t1.386938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3534 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384486\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3535 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391014\n",
      "  validation loss:\t\t1.386889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3536 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392852\n",
      "  validation loss:\t\t1.386813\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3537 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378304\n",
      "  validation loss:\t\t1.386876\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3538 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387004\n",
      "  validation loss:\t\t1.386908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3539 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394518\n",
      "  validation loss:\t\t1.386998\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3540 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385485\n",
      "  validation loss:\t\t1.387024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3541 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387534\n",
      "  validation loss:\t\t1.387065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3542 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384509\n",
      "  validation loss:\t\t1.387099\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3543 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384057\n",
      "  validation loss:\t\t1.386964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3544 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382360\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3545 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381727\n",
      "  validation loss:\t\t1.386831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3546 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383421\n",
      "  validation loss:\t\t1.386870\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3547 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394870\n",
      "  validation loss:\t\t1.386974\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3548 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384227\n",
      "  validation loss:\t\t1.386976\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3549 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388744\n",
      "  validation loss:\t\t1.387085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3550 of 5000 took 0.024s\n",
      "  training loss:\t\t1.401762\n",
      "  validation loss:\t\t1.386923\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3551 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378061\n",
      "  validation loss:\t\t1.386906\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3552 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385222\n",
      "  validation loss:\t\t1.387001\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3553 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390202\n",
      "  validation loss:\t\t1.387226\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3554 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399913\n",
      "  validation loss:\t\t1.387382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3555 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391375\n",
      "  validation loss:\t\t1.387265\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3556 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391985\n",
      "  validation loss:\t\t1.387332\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3557 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390057\n",
      "  validation loss:\t\t1.387443\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3558 of 5000 took 0.025s\n",
      "  training loss:\t\t1.386014\n",
      "  validation loss:\t\t1.387645\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3559 of 5000 took 0.026s\n",
      "  training loss:\t\t1.395825\n",
      "  validation loss:\t\t1.388182\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3560 of 5000 took 0.027s\n",
      "  training loss:\t\t1.382314\n",
      "  validation loss:\t\t1.388834\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3561 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387076\n",
      "  validation loss:\t\t1.389167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3562 of 5000 took 0.021s\n",
      "  training loss:\t\t1.402189\n",
      "  validation loss:\t\t1.389656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3563 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386318\n",
      "  validation loss:\t\t1.389302\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3564 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395360\n",
      "  validation loss:\t\t1.388950\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3565 of 5000 took 0.022s\n",
      "  training loss:\t\t1.375557\n",
      "  validation loss:\t\t1.389235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3566 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385916\n",
      "  validation loss:\t\t1.389109\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3567 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383542\n",
      "  validation loss:\t\t1.388738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3568 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396869\n",
      "  validation loss:\t\t1.388100\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3569 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386757\n",
      "  validation loss:\t\t1.387976\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3570 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397798\n",
      "  validation loss:\t\t1.387721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3571 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380036\n",
      "  validation loss:\t\t1.387835\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3572 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382447\n",
      "  validation loss:\t\t1.388119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3573 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387581\n",
      "  validation loss:\t\t1.388096\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3574 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404933\n",
      "  validation loss:\t\t1.387620\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3575 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383454\n",
      "  validation loss:\t\t1.387353\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3576 of 5000 took 0.021s\n",
      "  training loss:\t\t1.405877\n",
      "  validation loss:\t\t1.387045\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3577 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392436\n",
      "  validation loss:\t\t1.386901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3578 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386633\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3579 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398697\n",
      "  validation loss:\t\t1.386921\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3580 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384190\n",
      "  validation loss:\t\t1.386985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3581 of 5000 took 0.026s\n",
      "  training loss:\t\t1.384914\n",
      "  validation loss:\t\t1.387024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3582 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378033\n",
      "  validation loss:\t\t1.387263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3583 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379184\n",
      "  validation loss:\t\t1.387468\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3584 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393046\n",
      "  validation loss:\t\t1.387593\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3585 of 5000 took 0.022s\n",
      "  training loss:\t\t1.404303\n",
      "  validation loss:\t\t1.387585\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3586 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397926\n",
      "  validation loss:\t\t1.387426\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3587 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392395\n",
      "  validation loss:\t\t1.387395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3588 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398115\n",
      "  validation loss:\t\t1.387390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3589 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387096\n",
      "  validation loss:\t\t1.387244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3590 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386106\n",
      "  validation loss:\t\t1.387071\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3591 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382159\n",
      "  validation loss:\t\t1.386914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3592 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400489\n",
      "  validation loss:\t\t1.386847\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3593 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395105\n",
      "  validation loss:\t\t1.386821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3594 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391837\n",
      "  validation loss:\t\t1.387016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3595 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384988\n",
      "  validation loss:\t\t1.387143\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3596 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379356\n",
      "  validation loss:\t\t1.387390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3597 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393661\n",
      "  validation loss:\t\t1.387368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3598 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384647\n",
      "  validation loss:\t\t1.387170\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3599 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392982\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3600 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397195\n",
      "  validation loss:\t\t1.387072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3601 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396451\n",
      "  validation loss:\t\t1.387332\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3602 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384582\n",
      "  validation loss:\t\t1.387678\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3603 of 5000 took 0.025s\n",
      "  training loss:\t\t1.381101\n",
      "  validation loss:\t\t1.388030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3604 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393686\n",
      "  validation loss:\t\t1.388497\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3605 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391627\n",
      "  validation loss:\t\t1.388684\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3606 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387519\n",
      "  validation loss:\t\t1.388719\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3607 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393180\n",
      "  validation loss:\t\t1.389263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3608 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402984\n",
      "  validation loss:\t\t1.389376\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3609 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389955\n",
      "  validation loss:\t\t1.389239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3610 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385075\n",
      "  validation loss:\t\t1.389369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3611 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393275\n",
      "  validation loss:\t\t1.389491\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3612 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392637\n",
      "  validation loss:\t\t1.389415\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3613 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388422\n",
      "  validation loss:\t\t1.389701\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3614 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395732\n",
      "  validation loss:\t\t1.389355\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3615 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395315\n",
      "  validation loss:\t\t1.388858\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3616 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385478\n",
      "  validation loss:\t\t1.388400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3617 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385170\n",
      "  validation loss:\t\t1.388473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3618 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384838\n",
      "  validation loss:\t\t1.388630\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3619 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384129\n",
      "  validation loss:\t\t1.388462\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3620 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398484\n",
      "  validation loss:\t\t1.388244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3621 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391725\n",
      "  validation loss:\t\t1.387906\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3622 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386504\n",
      "  validation loss:\t\t1.387577\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3623 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391199\n",
      "  validation loss:\t\t1.387189\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3624 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391550\n",
      "  validation loss:\t\t1.386951\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3625 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393854\n",
      "  validation loss:\t\t1.386842\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3626 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389480\n",
      "  validation loss:\t\t1.386819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3627 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393798\n",
      "  validation loss:\t\t1.386915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3628 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392238\n",
      "  validation loss:\t\t1.387057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3629 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398451\n",
      "  validation loss:\t\t1.387409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3630 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395113\n",
      "  validation loss:\t\t1.387722\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3631 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401975\n",
      "  validation loss:\t\t1.387867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3632 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388771\n",
      "  validation loss:\t\t1.388020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3633 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388107\n",
      "  validation loss:\t\t1.388129\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3634 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387457\n",
      "  validation loss:\t\t1.388024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3635 of 5000 took 0.024s\n",
      "  training loss:\t\t1.399194\n",
      "  validation loss:\t\t1.387400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3636 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400370\n",
      "  validation loss:\t\t1.387017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3637 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389799\n",
      "  validation loss:\t\t1.386807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3638 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386744\n",
      "  validation loss:\t\t1.386805\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3639 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392200\n",
      "  validation loss:\t\t1.386930\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3640 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383845\n",
      "  validation loss:\t\t1.387068\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3641 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390996\n",
      "  validation loss:\t\t1.387168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3642 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384720\n",
      "  validation loss:\t\t1.387308\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3643 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384475\n",
      "  validation loss:\t\t1.387478\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3644 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392816\n",
      "  validation loss:\t\t1.387833\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3645 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392864\n",
      "  validation loss:\t\t1.387705\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3646 of 5000 took 0.027s\n",
      "  training loss:\t\t1.382143\n",
      "  validation loss:\t\t1.387949\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3647 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397268\n",
      "  validation loss:\t\t1.387791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3648 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386851\n",
      "  validation loss:\t\t1.387891\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3649 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390221\n",
      "  validation loss:\t\t1.387888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3650 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395736\n",
      "  validation loss:\t\t1.387674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3651 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388551\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3652 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384710\n",
      "  validation loss:\t\t1.387859\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3653 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384413\n",
      "  validation loss:\t\t1.387972\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3654 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390820\n",
      "  validation loss:\t\t1.387743\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3655 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393058\n",
      "  validation loss:\t\t1.387519\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3656 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389713\n",
      "  validation loss:\t\t1.387269\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3657 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389606\n",
      "  validation loss:\t\t1.387181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3658 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394516\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3659 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398105\n",
      "  validation loss:\t\t1.387003\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3660 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401761\n",
      "  validation loss:\t\t1.387110\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3661 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388518\n",
      "  validation loss:\t\t1.387164\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3662 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382878\n",
      "  validation loss:\t\t1.387082\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3663 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376888\n",
      "  validation loss:\t\t1.387065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3664 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392511\n",
      "  validation loss:\t\t1.387060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3665 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391361\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3666 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396296\n",
      "  validation loss:\t\t1.386829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3667 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398753\n",
      "  validation loss:\t\t1.386765\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3668 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391023\n",
      "  validation loss:\t\t1.386793\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3669 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392562\n",
      "  validation loss:\t\t1.386854\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3670 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392680\n",
      "  validation loss:\t\t1.387106\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3671 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393972\n",
      "  validation loss:\t\t1.387442\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3672 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388804\n",
      "  validation loss:\t\t1.387572\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3673 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389505\n",
      "  validation loss:\t\t1.387609\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3674 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391490\n",
      "  validation loss:\t\t1.387731\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3675 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385335\n",
      "  validation loss:\t\t1.387761\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3676 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378011\n",
      "  validation loss:\t\t1.387747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3677 of 5000 took 0.022s\n",
      "  training loss:\t\t1.367155\n",
      "  validation loss:\t\t1.388057\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3678 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386715\n",
      "  validation loss:\t\t1.387985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3679 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392203\n",
      "  validation loss:\t\t1.387920\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3680 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394320\n",
      "  validation loss:\t\t1.388048\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3681 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392013\n",
      "  validation loss:\t\t1.387848\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3682 of 5000 took 0.025s\n",
      "  training loss:\t\t1.395824\n",
      "  validation loss:\t\t1.387422\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3683 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392600\n",
      "  validation loss:\t\t1.387279\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3684 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397606\n",
      "  validation loss:\t\t1.387178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3685 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393633\n",
      "  validation loss:\t\t1.387097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3686 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392841\n",
      "  validation loss:\t\t1.387127\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3687 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394827\n",
      "  validation loss:\t\t1.387152\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3688 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389878\n",
      "  validation loss:\t\t1.387060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3689 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389641\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3690 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397236\n",
      "  validation loss:\t\t1.387157\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3691 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391964\n",
      "  validation loss:\t\t1.387270\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3692 of 5000 took 0.024s\n",
      "  training loss:\t\t1.379839\n",
      "  validation loss:\t\t1.387338\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3693 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381187\n",
      "  validation loss:\t\t1.387620\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3694 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388045\n",
      "  validation loss:\t\t1.387703\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3695 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387992\n",
      "  validation loss:\t\t1.387715\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3696 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393676\n",
      "  validation loss:\t\t1.387517\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3697 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392886\n",
      "  validation loss:\t\t1.387310\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3698 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397899\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3699 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391021\n",
      "  validation loss:\t\t1.386925\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3700 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394786\n",
      "  validation loss:\t\t1.386959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3701 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395536\n",
      "  validation loss:\t\t1.387227\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3702 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386033\n",
      "  validation loss:\t\t1.387248\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3703 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386879\n",
      "  validation loss:\t\t1.387557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3704 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386133\n",
      "  validation loss:\t\t1.387447\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3705 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398386\n",
      "  validation loss:\t\t1.387353\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3706 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392743\n",
      "  validation loss:\t\t1.387368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3707 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396795\n",
      "  validation loss:\t\t1.387306\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3708 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389133\n",
      "  validation loss:\t\t1.387282\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3709 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385335\n",
      "  validation loss:\t\t1.387136\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3710 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389743\n",
      "  validation loss:\t\t1.387161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3711 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384367\n",
      "  validation loss:\t\t1.387063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3712 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390776\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3713 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392956\n",
      "  validation loss:\t\t1.386815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3714 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395269\n",
      "  validation loss:\t\t1.386810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3715 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385310\n",
      "  validation loss:\t\t1.386936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3716 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388035\n",
      "  validation loss:\t\t1.386997\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3717 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391634\n",
      "  validation loss:\t\t1.387129\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3718 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379686\n",
      "  validation loss:\t\t1.387248\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3719 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377639\n",
      "  validation loss:\t\t1.387481\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3720 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395977\n",
      "  validation loss:\t\t1.387589\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3721 of 5000 took 0.023s\n",
      "  training loss:\t\t1.372690\n",
      "  validation loss:\t\t1.387966\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3722 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397858\n",
      "  validation loss:\t\t1.388006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3723 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390643\n",
      "  validation loss:\t\t1.387898\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3724 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386637\n",
      "  validation loss:\t\t1.387715\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3725 of 5000 took 0.025s\n",
      "  training loss:\t\t1.381207\n",
      "  validation loss:\t\t1.387590\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3726 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392044\n",
      "  validation loss:\t\t1.387395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3727 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392273\n",
      "  validation loss:\t\t1.387084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3728 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380724\n",
      "  validation loss:\t\t1.386856\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3729 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396222\n",
      "  validation loss:\t\t1.386803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3730 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401035\n",
      "  validation loss:\t\t1.386873\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3731 of 5000 took 0.024s\n",
      "  training loss:\t\t1.383247\n",
      "  validation loss:\t\t1.386999\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3732 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389475\n",
      "  validation loss:\t\t1.387517\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3733 of 5000 took 0.026s\n",
      "  training loss:\t\t1.397329\n",
      "  validation loss:\t\t1.387714\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3734 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391845\n",
      "  validation loss:\t\t1.388133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3735 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388780\n",
      "  validation loss:\t\t1.388185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3736 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386244\n",
      "  validation loss:\t\t1.388656\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3737 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386919\n",
      "  validation loss:\t\t1.389312\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3738 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390687\n",
      "  validation loss:\t\t1.389297\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3739 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384985\n",
      "  validation loss:\t\t1.389402\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3740 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393725\n",
      "  validation loss:\t\t1.388706\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3741 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397765\n",
      "  validation loss:\t\t1.388363\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3742 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391173\n",
      "  validation loss:\t\t1.388145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3743 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390246\n",
      "  validation loss:\t\t1.387761\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3744 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393766\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3745 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390282\n",
      "  validation loss:\t\t1.387105\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3746 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383818\n",
      "  validation loss:\t\t1.386982\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3747 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400614\n",
      "  validation loss:\t\t1.386978\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3748 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389949\n",
      "  validation loss:\t\t1.387149\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3749 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388540\n",
      "  validation loss:\t\t1.387200\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3750 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379727\n",
      "  validation loss:\t\t1.387311\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3751 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391160\n",
      "  validation loss:\t\t1.387527\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3752 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392501\n",
      "  validation loss:\t\t1.387400\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3753 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386896\n",
      "  validation loss:\t\t1.387198\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3754 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393461\n",
      "  validation loss:\t\t1.387100\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3755 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390763\n",
      "  validation loss:\t\t1.386976\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3756 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392502\n",
      "  validation loss:\t\t1.387023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3757 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391110\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3758 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386105\n",
      "  validation loss:\t\t1.387131\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3759 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383781\n",
      "  validation loss:\t\t1.387464\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3760 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387500\n",
      "  validation loss:\t\t1.387527\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3761 of 5000 took 0.025s\n",
      "  training loss:\t\t1.391207\n",
      "  validation loss:\t\t1.387737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3762 of 5000 took 0.024s\n",
      "  training loss:\t\t1.380341\n",
      "  validation loss:\t\t1.387922\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3763 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387334\n",
      "  validation loss:\t\t1.388091\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3764 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379872\n",
      "  validation loss:\t\t1.388100\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3765 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389866\n",
      "  validation loss:\t\t1.387961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3766 of 5000 took 0.025s\n",
      "  training loss:\t\t1.393868\n",
      "  validation loss:\t\t1.387764\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3767 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390665\n",
      "  validation loss:\t\t1.387798\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3768 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393673\n",
      "  validation loss:\t\t1.387648\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3769 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382233\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3770 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383324\n",
      "  validation loss:\t\t1.387425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3771 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378802\n",
      "  validation loss:\t\t1.387304\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3772 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390447\n",
      "  validation loss:\t\t1.387297\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3773 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394863\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3774 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393674\n",
      "  validation loss:\t\t1.386969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3775 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379909\n",
      "  validation loss:\t\t1.386880\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3776 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386283\n",
      "  validation loss:\t\t1.386805\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3777 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391286\n",
      "  validation loss:\t\t1.386832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3778 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397385\n",
      "  validation loss:\t\t1.386857\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3779 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380883\n",
      "  validation loss:\t\t1.386980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3780 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385573\n",
      "  validation loss:\t\t1.387108\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3781 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387905\n",
      "  validation loss:\t\t1.387181\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3782 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385676\n",
      "  validation loss:\t\t1.387344\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3783 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381137\n",
      "  validation loss:\t\t1.387157\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3784 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392797\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3785 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394197\n",
      "  validation loss:\t\t1.386905\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3786 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393479\n",
      "  validation loss:\t\t1.386938\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3787 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390137\n",
      "  validation loss:\t\t1.387045\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3788 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397365\n",
      "  validation loss:\t\t1.386954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3789 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386721\n",
      "  validation loss:\t\t1.386950\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3790 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401975\n",
      "  validation loss:\t\t1.386924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3791 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375720\n",
      "  validation loss:\t\t1.387063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3792 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386884\n",
      "  validation loss:\t\t1.386897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3793 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382666\n",
      "  validation loss:\t\t1.387047\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3794 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378528\n",
      "  validation loss:\t\t1.387129\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3795 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386394\n",
      "  validation loss:\t\t1.387279\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3796 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393687\n",
      "  validation loss:\t\t1.387348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3797 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386284\n",
      "  validation loss:\t\t1.387347\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3798 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384197\n",
      "  validation loss:\t\t1.387508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3799 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394699\n",
      "  validation loss:\t\t1.387320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3800 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392298\n",
      "  validation loss:\t\t1.387049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3801 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395209\n",
      "  validation loss:\t\t1.386907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3802 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396279\n",
      "  validation loss:\t\t1.386878\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3803 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396073\n",
      "  validation loss:\t\t1.386820\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3804 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381261\n",
      "  validation loss:\t\t1.386845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3805 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390661\n",
      "  validation loss:\t\t1.386875\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3806 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382102\n",
      "  validation loss:\t\t1.386788\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3807 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380698\n",
      "  validation loss:\t\t1.386770\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3808 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385168\n",
      "  validation loss:\t\t1.386847\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3809 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393699\n",
      "  validation loss:\t\t1.386883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3810 of 5000 took 0.025s\n",
      "  training loss:\t\t1.384151\n",
      "  validation loss:\t\t1.386882\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3811 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389868\n",
      "  validation loss:\t\t1.386802\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3812 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394162\n",
      "  validation loss:\t\t1.386823\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3813 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394526\n",
      "  validation loss:\t\t1.386883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3814 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396876\n",
      "  validation loss:\t\t1.386894\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3815 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386927\n",
      "  validation loss:\t\t1.386916\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3816 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381003\n",
      "  validation loss:\t\t1.386964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3817 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396325\n",
      "  validation loss:\t\t1.386993\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3818 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379256\n",
      "  validation loss:\t\t1.387040\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3819 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395280\n",
      "  validation loss:\t\t1.386895\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3820 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384944\n",
      "  validation loss:\t\t1.386951\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3821 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389082\n",
      "  validation loss:\t\t1.387022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3822 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390480\n",
      "  validation loss:\t\t1.386961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3823 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393563\n",
      "  validation loss:\t\t1.387064\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3824 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376739\n",
      "  validation loss:\t\t1.387426\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3825 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395451\n",
      "  validation loss:\t\t1.387314\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3826 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396543\n",
      "  validation loss:\t\t1.387159\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3827 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386786\n",
      "  validation loss:\t\t1.386983\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3828 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393863\n",
      "  validation loss:\t\t1.386777\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3829 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389999\n",
      "  validation loss:\t\t1.386757\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3830 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394239\n",
      "  validation loss:\t\t1.386797\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3831 of 5000 took 0.024s\n",
      "  training loss:\t\t1.374651\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3832 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393549\n",
      "  validation loss:\t\t1.387298\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3833 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383271\n",
      "  validation loss:\t\t1.387919\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3834 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383833\n",
      "  validation loss:\t\t1.388254\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3835 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400674\n",
      "  validation loss:\t\t1.388575\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3836 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395943\n",
      "  validation loss:\t\t1.388615\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3837 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389668\n",
      "  validation loss:\t\t1.388630\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3838 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385626\n",
      "  validation loss:\t\t1.388701\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3839 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392814\n",
      "  validation loss:\t\t1.388709\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3840 of 5000 took 0.021s\n",
      "  training loss:\t\t1.400394\n",
      "  validation loss:\t\t1.388150\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3841 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396860\n",
      "  validation loss:\t\t1.387699\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3842 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398566\n",
      "  validation loss:\t\t1.387653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3843 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380390\n",
      "  validation loss:\t\t1.387522\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3844 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395205\n",
      "  validation loss:\t\t1.387247\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3845 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392086\n",
      "  validation loss:\t\t1.387030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3846 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386056\n",
      "  validation loss:\t\t1.386900\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3847 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387032\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3848 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395805\n",
      "  validation loss:\t\t1.387092\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3849 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382886\n",
      "  validation loss:\t\t1.387220\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3850 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381567\n",
      "  validation loss:\t\t1.387154\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3851 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391861\n",
      "  validation loss:\t\t1.387031\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3852 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396302\n",
      "  validation loss:\t\t1.386969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3853 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378988\n",
      "  validation loss:\t\t1.387052\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3854 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397867\n",
      "  validation loss:\t\t1.386960\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3855 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390376\n",
      "  validation loss:\t\t1.387007\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3856 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390198\n",
      "  validation loss:\t\t1.387016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3857 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388084\n",
      "  validation loss:\t\t1.387104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3858 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396976\n",
      "  validation loss:\t\t1.387167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3859 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398311\n",
      "  validation loss:\t\t1.387158\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3860 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390236\n",
      "  validation loss:\t\t1.387130\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3861 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397985\n",
      "  validation loss:\t\t1.387178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3862 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391282\n",
      "  validation loss:\t\t1.386960\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3863 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389194\n",
      "  validation loss:\t\t1.386881\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3864 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392086\n",
      "  validation loss:\t\t1.386838\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3865 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397049\n",
      "  validation loss:\t\t1.386841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3866 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385661\n",
      "  validation loss:\t\t1.386900\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3867 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386443\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3868 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390324\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3869 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382052\n",
      "  validation loss:\t\t1.386985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3870 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400676\n",
      "  validation loss:\t\t1.386879\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3871 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394838\n",
      "  validation loss:\t\t1.386831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3872 of 5000 took 0.022s\n",
      "  training loss:\t\t1.406238\n",
      "  validation loss:\t\t1.386923\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3873 of 5000 took 0.021s\n",
      "  training loss:\t\t1.377586\n",
      "  validation loss:\t\t1.386967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3874 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389215\n",
      "  validation loss:\t\t1.386961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3875 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390727\n",
      "  validation loss:\t\t1.386995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3876 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396040\n",
      "  validation loss:\t\t1.387177\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3877 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386590\n",
      "  validation loss:\t\t1.387454\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3878 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386834\n",
      "  validation loss:\t\t1.387638\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3879 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389924\n",
      "  validation loss:\t\t1.387846\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3880 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394725\n",
      "  validation loss:\t\t1.387889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3881 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390452\n",
      "  validation loss:\t\t1.388124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3882 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392526\n",
      "  validation loss:\t\t1.388128\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3883 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389573\n",
      "  validation loss:\t\t1.387914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3884 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396004\n",
      "  validation loss:\t\t1.387522\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3885 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393668\n",
      "  validation loss:\t\t1.387104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3886 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389624\n",
      "  validation loss:\t\t1.387060\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3887 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378881\n",
      "  validation loss:\t\t1.386901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3888 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389640\n",
      "  validation loss:\t\t1.386788\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3889 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382890\n",
      "  validation loss:\t\t1.386782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3890 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380437\n",
      "  validation loss:\t\t1.386848\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3891 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389331\n",
      "  validation loss:\t\t1.386807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3892 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381855\n",
      "  validation loss:\t\t1.386873\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3893 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392388\n",
      "  validation loss:\t\t1.386932\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3894 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397962\n",
      "  validation loss:\t\t1.386948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3895 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390741\n",
      "  validation loss:\t\t1.387046\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3896 of 5000 took 0.025s\n",
      "  training loss:\t\t1.390720\n",
      "  validation loss:\t\t1.387090\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3897 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384007\n",
      "  validation loss:\t\t1.387292\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3898 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387422\n",
      "  validation loss:\t\t1.387530\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3899 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380430\n",
      "  validation loss:\t\t1.387556\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3900 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390630\n",
      "  validation loss:\t\t1.387767\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3901 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387507\n",
      "  validation loss:\t\t1.387742\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3902 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393474\n",
      "  validation loss:\t\t1.387508\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3903 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392903\n",
      "  validation loss:\t\t1.387326\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3904 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386948\n",
      "  validation loss:\t\t1.387072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3905 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396766\n",
      "  validation loss:\t\t1.386880\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3906 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389538\n",
      "  validation loss:\t\t1.386885\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3907 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393164\n",
      "  validation loss:\t\t1.387231\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3908 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390783\n",
      "  validation loss:\t\t1.387557\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3909 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391621\n",
      "  validation loss:\t\t1.387783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3910 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389498\n",
      "  validation loss:\t\t1.387640\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3911 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387776\n",
      "  validation loss:\t\t1.387813\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3912 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383222\n",
      "  validation loss:\t\t1.387877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3913 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374880\n",
      "  validation loss:\t\t1.388095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3914 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381739\n",
      "  validation loss:\t\t1.387922\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3915 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390776\n",
      "  validation loss:\t\t1.387718\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3916 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392034\n",
      "  validation loss:\t\t1.387441\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3917 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387173\n",
      "  validation loss:\t\t1.387346\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3918 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382686\n",
      "  validation loss:\t\t1.387419\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3919 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401953\n",
      "  validation loss:\t\t1.387201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3920 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381829\n",
      "  validation loss:\t\t1.387338\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3921 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391791\n",
      "  validation loss:\t\t1.387470\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3922 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388668\n",
      "  validation loss:\t\t1.387551\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3923 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395751\n",
      "  validation loss:\t\t1.387488\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3924 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396277\n",
      "  validation loss:\t\t1.387379\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3925 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397118\n",
      "  validation loss:\t\t1.387277\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3926 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388390\n",
      "  validation loss:\t\t1.387171\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3927 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389885\n",
      "  validation loss:\t\t1.387276\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3928 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382044\n",
      "  validation loss:\t\t1.387385\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3929 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381696\n",
      "  validation loss:\t\t1.387401\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3930 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392143\n",
      "  validation loss:\t\t1.387497\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3931 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390812\n",
      "  validation loss:\t\t1.387458\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3932 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395058\n",
      "  validation loss:\t\t1.387422\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3933 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385848\n",
      "  validation loss:\t\t1.387695\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3934 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395665\n",
      "  validation loss:\t\t1.387607\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3935 of 5000 took 0.022s\n",
      "  training loss:\t\t1.372960\n",
      "  validation loss:\t\t1.387585\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3936 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380358\n",
      "  validation loss:\t\t1.387590\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3937 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392496\n",
      "  validation loss:\t\t1.387553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3938 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393879\n",
      "  validation loss:\t\t1.387623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3939 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388103\n",
      "  validation loss:\t\t1.387408\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3940 of 5000 took 0.025s\n",
      "  training loss:\t\t1.393690\n",
      "  validation loss:\t\t1.387139\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3941 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389761\n",
      "  validation loss:\t\t1.387046\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3942 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390347\n",
      "  validation loss:\t\t1.387030\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3943 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395089\n",
      "  validation loss:\t\t1.386886\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3944 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388091\n",
      "  validation loss:\t\t1.386757\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3945 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392990\n",
      "  validation loss:\t\t1.386715\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3946 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390550\n",
      "  validation loss:\t\t1.386907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3947 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380843\n",
      "  validation loss:\t\t1.387012\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3948 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379422\n",
      "  validation loss:\t\t1.387293\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3949 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390550\n",
      "  validation loss:\t\t1.387650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3950 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377148\n",
      "  validation loss:\t\t1.387696\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3951 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390870\n",
      "  validation loss:\t\t1.387661\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3952 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394373\n",
      "  validation loss:\t\t1.387509\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3953 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389778\n",
      "  validation loss:\t\t1.387738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3954 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392781\n",
      "  validation loss:\t\t1.387477\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3955 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390777\n",
      "  validation loss:\t\t1.387690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3956 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397851\n",
      "  validation loss:\t\t1.387680\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3957 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383207\n",
      "  validation loss:\t\t1.387867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3958 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389030\n",
      "  validation loss:\t\t1.388183\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3959 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403361\n",
      "  validation loss:\t\t1.388146\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3960 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390823\n",
      "  validation loss:\t\t1.388186\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3961 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396144\n",
      "  validation loss:\t\t1.388306\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3962 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396190\n",
      "  validation loss:\t\t1.388259\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3963 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384873\n",
      "  validation loss:\t\t1.388412\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3964 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390773\n",
      "  validation loss:\t\t1.388669\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3965 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395993\n",
      "  validation loss:\t\t1.388554\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3966 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386948\n",
      "  validation loss:\t\t1.388430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3967 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397405\n",
      "  validation loss:\t\t1.388225\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3968 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394062\n",
      "  validation loss:\t\t1.387761\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3969 of 5000 took 0.023s\n",
      "  training loss:\t\t1.371148\n",
      "  validation loss:\t\t1.388137\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3970 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390242\n",
      "  validation loss:\t\t1.388368\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3971 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393652\n",
      "  validation loss:\t\t1.388671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3972 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396841\n",
      "  validation loss:\t\t1.389192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3973 of 5000 took 0.022s\n",
      "  training loss:\t\t1.410799\n",
      "  validation loss:\t\t1.389024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3974 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393739\n",
      "  validation loss:\t\t1.388730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3975 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381986\n",
      "  validation loss:\t\t1.388690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3976 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386981\n",
      "  validation loss:\t\t1.388623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3977 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386636\n",
      "  validation loss:\t\t1.388640\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3978 of 5000 took 0.022s\n",
      "  training loss:\t\t1.401741\n",
      "  validation loss:\t\t1.388574\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3979 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392159\n",
      "  validation loss:\t\t1.388111\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3980 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400472\n",
      "  validation loss:\t\t1.387695\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3981 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379564\n",
      "  validation loss:\t\t1.387491\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3982 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382845\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3983 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386268\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3984 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382254\n",
      "  validation loss:\t\t1.387120\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3985 of 5000 took 0.025s\n",
      "  training loss:\t\t1.390370\n",
      "  validation loss:\t\t1.387088\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3986 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387441\n",
      "  validation loss:\t\t1.387322\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3987 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377351\n",
      "  validation loss:\t\t1.387328\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3988 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392127\n",
      "  validation loss:\t\t1.387383\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3989 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384846\n",
      "  validation loss:\t\t1.387588\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3990 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381713\n",
      "  validation loss:\t\t1.387618\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3991 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388129\n",
      "  validation loss:\t\t1.387841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3992 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381986\n",
      "  validation loss:\t\t1.387988\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3993 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387906\n",
      "  validation loss:\t\t1.387827\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3994 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379305\n",
      "  validation loss:\t\t1.387735\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3995 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391297\n",
      "  validation loss:\t\t1.387694\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3996 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398118\n",
      "  validation loss:\t\t1.387455\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3997 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395247\n",
      "  validation loss:\t\t1.387244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3998 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394049\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 3999 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393182\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4000 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389157\n",
      "  validation loss:\t\t1.386791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4001 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391612\n",
      "  validation loss:\t\t1.386814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4002 of 5000 took 0.024s\n",
      "  training loss:\t\t1.380186\n",
      "  validation loss:\t\t1.386919\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4003 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390140\n",
      "  validation loss:\t\t1.386987\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4004 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395361\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4005 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395095\n",
      "  validation loss:\t\t1.387151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4006 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395079\n",
      "  validation loss:\t\t1.387091\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4007 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386463\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4008 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398085\n",
      "  validation loss:\t\t1.387411\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4009 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389241\n",
      "  validation loss:\t\t1.387366\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4010 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381329\n",
      "  validation loss:\t\t1.387393\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4011 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381935\n",
      "  validation loss:\t\t1.387278\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4012 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388464\n",
      "  validation loss:\t\t1.387102\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4013 of 5000 took 0.023s\n",
      "  training loss:\t\t1.403300\n",
      "  validation loss:\t\t1.387120\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4014 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389054\n",
      "  validation loss:\t\t1.387080\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4015 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393324\n",
      "  validation loss:\t\t1.387022\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4016 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385678\n",
      "  validation loss:\t\t1.386808\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4017 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389149\n",
      "  validation loss:\t\t1.386746\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4018 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386908\n",
      "  validation loss:\t\t1.386764\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4019 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387391\n",
      "  validation loss:\t\t1.386747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4020 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387096\n",
      "  validation loss:\t\t1.386799\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4021 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385761\n",
      "  validation loss:\t\t1.386810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4022 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387571\n",
      "  validation loss:\t\t1.386904\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4023 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393045\n",
      "  validation loss:\t\t1.386979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4024 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382939\n",
      "  validation loss:\t\t1.386969\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4025 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374868\n",
      "  validation loss:\t\t1.387045\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4026 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389561\n",
      "  validation loss:\t\t1.386955\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4027 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394419\n",
      "  validation loss:\t\t1.386907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4028 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387670\n",
      "  validation loss:\t\t1.386948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4029 of 5000 took 0.024s\n",
      "  training loss:\t\t1.399192\n",
      "  validation loss:\t\t1.386908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4030 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392739\n",
      "  validation loss:\t\t1.386879\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4031 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385935\n",
      "  validation loss:\t\t1.386944\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4032 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398957\n",
      "  validation loss:\t\t1.386806\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4033 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374568\n",
      "  validation loss:\t\t1.386817\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4034 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385804\n",
      "  validation loss:\t\t1.386795\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4035 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393454\n",
      "  validation loss:\t\t1.386830\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4036 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396396\n",
      "  validation loss:\t\t1.386829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4037 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386983\n",
      "  validation loss:\t\t1.386877\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4038 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385866\n",
      "  validation loss:\t\t1.386874\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4039 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387165\n",
      "  validation loss:\t\t1.386897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4040 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380592\n",
      "  validation loss:\t\t1.387082\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4041 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382644\n",
      "  validation loss:\t\t1.387017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4042 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385627\n",
      "  validation loss:\t\t1.387166\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4043 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391426\n",
      "  validation loss:\t\t1.387167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4044 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385606\n",
      "  validation loss:\t\t1.387185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4045 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386789\n",
      "  validation loss:\t\t1.387357\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4046 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390312\n",
      "  validation loss:\t\t1.387358\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4047 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397216\n",
      "  validation loss:\t\t1.387069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4048 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378814\n",
      "  validation loss:\t\t1.386836\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4049 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395858\n",
      "  validation loss:\t\t1.386783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4050 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391876\n",
      "  validation loss:\t\t1.386837\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4051 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394189\n",
      "  validation loss:\t\t1.386924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4052 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385548\n",
      "  validation loss:\t\t1.387079\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4053 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392516\n",
      "  validation loss:\t\t1.387014\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4054 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396518\n",
      "  validation loss:\t\t1.387115\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4055 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388277\n",
      "  validation loss:\t\t1.387251\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4056 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385702\n",
      "  validation loss:\t\t1.387289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4057 of 5000 took 0.023s\n",
      "  training loss:\t\t1.402386\n",
      "  validation loss:\t\t1.387023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4058 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380384\n",
      "  validation loss:\t\t1.386909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4059 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385730\n",
      "  validation loss:\t\t1.386967\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4060 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386951\n",
      "  validation loss:\t\t1.387097\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4061 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391371\n",
      "  validation loss:\t\t1.386947\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4062 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392594\n",
      "  validation loss:\t\t1.386811\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4063 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398350\n",
      "  validation loss:\t\t1.386746\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4064 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397282\n",
      "  validation loss:\t\t1.386727\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4065 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395074\n",
      "  validation loss:\t\t1.386760\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4066 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391956\n",
      "  validation loss:\t\t1.386880\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4067 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381736\n",
      "  validation loss:\t\t1.387241\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4068 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387201\n",
      "  validation loss:\t\t1.387536\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4069 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394150\n",
      "  validation loss:\t\t1.387851\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4070 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386257\n",
      "  validation loss:\t\t1.388066\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4071 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382327\n",
      "  validation loss:\t\t1.387963\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4072 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392913\n",
      "  validation loss:\t\t1.387841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4073 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392376\n",
      "  validation loss:\t\t1.387523\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4074 of 5000 took 0.025s\n",
      "  training loss:\t\t1.398430\n",
      "  validation loss:\t\t1.387432\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4075 of 5000 took 0.024s\n",
      "  training loss:\t\t1.392281\n",
      "  validation loss:\t\t1.387227\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4076 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387922\n",
      "  validation loss:\t\t1.387067\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4077 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384527\n",
      "  validation loss:\t\t1.386984\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4078 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396675\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4079 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384749\n",
      "  validation loss:\t\t1.387225\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4080 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390666\n",
      "  validation loss:\t\t1.387375\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4081 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386191\n",
      "  validation loss:\t\t1.387581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4082 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395683\n",
      "  validation loss:\t\t1.387249\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4083 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401348\n",
      "  validation loss:\t\t1.386934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4084 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386733\n",
      "  validation loss:\t\t1.386775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4085 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400781\n",
      "  validation loss:\t\t1.386715\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4086 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394664\n",
      "  validation loss:\t\t1.386689\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4087 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388019\n",
      "  validation loss:\t\t1.386726\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4088 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385680\n",
      "  validation loss:\t\t1.386823\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4089 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391638\n",
      "  validation loss:\t\t1.386896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4090 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391361\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4091 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376456\n",
      "  validation loss:\t\t1.387223\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4092 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377233\n",
      "  validation loss:\t\t1.387386\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4093 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380266\n",
      "  validation loss:\t\t1.387532\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4094 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386575\n",
      "  validation loss:\t\t1.387481\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4095 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389766\n",
      "  validation loss:\t\t1.387523\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4096 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386716\n",
      "  validation loss:\t\t1.387782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4097 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394002\n",
      "  validation loss:\t\t1.388015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4098 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384761\n",
      "  validation loss:\t\t1.388144\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4099 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390032\n",
      "  validation loss:\t\t1.388162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4100 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392121\n",
      "  validation loss:\t\t1.388102\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4101 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390694\n",
      "  validation loss:\t\t1.387794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4102 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387483\n",
      "  validation loss:\t\t1.387558\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4103 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386895\n",
      "  validation loss:\t\t1.387195\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4104 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387905\n",
      "  validation loss:\t\t1.387276\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4105 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386343\n",
      "  validation loss:\t\t1.387483\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4106 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391861\n",
      "  validation loss:\t\t1.387582\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4107 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389636\n",
      "  validation loss:\t\t1.387710\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4108 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393433\n",
      "  validation loss:\t\t1.387467\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4109 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390518\n",
      "  validation loss:\t\t1.387112\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4110 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391349\n",
      "  validation loss:\t\t1.387106\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4111 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378977\n",
      "  validation loss:\t\t1.386917\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4112 of 5000 took 0.023s\n",
      "  training loss:\t\t1.373230\n",
      "  validation loss:\t\t1.386876\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4113 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396557\n",
      "  validation loss:\t\t1.387019\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4114 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388051\n",
      "  validation loss:\t\t1.387047\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4115 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379046\n",
      "  validation loss:\t\t1.387115\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4116 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380441\n",
      "  validation loss:\t\t1.387605\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4117 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391382\n",
      "  validation loss:\t\t1.388005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4118 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380568\n",
      "  validation loss:\t\t1.388417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4119 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397556\n",
      "  validation loss:\t\t1.388363\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4120 of 5000 took 0.024s\n",
      "  training loss:\t\t1.377969\n",
      "  validation loss:\t\t1.388474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4121 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386629\n",
      "  validation loss:\t\t1.388409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4122 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384111\n",
      "  validation loss:\t\t1.388216\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4123 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389573\n",
      "  validation loss:\t\t1.388285\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4124 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378468\n",
      "  validation loss:\t\t1.388269\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4125 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393994\n",
      "  validation loss:\t\t1.388000\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4126 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394197\n",
      "  validation loss:\t\t1.387744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4127 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392362\n",
      "  validation loss:\t\t1.387534\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4128 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390682\n",
      "  validation loss:\t\t1.387043\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4129 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396546\n",
      "  validation loss:\t\t1.386981\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4130 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389021\n",
      "  validation loss:\t\t1.386795\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4131 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385391\n",
      "  validation loss:\t\t1.386812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4132 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393981\n",
      "  validation loss:\t\t1.386754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4133 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387085\n",
      "  validation loss:\t\t1.386780\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4134 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379866\n",
      "  validation loss:\t\t1.386744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4135 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386964\n",
      "  validation loss:\t\t1.386785\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4136 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389073\n",
      "  validation loss:\t\t1.386805\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4137 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396692\n",
      "  validation loss:\t\t1.386794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4138 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394960\n",
      "  validation loss:\t\t1.386824\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4139 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385661\n",
      "  validation loss:\t\t1.386813\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4140 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393391\n",
      "  validation loss:\t\t1.386815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4141 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385686\n",
      "  validation loss:\t\t1.386936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4142 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383930\n",
      "  validation loss:\t\t1.386957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4143 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390483\n",
      "  validation loss:\t\t1.387099\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4144 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393261\n",
      "  validation loss:\t\t1.387083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4145 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376262\n",
      "  validation loss:\t\t1.387266\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4146 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395029\n",
      "  validation loss:\t\t1.387474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4147 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390199\n",
      "  validation loss:\t\t1.387536\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4148 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391904\n",
      "  validation loss:\t\t1.387565\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4149 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389115\n",
      "  validation loss:\t\t1.387512\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4150 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378160\n",
      "  validation loss:\t\t1.387468\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4151 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392381\n",
      "  validation loss:\t\t1.387344\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4152 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392639\n",
      "  validation loss:\t\t1.387209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4153 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389812\n",
      "  validation loss:\t\t1.387121\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4154 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376707\n",
      "  validation loss:\t\t1.386931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4155 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380713\n",
      "  validation loss:\t\t1.387002\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4156 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396506\n",
      "  validation loss:\t\t1.387054\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4157 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389437\n",
      "  validation loss:\t\t1.387258\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4158 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386779\n",
      "  validation loss:\t\t1.387240\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4159 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384402\n",
      "  validation loss:\t\t1.387091\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4160 of 5000 took 0.021s\n",
      "  training loss:\t\t1.377421\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4161 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386336\n",
      "  validation loss:\t\t1.387404\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4162 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390149\n",
      "  validation loss:\t\t1.387464\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4163 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398194\n",
      "  validation loss:\t\t1.387459\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4164 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391683\n",
      "  validation loss:\t\t1.387520\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4165 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385108\n",
      "  validation loss:\t\t1.387558\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4166 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386933\n",
      "  validation loss:\t\t1.387434\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4167 of 5000 took 0.025s\n",
      "  training loss:\t\t1.390224\n",
      "  validation loss:\t\t1.387431\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4168 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384883\n",
      "  validation loss:\t\t1.387516\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4169 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388595\n",
      "  validation loss:\t\t1.387297\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4170 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381426\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4171 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387181\n",
      "  validation loss:\t\t1.387076\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4172 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388798\n",
      "  validation loss:\t\t1.386944\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4173 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390122\n",
      "  validation loss:\t\t1.386816\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4174 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386959\n",
      "  validation loss:\t\t1.386774\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4175 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386259\n",
      "  validation loss:\t\t1.386773\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4176 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394942\n",
      "  validation loss:\t\t1.386737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4177 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379782\n",
      "  validation loss:\t\t1.386720\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4178 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388114\n",
      "  validation loss:\t\t1.386766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4179 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392236\n",
      "  validation loss:\t\t1.386819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4180 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396139\n",
      "  validation loss:\t\t1.387071\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4181 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385512\n",
      "  validation loss:\t\t1.387241\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4182 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384145\n",
      "  validation loss:\t\t1.387581\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4183 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403008\n",
      "  validation loss:\t\t1.387484\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4184 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379341\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4185 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383831\n",
      "  validation loss:\t\t1.387407\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4186 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390913\n",
      "  validation loss:\t\t1.387268\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4187 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384090\n",
      "  validation loss:\t\t1.387246\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4188 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399625\n",
      "  validation loss:\t\t1.387096\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4189 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385515\n",
      "  validation loss:\t\t1.387065\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4190 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388829\n",
      "  validation loss:\t\t1.387074\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4191 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378144\n",
      "  validation loss:\t\t1.386981\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4192 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383233\n",
      "  validation loss:\t\t1.386925\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4193 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381645\n",
      "  validation loss:\t\t1.386858\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4194 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387370\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4195 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383602\n",
      "  validation loss:\t\t1.387160\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4196 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384441\n",
      "  validation loss:\t\t1.387481\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4197 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387704\n",
      "  validation loss:\t\t1.387583\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4198 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387055\n",
      "  validation loss:\t\t1.387564\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4199 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394378\n",
      "  validation loss:\t\t1.387239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4200 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393025\n",
      "  validation loss:\t\t1.387041\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4201 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392596\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4202 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381013\n",
      "  validation loss:\t\t1.386954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4203 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380490\n",
      "  validation loss:\t\t1.386840\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4204 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382000\n",
      "  validation loss:\t\t1.386758\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4205 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376872\n",
      "  validation loss:\t\t1.386852\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4206 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386517\n",
      "  validation loss:\t\t1.386870\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4207 of 5000 took 0.024s\n",
      "  training loss:\t\t1.400330\n",
      "  validation loss:\t\t1.387043\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4208 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391276\n",
      "  validation loss:\t\t1.386931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4209 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388743\n",
      "  validation loss:\t\t1.387026\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4210 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398708\n",
      "  validation loss:\t\t1.387027\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4211 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381067\n",
      "  validation loss:\t\t1.386924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4212 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391424\n",
      "  validation loss:\t\t1.386873\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4213 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387546\n",
      "  validation loss:\t\t1.386902\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4214 of 5000 took 0.026s\n",
      "  training loss:\t\t1.387778\n",
      "  validation loss:\t\t1.386841\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4215 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386264\n",
      "  validation loss:\t\t1.386931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4216 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391197\n",
      "  validation loss:\t\t1.386979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4217 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389862\n",
      "  validation loss:\t\t1.386957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4218 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386838\n",
      "  validation loss:\t\t1.386860\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4219 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392781\n",
      "  validation loss:\t\t1.386865\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4220 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392694\n",
      "  validation loss:\t\t1.386748\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4221 of 5000 took 0.024s\n",
      "  training loss:\t\t1.398692\n",
      "  validation loss:\t\t1.386708\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4222 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387278\n",
      "  validation loss:\t\t1.386709\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4223 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390824\n",
      "  validation loss:\t\t1.386725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4224 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392243\n",
      "  validation loss:\t\t1.386733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4225 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381016\n",
      "  validation loss:\t\t1.386810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4226 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387896\n",
      "  validation loss:\t\t1.386913\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4227 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379486\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4228 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391123\n",
      "  validation loss:\t\t1.386947\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4229 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390761\n",
      "  validation loss:\t\t1.387094\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4230 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390674\n",
      "  validation loss:\t\t1.387023\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4231 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387457\n",
      "  validation loss:\t\t1.386915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4232 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386912\n",
      "  validation loss:\t\t1.386883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4233 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385149\n",
      "  validation loss:\t\t1.386950\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4234 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384743\n",
      "  validation loss:\t\t1.386948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4235 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391954\n",
      "  validation loss:\t\t1.387015\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4236 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382784\n",
      "  validation loss:\t\t1.387154\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4237 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389274\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4238 of 5000 took 0.023s\n",
      "  training loss:\t\t1.376742\n",
      "  validation loss:\t\t1.387470\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4239 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391603\n",
      "  validation loss:\t\t1.387478\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4240 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394566\n",
      "  validation loss:\t\t1.387474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4241 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390802\n",
      "  validation loss:\t\t1.387436\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4242 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378071\n",
      "  validation loss:\t\t1.387409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4243 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399951\n",
      "  validation loss:\t\t1.387183\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4244 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393254\n",
      "  validation loss:\t\t1.387147\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4245 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390437\n",
      "  validation loss:\t\t1.387029\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4246 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391573\n",
      "  validation loss:\t\t1.387038\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4247 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386501\n",
      "  validation loss:\t\t1.386902\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4248 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382297\n",
      "  validation loss:\t\t1.386988\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4249 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386823\n",
      "  validation loss:\t\t1.387104\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4250 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387229\n",
      "  validation loss:\t\t1.387311\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4251 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391380\n",
      "  validation loss:\t\t1.387402\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4252 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400648\n",
      "  validation loss:\t\t1.387204\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4253 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397095\n",
      "  validation loss:\t\t1.387101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4254 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389199\n",
      "  validation loss:\t\t1.387083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4255 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389585\n",
      "  validation loss:\t\t1.387069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4256 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395445\n",
      "  validation loss:\t\t1.387386\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4257 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377536\n",
      "  validation loss:\t\t1.387531\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4258 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386885\n",
      "  validation loss:\t\t1.387814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4259 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396552\n",
      "  validation loss:\t\t1.388115\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4260 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383327\n",
      "  validation loss:\t\t1.388118\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4261 of 5000 took 0.026s\n",
      "  training loss:\t\t1.388357\n",
      "  validation loss:\t\t1.387954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4262 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377974\n",
      "  validation loss:\t\t1.387616\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4263 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388500\n",
      "  validation loss:\t\t1.387561\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4264 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385382\n",
      "  validation loss:\t\t1.387506\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4265 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378589\n",
      "  validation loss:\t\t1.387316\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4266 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398141\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4267 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395917\n",
      "  validation loss:\t\t1.387105\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4268 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392717\n",
      "  validation loss:\t\t1.387140\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4269 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389262\n",
      "  validation loss:\t\t1.387201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4270 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381940\n",
      "  validation loss:\t\t1.387514\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4271 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393034\n",
      "  validation loss:\t\t1.387696\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4272 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390787\n",
      "  validation loss:\t\t1.387524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4273 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383564\n",
      "  validation loss:\t\t1.387550\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4274 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395417\n",
      "  validation loss:\t\t1.387235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4275 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387189\n",
      "  validation loss:\t\t1.387210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4276 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379203\n",
      "  validation loss:\t\t1.387083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4277 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382171\n",
      "  validation loss:\t\t1.386876\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4278 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387776\n",
      "  validation loss:\t\t1.386868\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4279 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390981\n",
      "  validation loss:\t\t1.386812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4280 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393875\n",
      "  validation loss:\t\t1.386729\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4281 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395296\n",
      "  validation loss:\t\t1.386686\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4282 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389769\n",
      "  validation loss:\t\t1.386697\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4283 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383164\n",
      "  validation loss:\t\t1.386707\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4284 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395730\n",
      "  validation loss:\t\t1.386692\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4285 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382854\n",
      "  validation loss:\t\t1.386673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4286 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383720\n",
      "  validation loss:\t\t1.386655\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4287 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388937\n",
      "  validation loss:\t\t1.386669\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4288 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388161\n",
      "  validation loss:\t\t1.386675\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4289 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383307\n",
      "  validation loss:\t\t1.386672\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4290 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389294\n",
      "  validation loss:\t\t1.386660\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4291 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389041\n",
      "  validation loss:\t\t1.386669\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4292 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388635\n",
      "  validation loss:\t\t1.386741\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4293 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380538\n",
      "  validation loss:\t\t1.386772\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4294 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390564\n",
      "  validation loss:\t\t1.386934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4295 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387473\n",
      "  validation loss:\t\t1.386979\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4296 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379110\n",
      "  validation loss:\t\t1.387161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4297 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390852\n",
      "  validation loss:\t\t1.387321\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4298 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395241\n",
      "  validation loss:\t\t1.387390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4299 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377523\n",
      "  validation loss:\t\t1.387426\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4300 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389379\n",
      "  validation loss:\t\t1.387491\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4301 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391721\n",
      "  validation loss:\t\t1.387439\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4302 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392313\n",
      "  validation loss:\t\t1.387190\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4303 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386785\n",
      "  validation loss:\t\t1.387046\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4304 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388617\n",
      "  validation loss:\t\t1.387020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4305 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389272\n",
      "  validation loss:\t\t1.387052\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4306 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395363\n",
      "  validation loss:\t\t1.386974\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4307 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395017\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4308 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387354\n",
      "  validation loss:\t\t1.386786\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4309 of 5000 took 0.025s\n",
      "  training loss:\t\t1.397051\n",
      "  validation loss:\t\t1.386827\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4310 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397829\n",
      "  validation loss:\t\t1.386779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4311 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387133\n",
      "  validation loss:\t\t1.386702\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4312 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394651\n",
      "  validation loss:\t\t1.386649\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4313 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382252\n",
      "  validation loss:\t\t1.386814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4314 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393989\n",
      "  validation loss:\t\t1.386996\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4315 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390338\n",
      "  validation loss:\t\t1.387013\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4316 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381921\n",
      "  validation loss:\t\t1.387180\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4317 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391514\n",
      "  validation loss:\t\t1.387009\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4318 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377121\n",
      "  validation loss:\t\t1.387007\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4319 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385063\n",
      "  validation loss:\t\t1.387006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4320 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383324\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4321 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396713\n",
      "  validation loss:\t\t1.386904\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4322 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393218\n",
      "  validation loss:\t\t1.386883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4323 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389393\n",
      "  validation loss:\t\t1.386915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4324 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386679\n",
      "  validation loss:\t\t1.387212\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4325 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381098\n",
      "  validation loss:\t\t1.387214\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4326 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394895\n",
      "  validation loss:\t\t1.387244\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4327 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390918\n",
      "  validation loss:\t\t1.387136\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4328 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393743\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4329 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392798\n",
      "  validation loss:\t\t1.386955\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4330 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386616\n",
      "  validation loss:\t\t1.386822\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4331 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388867\n",
      "  validation loss:\t\t1.386803\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4332 of 5000 took 0.024s\n",
      "  training loss:\t\t1.390567\n",
      "  validation loss:\t\t1.386806\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4333 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377619\n",
      "  validation loss:\t\t1.386733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4334 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394475\n",
      "  validation loss:\t\t1.386766\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4335 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382592\n",
      "  validation loss:\t\t1.386872\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4336 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389976\n",
      "  validation loss:\t\t1.387068\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4337 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387863\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4338 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402762\n",
      "  validation loss:\t\t1.387298\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4339 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391492\n",
      "  validation loss:\t\t1.387371\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4340 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384411\n",
      "  validation loss:\t\t1.387172\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4341 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376249\n",
      "  validation loss:\t\t1.387190\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4342 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396831\n",
      "  validation loss:\t\t1.387116\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4343 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391599\n",
      "  validation loss:\t\t1.387168\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4344 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384845\n",
      "  validation loss:\t\t1.387162\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4345 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394564\n",
      "  validation loss:\t\t1.387138\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4346 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383087\n",
      "  validation loss:\t\t1.387173\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4347 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388368\n",
      "  validation loss:\t\t1.387235\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4348 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387811\n",
      "  validation loss:\t\t1.387171\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4349 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379624\n",
      "  validation loss:\t\t1.387433\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4350 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389308\n",
      "  validation loss:\t\t1.387625\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4351 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386992\n",
      "  validation loss:\t\t1.387694\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4352 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379821\n",
      "  validation loss:\t\t1.387914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4353 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390502\n",
      "  validation loss:\t\t1.388176\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4354 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394884\n",
      "  validation loss:\t\t1.387816\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4355 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392246\n",
      "  validation loss:\t\t1.387561\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4356 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395652\n",
      "  validation loss:\t\t1.387388\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4357 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390403\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4358 of 5000 took 0.025s\n",
      "  training loss:\t\t1.392208\n",
      "  validation loss:\t\t1.386838\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4359 of 5000 took 0.025s\n",
      "  training loss:\t\t1.383663\n",
      "  validation loss:\t\t1.386779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4360 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389047\n",
      "  validation loss:\t\t1.386737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4361 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384861\n",
      "  validation loss:\t\t1.386827\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4362 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386504\n",
      "  validation loss:\t\t1.386907\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4363 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382685\n",
      "  validation loss:\t\t1.387049\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4364 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390738\n",
      "  validation loss:\t\t1.387101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4365 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383441\n",
      "  validation loss:\t\t1.387274\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4366 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386499\n",
      "  validation loss:\t\t1.387289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4367 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395899\n",
      "  validation loss:\t\t1.387186\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4368 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391155\n",
      "  validation loss:\t\t1.387422\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4369 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384471\n",
      "  validation loss:\t\t1.387460\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4370 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396810\n",
      "  validation loss:\t\t1.387341\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4371 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384244\n",
      "  validation loss:\t\t1.387286\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4372 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384397\n",
      "  validation loss:\t\t1.387337\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4373 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381136\n",
      "  validation loss:\t\t1.387330\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4374 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387157\n",
      "  validation loss:\t\t1.387334\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4375 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396738\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4376 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388616\n",
      "  validation loss:\t\t1.387265\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4377 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377253\n",
      "  validation loss:\t\t1.387426\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4378 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386458\n",
      "  validation loss:\t\t1.387255\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4379 of 5000 took 0.022s\n",
      "  training loss:\t\t1.377012\n",
      "  validation loss:\t\t1.387230\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4380 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383863\n",
      "  validation loss:\t\t1.387394\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4381 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382864\n",
      "  validation loss:\t\t1.387569\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4382 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380888\n",
      "  validation loss:\t\t1.388066\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4383 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389429\n",
      "  validation loss:\t\t1.388524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4384 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387753\n",
      "  validation loss:\t\t1.388623\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4385 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398278\n",
      "  validation loss:\t\t1.388558\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4386 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389627\n",
      "  validation loss:\t\t1.388159\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4387 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389301\n",
      "  validation loss:\t\t1.387549\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4388 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386941\n",
      "  validation loss:\t\t1.387002\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4389 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391293\n",
      "  validation loss:\t\t1.386771\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4390 of 5000 took 0.024s\n",
      "  training loss:\t\t1.383410\n",
      "  validation loss:\t\t1.386799\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4391 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382473\n",
      "  validation loss:\t\t1.386844\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4392 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392843\n",
      "  validation loss:\t\t1.387256\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4393 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380972\n",
      "  validation loss:\t\t1.387827\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4394 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388472\n",
      "  validation loss:\t\t1.388092\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4395 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391666\n",
      "  validation loss:\t\t1.388216\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4396 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388317\n",
      "  validation loss:\t\t1.388403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4397 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389396\n",
      "  validation loss:\t\t1.388452\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4398 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390297\n",
      "  validation loss:\t\t1.388237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4399 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391227\n",
      "  validation loss:\t\t1.387948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4400 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381744\n",
      "  validation loss:\t\t1.387642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4401 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396088\n",
      "  validation loss:\t\t1.387380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4402 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393458\n",
      "  validation loss:\t\t1.387034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4403 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384678\n",
      "  validation loss:\t\t1.387108\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4404 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388772\n",
      "  validation loss:\t\t1.386955\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4405 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390351\n",
      "  validation loss:\t\t1.386822\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4406 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391043\n",
      "  validation loss:\t\t1.386843\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4407 of 5000 took 0.025s\n",
      "  training loss:\t\t1.384770\n",
      "  validation loss:\t\t1.387020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4408 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387241\n",
      "  validation loss:\t\t1.387201\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4409 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386590\n",
      "  validation loss:\t\t1.387340\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4410 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382646\n",
      "  validation loss:\t\t1.387435\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4411 of 5000 took 0.021s\n",
      "  training loss:\t\t1.392571\n",
      "  validation loss:\t\t1.387498\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4412 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380462\n",
      "  validation loss:\t\t1.387879\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4413 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381935\n",
      "  validation loss:\t\t1.388092\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4414 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391495\n",
      "  validation loss:\t\t1.387887\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4415 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392349\n",
      "  validation loss:\t\t1.388131\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4416 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390149\n",
      "  validation loss:\t\t1.388067\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4417 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383049\n",
      "  validation loss:\t\t1.388016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4418 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390973\n",
      "  validation loss:\t\t1.387890\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4419 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388118\n",
      "  validation loss:\t\t1.387597\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4420 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393884\n",
      "  validation loss:\t\t1.387254\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4421 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380114\n",
      "  validation loss:\t\t1.387151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4422 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389876\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4423 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384672\n",
      "  validation loss:\t\t1.387309\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4424 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390205\n",
      "  validation loss:\t\t1.387144\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4425 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394424\n",
      "  validation loss:\t\t1.387138\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4426 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379348\n",
      "  validation loss:\t\t1.387063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4427 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389248\n",
      "  validation loss:\t\t1.386866\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4428 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383164\n",
      "  validation loss:\t\t1.386708\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4429 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383995\n",
      "  validation loss:\t\t1.386653\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4430 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394781\n",
      "  validation loss:\t\t1.386693\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4431 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386198\n",
      "  validation loss:\t\t1.386775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4432 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393107\n",
      "  validation loss:\t\t1.386751\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4433 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378059\n",
      "  validation loss:\t\t1.386814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4434 of 5000 took 0.024s\n",
      "  training loss:\t\t1.381782\n",
      "  validation loss:\t\t1.386773\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4435 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396643\n",
      "  validation loss:\t\t1.386721\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4436 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382518\n",
      "  validation loss:\t\t1.386646\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4437 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390408\n",
      "  validation loss:\t\t1.386641\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4438 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393520\n",
      "  validation loss:\t\t1.386642\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4439 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391580\n",
      "  validation loss:\t\t1.386667\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4440 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381851\n",
      "  validation loss:\t\t1.386845\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4441 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390361\n",
      "  validation loss:\t\t1.387133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4442 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385649\n",
      "  validation loss:\t\t1.387382\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4443 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388773\n",
      "  validation loss:\t\t1.387376\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4444 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383847\n",
      "  validation loss:\t\t1.387464\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4445 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387339\n",
      "  validation loss:\t\t1.387350\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4446 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390853\n",
      "  validation loss:\t\t1.387673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4447 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390809\n",
      "  validation loss:\t\t1.387831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4448 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388272\n",
      "  validation loss:\t\t1.387858\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4449 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393397\n",
      "  validation loss:\t\t1.388113\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4450 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393084\n",
      "  validation loss:\t\t1.387831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4451 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390857\n",
      "  validation loss:\t\t1.387860\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4452 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384758\n",
      "  validation loss:\t\t1.387776\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4453 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394938\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4454 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393359\n",
      "  validation loss:\t\t1.387414\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4455 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381858\n",
      "  validation loss:\t\t1.387187\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4456 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383392\n",
      "  validation loss:\t\t1.386978\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4457 of 5000 took 0.025s\n",
      "  training loss:\t\t1.386617\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4458 of 5000 took 0.025s\n",
      "  training loss:\t\t1.383392\n",
      "  validation loss:\t\t1.386804\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4459 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384844\n",
      "  validation loss:\t\t1.386715\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4460 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393719\n",
      "  validation loss:\t\t1.386683\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4461 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395882\n",
      "  validation loss:\t\t1.386733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4462 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389345\n",
      "  validation loss:\t\t1.386724\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4463 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380108\n",
      "  validation loss:\t\t1.386744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4464 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380268\n",
      "  validation loss:\t\t1.386777\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4465 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382557\n",
      "  validation loss:\t\t1.386914\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4466 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381066\n",
      "  validation loss:\t\t1.386885\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4467 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389383\n",
      "  validation loss:\t\t1.386745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4468 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393886\n",
      "  validation loss:\t\t1.386743\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4469 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386687\n",
      "  validation loss:\t\t1.386773\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4470 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386688\n",
      "  validation loss:\t\t1.386770\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4471 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386471\n",
      "  validation loss:\t\t1.386851\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4472 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384438\n",
      "  validation loss:\t\t1.386929\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4473 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386875\n",
      "  validation loss:\t\t1.386916\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4474 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383692\n",
      "  validation loss:\t\t1.387033\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4475 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386824\n",
      "  validation loss:\t\t1.387236\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4476 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378131\n",
      "  validation loss:\t\t1.387537\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4477 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389466\n",
      "  validation loss:\t\t1.387587\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4478 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392381\n",
      "  validation loss:\t\t1.387790\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4479 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384084\n",
      "  validation loss:\t\t1.387788\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4480 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395967\n",
      "  validation loss:\t\t1.387728\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4481 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385176\n",
      "  validation loss:\t\t1.387725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4482 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389838\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4483 of 5000 took 0.024s\n",
      "  training loss:\t\t1.381305\n",
      "  validation loss:\t\t1.387355\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4484 of 5000 took 0.023s\n",
      "  training loss:\t\t1.399343\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4485 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391823\n",
      "  validation loss:\t\t1.387056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4486 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393611\n",
      "  validation loss:\t\t1.387037\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4487 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392143\n",
      "  validation loss:\t\t1.387182\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4488 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382023\n",
      "  validation loss:\t\t1.387666\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4489 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387598\n",
      "  validation loss:\t\t1.388007\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4490 of 5000 took 0.021s\n",
      "  training loss:\t\t1.388368\n",
      "  validation loss:\t\t1.388474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4491 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387926\n",
      "  validation loss:\t\t1.388717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4492 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387134\n",
      "  validation loss:\t\t1.388738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4493 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385427\n",
      "  validation loss:\t\t1.388712\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4494 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379802\n",
      "  validation loss:\t\t1.388634\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4495 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375848\n",
      "  validation loss:\t\t1.388262\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4496 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375598\n",
      "  validation loss:\t\t1.388095\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4497 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384462\n",
      "  validation loss:\t\t1.387965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4498 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398856\n",
      "  validation loss:\t\t1.387480\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4499 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385825\n",
      "  validation loss:\t\t1.387195\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4500 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389483\n",
      "  validation loss:\t\t1.386855\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4501 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385162\n",
      "  validation loss:\t\t1.386668\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4502 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394617\n",
      "  validation loss:\t\t1.386637\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4503 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374443\n",
      "  validation loss:\t\t1.386673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4504 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388652\n",
      "  validation loss:\t\t1.386744\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4505 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379682\n",
      "  validation loss:\t\t1.386832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4506 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395387\n",
      "  validation loss:\t\t1.386791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4507 of 5000 took 0.025s\n",
      "  training loss:\t\t1.391688\n",
      "  validation loss:\t\t1.386898\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4508 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386655\n",
      "  validation loss:\t\t1.387017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4509 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387113\n",
      "  validation loss:\t\t1.387050\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4510 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383417\n",
      "  validation loss:\t\t1.387264\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4511 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392008\n",
      "  validation loss:\t\t1.387371\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4512 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388893\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4513 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396240\n",
      "  validation loss:\t\t1.387145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4514 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395110\n",
      "  validation loss:\t\t1.387281\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4515 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376449\n",
      "  validation loss:\t\t1.387430\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4516 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391001\n",
      "  validation loss:\t\t1.387450\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4517 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395665\n",
      "  validation loss:\t\t1.387499\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4518 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385927\n",
      "  validation loss:\t\t1.387553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4519 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395847\n",
      "  validation loss:\t\t1.387209\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4520 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392637\n",
      "  validation loss:\t\t1.387069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4521 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379923\n",
      "  validation loss:\t\t1.387002\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4522 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394038\n",
      "  validation loss:\t\t1.386857\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4523 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385664\n",
      "  validation loss:\t\t1.386816\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4524 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397195\n",
      "  validation loss:\t\t1.387017\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4525 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382058\n",
      "  validation loss:\t\t1.387261\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4526 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381022\n",
      "  validation loss:\t\t1.387425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4527 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385788\n",
      "  validation loss:\t\t1.387257\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4528 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387406\n",
      "  validation loss:\t\t1.387341\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4529 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388151\n",
      "  validation loss:\t\t1.387360\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4530 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389920\n",
      "  validation loss:\t\t1.387408\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4531 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384689\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4532 of 5000 took 0.024s\n",
      "  training loss:\t\t1.396448\n",
      "  validation loss:\t\t1.387435\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4533 of 5000 took 0.025s\n",
      "  training loss:\t\t1.387233\n",
      "  validation loss:\t\t1.387597\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4534 of 5000 took 0.022s\n",
      "  training loss:\t\t1.397247\n",
      "  validation loss:\t\t1.387355\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4535 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379201\n",
      "  validation loss:\t\t1.387283\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4536 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389490\n",
      "  validation loss:\t\t1.387184\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4537 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388764\n",
      "  validation loss:\t\t1.387191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4538 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380459\n",
      "  validation loss:\t\t1.387083\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4539 of 5000 took 0.021s\n",
      "  training loss:\t\t1.383304\n",
      "  validation loss:\t\t1.386958\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4540 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392510\n",
      "  validation loss:\t\t1.386997\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4541 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385298\n",
      "  validation loss:\t\t1.386961\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4542 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379838\n",
      "  validation loss:\t\t1.386861\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4543 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391634\n",
      "  validation loss:\t\t1.386936\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4544 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395210\n",
      "  validation loss:\t\t1.386962\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4545 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383087\n",
      "  validation loss:\t\t1.386891\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4546 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374915\n",
      "  validation loss:\t\t1.386901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4547 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383469\n",
      "  validation loss:\t\t1.386994\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4548 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386991\n",
      "  validation loss:\t\t1.387025\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4549 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384423\n",
      "  validation loss:\t\t1.387224\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4550 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396346\n",
      "  validation loss:\t\t1.387079\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4551 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394424\n",
      "  validation loss:\t\t1.387035\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4552 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385173\n",
      "  validation loss:\t\t1.386940\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4553 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381694\n",
      "  validation loss:\t\t1.386954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4554 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380483\n",
      "  validation loss:\t\t1.386838\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4555 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381835\n",
      "  validation loss:\t\t1.386881\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4556 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393305\n",
      "  validation loss:\t\t1.386915\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4557 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387658\n",
      "  validation loss:\t\t1.387117\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4558 of 5000 took 0.026s\n",
      "  training loss:\t\t1.399105\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4559 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387972\n",
      "  validation loss:\t\t1.387246\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4560 of 5000 took 0.023s\n",
      "  training loss:\t\t1.375054\n",
      "  validation loss:\t\t1.387476\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4561 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379498\n",
      "  validation loss:\t\t1.387704\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4562 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387005\n",
      "  validation loss:\t\t1.387808\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4563 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387170\n",
      "  validation loss:\t\t1.387856\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4564 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389618\n",
      "  validation loss:\t\t1.387648\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4565 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393611\n",
      "  validation loss:\t\t1.387465\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4566 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390339\n",
      "  validation loss:\t\t1.387332\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4567 of 5000 took 0.021s\n",
      "  training loss:\t\t1.401097\n",
      "  validation loss:\t\t1.387085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4568 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381913\n",
      "  validation loss:\t\t1.387119\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4569 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397704\n",
      "  validation loss:\t\t1.387403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4570 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385663\n",
      "  validation loss:\t\t1.387287\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4571 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389531\n",
      "  validation loss:\t\t1.387427\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4572 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387166\n",
      "  validation loss:\t\t1.387327\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4573 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391161\n",
      "  validation loss:\t\t1.387161\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4574 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394560\n",
      "  validation loss:\t\t1.387246\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4575 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389554\n",
      "  validation loss:\t\t1.387510\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4576 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389381\n",
      "  validation loss:\t\t1.387482\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4577 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394456\n",
      "  validation loss:\t\t1.387409\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4578 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396392\n",
      "  validation loss:\t\t1.387272\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4579 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393696\n",
      "  validation loss:\t\t1.387167\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4580 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387234\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4581 of 5000 took 0.021s\n",
      "  training loss:\t\t1.399069\n",
      "  validation loss:\t\t1.386896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4582 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392311\n",
      "  validation loss:\t\t1.386816\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4583 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387097\n",
      "  validation loss:\t\t1.386819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4584 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387842\n",
      "  validation loss:\t\t1.386783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4585 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395501\n",
      "  validation loss:\t\t1.386723\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4586 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380844\n",
      "  validation loss:\t\t1.386662\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4587 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393798\n",
      "  validation loss:\t\t1.386725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4588 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389205\n",
      "  validation loss:\t\t1.386703\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4589 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389600\n",
      "  validation loss:\t\t1.386745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4590 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394058\n",
      "  validation loss:\t\t1.386654\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4591 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384958\n",
      "  validation loss:\t\t1.386693\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4592 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398405\n",
      "  validation loss:\t\t1.386693\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4593 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386990\n",
      "  validation loss:\t\t1.386759\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4594 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389293\n",
      "  validation loss:\t\t1.386833\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4595 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379587\n",
      "  validation loss:\t\t1.386990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4596 of 5000 took 0.022s\n",
      "  training loss:\t\t1.403827\n",
      "  validation loss:\t\t1.387073\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4597 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385646\n",
      "  validation loss:\t\t1.387277\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4598 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388968\n",
      "  validation loss:\t\t1.387214\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4599 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386281\n",
      "  validation loss:\t\t1.387169\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4600 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390086\n",
      "  validation loss:\t\t1.387308\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4601 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393709\n",
      "  validation loss:\t\t1.387250\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4602 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397168\n",
      "  validation loss:\t\t1.387331\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4603 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390779\n",
      "  validation loss:\t\t1.387239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4604 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384332\n",
      "  validation loss:\t\t1.387228\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4605 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388558\n",
      "  validation loss:\t\t1.387265\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4606 of 5000 took 0.022s\n",
      "  training loss:\t\t1.402509\n",
      "  validation loss:\t\t1.387070\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4607 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382495\n",
      "  validation loss:\t\t1.386919\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4608 of 5000 took 0.028s\n",
      "  training loss:\t\t1.386035\n",
      "  validation loss:\t\t1.386953\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4609 of 5000 took 0.024s\n",
      "  training loss:\t\t1.383723\n",
      "  validation loss:\t\t1.386931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4610 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391137\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4611 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391192\n",
      "  validation loss:\t\t1.386779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4612 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397757\n",
      "  validation loss:\t\t1.386683\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4613 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398128\n",
      "  validation loss:\t\t1.386654\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4614 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385079\n",
      "  validation loss:\t\t1.386673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4615 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381014\n",
      "  validation loss:\t\t1.386670\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4616 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384358\n",
      "  validation loss:\t\t1.386755\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4617 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389942\n",
      "  validation loss:\t\t1.386840\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4618 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392778\n",
      "  validation loss:\t\t1.387092\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4619 of 5000 took 0.021s\n",
      "  training loss:\t\t1.382639\n",
      "  validation loss:\t\t1.387020\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4620 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390586\n",
      "  validation loss:\t\t1.386903\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4621 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395242\n",
      "  validation loss:\t\t1.386765\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4622 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388358\n",
      "  validation loss:\t\t1.386707\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4623 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395991\n",
      "  validation loss:\t\t1.386783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4624 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389688\n",
      "  validation loss:\t\t1.386819\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4625 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388546\n",
      "  validation loss:\t\t1.386708\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4626 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396646\n",
      "  validation loss:\t\t1.386651\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4627 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387486\n",
      "  validation loss:\t\t1.386665\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4628 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380584\n",
      "  validation loss:\t\t1.386667\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4629 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387346\n",
      "  validation loss:\t\t1.386723\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4630 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388409\n",
      "  validation loss:\t\t1.386783\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4631 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385200\n",
      "  validation loss:\t\t1.386740\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4632 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396071\n",
      "  validation loss:\t\t1.386814\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4633 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384050\n",
      "  validation loss:\t\t1.386954\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4634 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400337\n",
      "  validation loss:\t\t1.387080\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4635 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391845\n",
      "  validation loss:\t\t1.387098\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4636 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398483\n",
      "  validation loss:\t\t1.387185\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4637 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389341\n",
      "  validation loss:\t\t1.387233\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4638 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386905\n",
      "  validation loss:\t\t1.386896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4639 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383249\n",
      "  validation loss:\t\t1.386921\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4640 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391681\n",
      "  validation loss:\t\t1.386821\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4641 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380911\n",
      "  validation loss:\t\t1.386747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4642 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386588\n",
      "  validation loss:\t\t1.386736\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4643 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382302\n",
      "  validation loss:\t\t1.386717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4644 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383343\n",
      "  validation loss:\t\t1.386811\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4645 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390863\n",
      "  validation loss:\t\t1.386885\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4646 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387945\n",
      "  validation loss:\t\t1.386815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4647 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391559\n",
      "  validation loss:\t\t1.386817\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4648 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380903\n",
      "  validation loss:\t\t1.386990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4649 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381147\n",
      "  validation loss:\t\t1.386904\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4650 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390219\n",
      "  validation loss:\t\t1.386771\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4651 of 5000 took 0.024s\n",
      "  training loss:\t\t1.386183\n",
      "  validation loss:\t\t1.386809\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4652 of 5000 took 0.022s\n",
      "  training loss:\t\t1.405301\n",
      "  validation loss:\t\t1.386645\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4653 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386311\n",
      "  validation loss:\t\t1.386631\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4654 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388144\n",
      "  validation loss:\t\t1.386621\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4655 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389576\n",
      "  validation loss:\t\t1.386633\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4656 of 5000 took 0.021s\n",
      "  training loss:\t\t1.377388\n",
      "  validation loss:\t\t1.386628\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4657 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393469\n",
      "  validation loss:\t\t1.386659\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4658 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384694\n",
      "  validation loss:\t\t1.386692\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4659 of 5000 took 0.026s\n",
      "  training loss:\t\t1.388275\n",
      "  validation loss:\t\t1.386679\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4660 of 5000 took 0.024s\n",
      "  training loss:\t\t1.384564\n",
      "  validation loss:\t\t1.386714\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4661 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387485\n",
      "  validation loss:\t\t1.386808\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4662 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393776\n",
      "  validation loss:\t\t1.386791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4663 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377705\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4664 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389472\n",
      "  validation loss:\t\t1.386986\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4665 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389016\n",
      "  validation loss:\t\t1.387269\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4666 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389455\n",
      "  validation loss:\t\t1.387490\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4667 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390849\n",
      "  validation loss:\t\t1.387579\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4668 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390132\n",
      "  validation loss:\t\t1.387921\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4669 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395894\n",
      "  validation loss:\t\t1.387749\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4670 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392781\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4671 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376506\n",
      "  validation loss:\t\t1.387305\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4672 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385832\n",
      "  validation loss:\t\t1.387327\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4673 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390540\n",
      "  validation loss:\t\t1.387208\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4674 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390281\n",
      "  validation loss:\t\t1.387073\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4675 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390777\n",
      "  validation loss:\t\t1.386973\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4676 of 5000 took 0.021s\n",
      "  training loss:\t\t1.384850\n",
      "  validation loss:\t\t1.386931\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4677 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390508\n",
      "  validation loss:\t\t1.387122\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4678 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393737\n",
      "  validation loss:\t\t1.387062\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4679 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391283\n",
      "  validation loss:\t\t1.387050\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4680 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382529\n",
      "  validation loss:\t\t1.387210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4681 of 5000 took 0.021s\n",
      "  training loss:\t\t1.380771\n",
      "  validation loss:\t\t1.387600\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4682 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390451\n",
      "  validation loss:\t\t1.387734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4683 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384860\n",
      "  validation loss:\t\t1.388069\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4684 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384783\n",
      "  validation loss:\t\t1.388101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4685 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391566\n",
      "  validation loss:\t\t1.387999\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4686 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384358\n",
      "  validation loss:\t\t1.387829\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4687 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384903\n",
      "  validation loss:\t\t1.387782\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4688 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389504\n",
      "  validation loss:\t\t1.387471\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4689 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387891\n",
      "  validation loss:\t\t1.387315\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4690 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395463\n",
      "  validation loss:\t\t1.387263\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4691 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378198\n",
      "  validation loss:\t\t1.387376\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4692 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379868\n",
      "  validation loss:\t\t1.387582\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4693 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389460\n",
      "  validation loss:\t\t1.387703\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4694 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387328\n",
      "  validation loss:\t\t1.388224\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4695 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388595\n",
      "  validation loss:\t\t1.388864\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4696 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400321\n",
      "  validation loss:\t\t1.389226\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4697 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393205\n",
      "  validation loss:\t\t1.389202\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4698 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394150\n",
      "  validation loss:\t\t1.389410\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4699 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386569\n",
      "  validation loss:\t\t1.389147\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4700 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393854\n",
      "  validation loss:\t\t1.388888\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4701 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397127\n",
      "  validation loss:\t\t1.388607\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4702 of 5000 took 0.024s\n",
      "  training loss:\t\t1.378306\n",
      "  validation loss:\t\t1.388239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4703 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391258\n",
      "  validation loss:\t\t1.387717\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4704 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397496\n",
      "  validation loss:\t\t1.387443\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4705 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388605\n",
      "  validation loss:\t\t1.387298\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4706 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382939\n",
      "  validation loss:\t\t1.387074\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4707 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383844\n",
      "  validation loss:\t\t1.386949\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4708 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386123\n",
      "  validation loss:\t\t1.386763\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4709 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390426\n",
      "  validation loss:\t\t1.386719\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4710 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389699\n",
      "  validation loss:\t\t1.386699\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4711 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386900\n",
      "  validation loss:\t\t1.386905\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4712 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390158\n",
      "  validation loss:\t\t1.387150\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4713 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385835\n",
      "  validation loss:\t\t1.387395\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4714 of 5000 took 0.027s\n",
      "  training loss:\t\t1.392034\n",
      "  validation loss:\t\t1.387747\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4715 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389954\n",
      "  validation loss:\t\t1.387923\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4716 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392839\n",
      "  validation loss:\t\t1.387815\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4717 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392067\n",
      "  validation loss:\t\t1.387687\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4718 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391189\n",
      "  validation loss:\t\t1.387427\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4719 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398095\n",
      "  validation loss:\t\t1.387192\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4720 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384810\n",
      "  validation loss:\t\t1.387048\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4721 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388459\n",
      "  validation loss:\t\t1.386966\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4722 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392703\n",
      "  validation loss:\t\t1.387322\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4723 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389245\n",
      "  validation loss:\t\t1.387461\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4724 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381488\n",
      "  validation loss:\t\t1.387800\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4725 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390435\n",
      "  validation loss:\t\t1.388292\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4726 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390985\n",
      "  validation loss:\t\t1.388151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4727 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386561\n",
      "  validation loss:\t\t1.388239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4728 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391744\n",
      "  validation loss:\t\t1.388350\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4729 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394218\n",
      "  validation loss:\t\t1.387834\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4730 of 5000 took 0.022s\n",
      "  training loss:\t\t1.399756\n",
      "  validation loss:\t\t1.387237\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4731 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385030\n",
      "  validation loss:\t\t1.386922\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4732 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387619\n",
      "  validation loss:\t\t1.386745\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4733 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386461\n",
      "  validation loss:\t\t1.386948\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4734 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393851\n",
      "  validation loss:\t\t1.387417\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4735 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387946\n",
      "  validation loss:\t\t1.387990\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4736 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397164\n",
      "  validation loss:\t\t1.388226\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4737 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386650\n",
      "  validation loss:\t\t1.388310\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4738 of 5000 took 0.021s\n",
      "  training loss:\t\t1.393839\n",
      "  validation loss:\t\t1.388194\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4739 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389138\n",
      "  validation loss:\t\t1.388247\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4740 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380778\n",
      "  validation loss:\t\t1.388006\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4741 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385141\n",
      "  validation loss:\t\t1.387946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4742 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394554\n",
      "  validation loss:\t\t1.387528\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4743 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391189\n",
      "  validation loss:\t\t1.387221\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4744 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391085\n",
      "  validation loss:\t\t1.387084\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4745 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383618\n",
      "  validation loss:\t\t1.386995\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4746 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391619\n",
      "  validation loss:\t\t1.386837\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4747 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393762\n",
      "  validation loss:\t\t1.386738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4748 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386723\n",
      "  validation loss:\t\t1.386690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4749 of 5000 took 0.023s\n",
      "  training loss:\t\t1.374994\n",
      "  validation loss:\t\t1.386758\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4750 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386628\n",
      "  validation loss:\t\t1.387151\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4751 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392118\n",
      "  validation loss:\t\t1.387437\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4752 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389185\n",
      "  validation loss:\t\t1.387754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4753 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385256\n",
      "  validation loss:\t\t1.387676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4754 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392415\n",
      "  validation loss:\t\t1.387926\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4755 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383220\n",
      "  validation loss:\t\t1.387952\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4756 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384476\n",
      "  validation loss:\t\t1.388107\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4757 of 5000 took 0.024s\n",
      "  training loss:\t\t1.382087\n",
      "  validation loss:\t\t1.387959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4758 of 5000 took 0.024s\n",
      "  training loss:\t\t1.394139\n",
      "  validation loss:\t\t1.387456\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4759 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395028\n",
      "  validation loss:\t\t1.386984\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4760 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380868\n",
      "  validation loss:\t\t1.386876\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4761 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389535\n",
      "  validation loss:\t\t1.386724\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4762 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390591\n",
      "  validation loss:\t\t1.386636\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4763 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385039\n",
      "  validation loss:\t\t1.386685\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4764 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390643\n",
      "  validation loss:\t\t1.386901\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4765 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386612\n",
      "  validation loss:\t\t1.386927\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4766 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383531\n",
      "  validation loss:\t\t1.387133\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4767 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391572\n",
      "  validation loss:\t\t1.387034\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4768 of 5000 took 0.025s\n",
      "  training loss:\t\t1.388962\n",
      "  validation loss:\t\t1.387024\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4769 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387546\n",
      "  validation loss:\t\t1.387122\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4770 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395351\n",
      "  validation loss:\t\t1.387155\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4771 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389632\n",
      "  validation loss:\t\t1.387072\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4772 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380310\n",
      "  validation loss:\t\t1.387113\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4773 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392362\n",
      "  validation loss:\t\t1.387063\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4774 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388816\n",
      "  validation loss:\t\t1.387012\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4775 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388763\n",
      "  validation loss:\t\t1.386778\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4776 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387433\n",
      "  validation loss:\t\t1.386724\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4777 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391770\n",
      "  validation loss:\t\t1.386677\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4778 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386397\n",
      "  validation loss:\t\t1.386837\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4779 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387466\n",
      "  validation loss:\t\t1.386959\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4780 of 5000 took 0.023s\n",
      "  training loss:\t\t1.401120\n",
      "  validation loss:\t\t1.387132\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4781 of 5000 took 0.024s\n",
      "  training loss:\t\t1.397974\n",
      "  validation loss:\t\t1.387356\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4782 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386806\n",
      "  validation loss:\t\t1.387441\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4783 of 5000 took 0.021s\n",
      "  training loss:\t\t1.394122\n",
      "  validation loss:\t\t1.387553\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4784 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383874\n",
      "  validation loss:\t\t1.387575\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4785 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383929\n",
      "  validation loss:\t\t1.387320\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4786 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388636\n",
      "  validation loss:\t\t1.387291\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4787 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383198\n",
      "  validation loss:\t\t1.387380\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4788 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388843\n",
      "  validation loss:\t\t1.387239\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4789 of 5000 took 0.022s\n",
      "  training loss:\t\t1.374330\n",
      "  validation loss:\t\t1.387325\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4790 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386055\n",
      "  validation loss:\t\t1.387344\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4791 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387543\n",
      "  validation loss:\t\t1.387515\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4792 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380033\n",
      "  validation loss:\t\t1.387594\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4793 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381597\n",
      "  validation loss:\t\t1.387436\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4794 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382068\n",
      "  validation loss:\t\t1.387520\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4795 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383991\n",
      "  validation loss:\t\t1.387588\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4796 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386296\n",
      "  validation loss:\t\t1.387303\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4797 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378956\n",
      "  validation loss:\t\t1.387425\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4798 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384459\n",
      "  validation loss:\t\t1.387401\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4799 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396643\n",
      "  validation loss:\t\t1.387315\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4800 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380547\n",
      "  validation loss:\t\t1.387530\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4801 of 5000 took 0.024s\n",
      "  training loss:\t\t1.379513\n",
      "  validation loss:\t\t1.388145\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4802 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386202\n",
      "  validation loss:\t\t1.388210\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4803 of 5000 took 0.024s\n",
      "  training loss:\t\t1.385468\n",
      "  validation loss:\t\t1.388324\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4804 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393201\n",
      "  validation loss:\t\t1.387942\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4805 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390469\n",
      "  validation loss:\t\t1.387524\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4806 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393216\n",
      "  validation loss:\t\t1.387287\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4807 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395259\n",
      "  validation loss:\t\t1.387238\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4808 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393452\n",
      "  validation loss:\t\t1.387016\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4809 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393846\n",
      "  validation loss:\t\t1.386801\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4810 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382376\n",
      "  validation loss:\t\t1.386716\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4811 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390796\n",
      "  validation loss:\t\t1.386667\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4812 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391084\n",
      "  validation loss:\t\t1.386727\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4813 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384824\n",
      "  validation loss:\t\t1.386730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4814 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383239\n",
      "  validation loss:\t\t1.386916\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4815 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390171\n",
      "  validation loss:\t\t1.386971\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4816 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384248\n",
      "  validation loss:\t\t1.387086\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4817 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386582\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4818 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391381\n",
      "  validation loss:\t\t1.386980\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4819 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387382\n",
      "  validation loss:\t\t1.387125\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4820 of 5000 took 0.025s\n",
      "  training loss:\t\t1.385777\n",
      "  validation loss:\t\t1.387142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4821 of 5000 took 0.024s\n",
      "  training loss:\t\t1.387482\n",
      "  validation loss:\t\t1.387042\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4822 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379736\n",
      "  validation loss:\t\t1.386985\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4823 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396881\n",
      "  validation loss:\t\t1.386887\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4824 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384524\n",
      "  validation loss:\t\t1.386779\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4825 of 5000 took 0.022s\n",
      "  training loss:\t\t1.369658\n",
      "  validation loss:\t\t1.386800\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4826 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391942\n",
      "  validation loss:\t\t1.386696\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4827 of 5000 took 0.023s\n",
      "  training loss:\t\t1.383810\n",
      "  validation loss:\t\t1.386809\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4828 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387781\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4829 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385610\n",
      "  validation loss:\t\t1.387050\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4830 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389089\n",
      "  validation loss:\t\t1.387213\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4831 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395136\n",
      "  validation loss:\t\t1.387359\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4832 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397135\n",
      "  validation loss:\t\t1.387485\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4833 of 5000 took 0.023s\n",
      "  training loss:\t\t1.396736\n",
      "  validation loss:\t\t1.387344\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4834 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386158\n",
      "  validation loss:\t\t1.387254\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4835 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383393\n",
      "  validation loss:\t\t1.387178\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4836 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391664\n",
      "  validation loss:\t\t1.387176\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4837 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383765\n",
      "  validation loss:\t\t1.387252\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4838 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385623\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4839 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380936\n",
      "  validation loss:\t\t1.386934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4840 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387031\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4841 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389362\n",
      "  validation loss:\t\t1.387051\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4842 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391365\n",
      "  validation loss:\t\t1.386934\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4843 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386056\n",
      "  validation loss:\t\t1.387005\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4844 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388149\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4845 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391018\n",
      "  validation loss:\t\t1.386831\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4846 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389435\n",
      "  validation loss:\t\t1.386795\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4847 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381594\n",
      "  validation loss:\t\t1.386807\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4848 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395162\n",
      "  validation loss:\t\t1.386719\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4849 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392825\n",
      "  validation loss:\t\t1.386775\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4850 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389597\n",
      "  validation loss:\t\t1.386825\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4851 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392174\n",
      "  validation loss:\t\t1.386810\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4852 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393887\n",
      "  validation loss:\t\t1.386893\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4853 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398322\n",
      "  validation loss:\t\t1.386794\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4854 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385990\n",
      "  validation loss:\t\t1.386733\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4855 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386730\n",
      "  validation loss:\t\t1.386632\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4856 of 5000 took 0.021s\n",
      "  training loss:\t\t1.385307\n",
      "  validation loss:\t\t1.386644\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4857 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386181\n",
      "  validation loss:\t\t1.386709\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4858 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388997\n",
      "  validation loss:\t\t1.386811\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4859 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390505\n",
      "  validation loss:\t\t1.386973\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4860 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390833\n",
      "  validation loss:\t\t1.386867\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4861 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386022\n",
      "  validation loss:\t\t1.386897\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4862 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389382\n",
      "  validation loss:\t\t1.386908\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4863 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392864\n",
      "  validation loss:\t\t1.386817\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4864 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385566\n",
      "  validation loss:\t\t1.386849\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4865 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384175\n",
      "  validation loss:\t\t1.386734\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4866 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391620\n",
      "  validation loss:\t\t1.386743\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4867 of 5000 took 0.021s\n",
      "  training loss:\t\t1.397818\n",
      "  validation loss:\t\t1.386718\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4868 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389125\n",
      "  validation loss:\t\t1.386753\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4869 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379990\n",
      "  validation loss:\t\t1.386757\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4870 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385613\n",
      "  validation loss:\t\t1.386837\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4871 of 5000 took 0.023s\n",
      "  training loss:\t\t1.385357\n",
      "  validation loss:\t\t1.387004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4872 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392358\n",
      "  validation loss:\t\t1.387031\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4873 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382830\n",
      "  validation loss:\t\t1.387101\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4874 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377137\n",
      "  validation loss:\t\t1.387302\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4875 of 5000 took 0.026s\n",
      "  training loss:\t\t1.383312\n",
      "  validation loss:\t\t1.387290\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4876 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383616\n",
      "  validation loss:\t\t1.387438\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4877 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393567\n",
      "  validation loss:\t\t1.387222\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4878 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387518\n",
      "  validation loss:\t\t1.387174\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4879 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379007\n",
      "  validation loss:\t\t1.387132\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4880 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384705\n",
      "  validation loss:\t\t1.387004\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4881 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388001\n",
      "  validation loss:\t\t1.386872\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4882 of 5000 took 0.023s\n",
      "  training loss:\t\t1.379086\n",
      "  validation loss:\t\t1.387036\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4883 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386418\n",
      "  validation loss:\t\t1.387074\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4884 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390601\n",
      "  validation loss:\t\t1.387032\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4885 of 5000 took 0.021s\n",
      "  training loss:\t\t1.387024\n",
      "  validation loss:\t\t1.386913\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4886 of 5000 took 0.021s\n",
      "  training loss:\t\t1.381033\n",
      "  validation loss:\t\t1.386866\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4887 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382043\n",
      "  validation loss:\t\t1.386965\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4888 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394519\n",
      "  validation loss:\t\t1.386889\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4889 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387296\n",
      "  validation loss:\t\t1.386863\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4890 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391907\n",
      "  validation loss:\t\t1.386824\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4891 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391975\n",
      "  validation loss:\t\t1.386780\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4892 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381731\n",
      "  validation loss:\t\t1.386740\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4893 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386106\n",
      "  validation loss:\t\t1.386625\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4894 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384957\n",
      "  validation loss:\t\t1.386665\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4895 of 5000 took 0.021s\n",
      "  training loss:\t\t1.386475\n",
      "  validation loss:\t\t1.386758\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4896 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391249\n",
      "  validation loss:\t\t1.386708\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4897 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396150\n",
      "  validation loss:\t\t1.386690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4898 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388856\n",
      "  validation loss:\t\t1.386773\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4899 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383675\n",
      "  validation loss:\t\t1.386917\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4900 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393496\n",
      "  validation loss:\t\t1.386850\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4901 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388367\n",
      "  validation loss:\t\t1.386909\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4902 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384627\n",
      "  validation loss:\t\t1.386974\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4903 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391527\n",
      "  validation loss:\t\t1.386971\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4904 of 5000 took 0.023s\n",
      "  training loss:\t\t1.392434\n",
      "  validation loss:\t\t1.386738\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4905 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386528\n",
      "  validation loss:\t\t1.386629\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4906 of 5000 took 0.022s\n",
      "  training loss:\t\t1.383672\n",
      "  validation loss:\t\t1.386630\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4907 of 5000 took 0.023s\n",
      "  training loss:\t\t1.381040\n",
      "  validation loss:\t\t1.386748\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4908 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379819\n",
      "  validation loss:\t\t1.387053\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4909 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392077\n",
      "  validation loss:\t\t1.387056\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4910 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394344\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4911 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391486\n",
      "  validation loss:\t\t1.387179\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4912 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395570\n",
      "  validation loss:\t\t1.387141\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4913 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392238\n",
      "  validation loss:\t\t1.387277\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4914 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394284\n",
      "  validation loss:\t\t1.387473\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4915 of 5000 took 0.023s\n",
      "  training loss:\t\t1.398235\n",
      "  validation loss:\t\t1.387555\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4916 of 5000 took 0.023s\n",
      "  training loss:\t\t1.377803\n",
      "  validation loss:\t\t1.387862\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4917 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385040\n",
      "  validation loss:\t\t1.387964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4918 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388459\n",
      "  validation loss:\t\t1.388154\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4919 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392334\n",
      "  validation loss:\t\t1.388021\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4920 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394574\n",
      "  validation loss:\t\t1.388007\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4921 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386682\n",
      "  validation loss:\t\t1.387964\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4922 of 5000 took 0.023s\n",
      "  training loss:\t\t1.391260\n",
      "  validation loss:\t\t1.387577\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4923 of 5000 took 0.023s\n",
      "  training loss:\t\t1.389440\n",
      "  validation loss:\t\t1.387580\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4924 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388987\n",
      "  validation loss:\t\t1.387474\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4925 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389480\n",
      "  validation loss:\t\t1.387403\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4926 of 5000 took 0.021s\n",
      "  training loss:\t\t1.391705\n",
      "  validation loss:\t\t1.387142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4927 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392827\n",
      "  validation loss:\t\t1.387085\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4928 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387621\n",
      "  validation loss:\t\t1.386993\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4929 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391623\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4930 of 5000 took 0.024s\n",
      "  training loss:\t\t1.389637\n",
      "  validation loss:\t\t1.386997\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4931 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389132\n",
      "  validation loss:\t\t1.386992\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4932 of 5000 took 0.022s\n",
      "  training loss:\t\t1.392490\n",
      "  validation loss:\t\t1.387028\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4933 of 5000 took 0.021s\n",
      "  training loss:\t\t1.390135\n",
      "  validation loss:\t\t1.386943\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4934 of 5000 took 0.021s\n",
      "  training loss:\t\t1.396479\n",
      "  validation loss:\t\t1.386812\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4935 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391454\n",
      "  validation loss:\t\t1.386730\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4936 of 5000 took 0.021s\n",
      "  training loss:\t\t1.389634\n",
      "  validation loss:\t\t1.386902\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4937 of 5000 took 0.023s\n",
      "  training loss:\t\t1.386068\n",
      "  validation loss:\t\t1.386924\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4938 of 5000 took 0.021s\n",
      "  training loss:\t\t1.398808\n",
      "  validation loss:\t\t1.387260\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4939 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389609\n",
      "  validation loss:\t\t1.387550\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4940 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389722\n",
      "  validation loss:\t\t1.387872\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4941 of 5000 took 0.023s\n",
      "  training loss:\t\t1.397870\n",
      "  validation loss:\t\t1.387957\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4942 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389874\n",
      "  validation loss:\t\t1.387694\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4943 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388572\n",
      "  validation loss:\t\t1.387725\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4944 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387058\n",
      "  validation loss:\t\t1.387709\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4945 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398350\n",
      "  validation loss:\t\t1.387789\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4946 of 5000 took 0.022s\n",
      "  training loss:\t\t1.396961\n",
      "  validation loss:\t\t1.387369\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4947 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390674\n",
      "  validation loss:\t\t1.387354\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4948 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393638\n",
      "  validation loss:\t\t1.387191\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4949 of 5000 took 0.022s\n",
      "  training loss:\t\t1.395578\n",
      "  validation loss:\t\t1.386971\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4950 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390225\n",
      "  validation loss:\t\t1.386869\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4951 of 5000 took 0.022s\n",
      "  training loss:\t\t1.382013\n",
      "  validation loss:\t\t1.386724\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4952 of 5000 took 0.023s\n",
      "  training loss:\t\t1.393750\n",
      "  validation loss:\t\t1.386671\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4953 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378357\n",
      "  validation loss:\t\t1.386698\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4954 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387670\n",
      "  validation loss:\t\t1.386937\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4955 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393775\n",
      "  validation loss:\t\t1.386950\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4956 of 5000 took 0.022s\n",
      "  training loss:\t\t1.384007\n",
      "  validation loss:\t\t1.387124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4957 of 5000 took 0.023s\n",
      "  training loss:\t\t1.394124\n",
      "  validation loss:\t\t1.387052\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4958 of 5000 took 0.023s\n",
      "  training loss:\t\t1.395203\n",
      "  validation loss:\t\t1.386939\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4959 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387269\n",
      "  validation loss:\t\t1.386851\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4960 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384727\n",
      "  validation loss:\t\t1.386824\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4961 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378989\n",
      "  validation loss:\t\t1.386767\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4962 of 5000 took 0.022s\n",
      "  training loss:\t\t1.376284\n",
      "  validation loss:\t\t1.386896\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4963 of 5000 took 0.021s\n",
      "  training loss:\t\t1.395643\n",
      "  validation loss:\t\t1.387124\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4964 of 5000 took 0.023s\n",
      "  training loss:\t\t1.380896\n",
      "  validation loss:\t\t1.387555\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4965 of 5000 took 0.022s\n",
      "  training loss:\t\t1.394291\n",
      "  validation loss:\t\t1.387791\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4966 of 5000 took 0.022s\n",
      "  training loss:\t\t1.398533\n",
      "  validation loss:\t\t1.387674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4967 of 5000 took 0.022s\n",
      "  training loss:\t\t1.379241\n",
      "  validation loss:\t\t1.387741\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4968 of 5000 took 0.022s\n",
      "  training loss:\t\t1.386117\n",
      "  validation loss:\t\t1.387504\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4969 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387714\n",
      "  validation loss:\t\t1.387348\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4970 of 5000 took 0.023s\n",
      "  training loss:\t\t1.400368\n",
      "  validation loss:\t\t1.386935\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4971 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390138\n",
      "  validation loss:\t\t1.386832\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4972 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390697\n",
      "  validation loss:\t\t1.386737\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4973 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393257\n",
      "  validation loss:\t\t1.386925\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4974 of 5000 took 0.022s\n",
      "  training loss:\t\t1.381731\n",
      "  validation loss:\t\t1.387142\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4975 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400928\n",
      "  validation loss:\t\t1.387289\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4976 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387141\n",
      "  validation loss:\t\t1.387451\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4977 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388602\n",
      "  validation loss:\t\t1.387428\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4978 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385039\n",
      "  validation loss:\t\t1.387390\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4979 of 5000 took 0.023s\n",
      "  training loss:\t\t1.382032\n",
      "  validation loss:\t\t1.387444\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4980 of 5000 took 0.022s\n",
      "  training loss:\t\t1.400851\n",
      "  validation loss:\t\t1.387343\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4981 of 5000 took 0.022s\n",
      "  training loss:\t\t1.385564\n",
      "  validation loss:\t\t1.387356\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4982 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390267\n",
      "  validation loss:\t\t1.387148\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4983 of 5000 took 0.022s\n",
      "  training loss:\t\t1.380764\n",
      "  validation loss:\t\t1.386946\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4984 of 5000 took 0.022s\n",
      "  training loss:\t\t1.389644\n",
      "  validation loss:\t\t1.386866\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4985 of 5000 took 0.022s\n",
      "  training loss:\t\t1.378229\n",
      "  validation loss:\t\t1.386818\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4986 of 5000 took 0.026s\n",
      "  training loss:\t\t1.394696\n",
      "  validation loss:\t\t1.386674\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4987 of 5000 took 0.024s\n",
      "  training loss:\t\t1.395061\n",
      "  validation loss:\t\t1.386673\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4988 of 5000 took 0.024s\n",
      "  training loss:\t\t1.391549\n",
      "  validation loss:\t\t1.386785\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4989 of 5000 took 0.022s\n",
      "  training loss:\t\t1.387293\n",
      "  validation loss:\t\t1.386866\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4990 of 5000 took 0.022s\n",
      "  training loss:\t\t1.390520\n",
      "  validation loss:\t\t1.386750\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4991 of 5000 took 0.023s\n",
      "  training loss:\t\t1.387122\n",
      "  validation loss:\t\t1.386767\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4992 of 5000 took 0.023s\n",
      "  training loss:\t\t1.388781\n",
      "  validation loss:\t\t1.386833\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4993 of 5000 took 0.024s\n",
      "  training loss:\t\t1.388224\n",
      "  validation loss:\t\t1.386676\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4994 of 5000 took 0.022s\n",
      "  training loss:\t\t1.393299\n",
      "  validation loss:\t\t1.386650\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4995 of 5000 took 0.023s\n",
      "  training loss:\t\t1.384943\n",
      "  validation loss:\t\t1.386597\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4996 of 5000 took 0.022s\n",
      "  training loss:\t\t1.391971\n",
      "  validation loss:\t\t1.386690\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4997 of 5000 took 0.023s\n",
      "  training loss:\t\t1.378519\n",
      "  validation loss:\t\t1.386883\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4998 of 5000 took 0.022s\n",
      "  training loss:\t\t1.388260\n",
      "  validation loss:\t\t1.387327\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 4999 of 5000 took 0.023s\n",
      "  training loss:\t\t1.390156\n",
      "  validation loss:\t\t1.387754\n",
      "  validation accuracy:\t\t25.00 %\n",
      "Epoch 5000 of 5000 took 0.021s\n",
      "  training loss:\t\t1.379061\n",
      "  validation loss:\t\t1.388401\n",
      "  validation accuracy:\t\t25.00 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from mnistT import load_dataset \n",
    "\n",
    "(X_train,X_val,y_train,y_val) = dset.require_new_RNN(25,20)\n",
    "X_train=np.array(X_train)\n",
    "X_val=np.array(X_val)\n",
    "y_train=np.array(y_train)\n",
    "y_val=np.array(y_val)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 150, 150).astype('float32')\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, 150, 150).astype('float32')\n",
    "\n",
    "num_epochs=5000\n",
    "\n",
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 150, 150), input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.25)\n",
    "    l_hid1 = lasagne.layers.DenseLayer(l_in_drop, num_units=150, nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.1)\n",
    "    l_hid2 = lasagne.layers.DenseLayer(l_hid1_drop, num_units=150, nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.1)\n",
    "    l_out = lasagne.layers.DenseLayer(l_hid2_drop, num_units=74, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "# Create neural network model\n",
    "network = build_mlp(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\n",
    "test_loss = test_loss.mean()\n",
    "\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var), dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, 80, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        train_err += train_fn(inputs, targets)\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 80, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00331000803114\n",
      "Baseline Error: 47.84%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "dset50x50 = dataset.HiraSet('dset50x50', 2500)    \n",
    "dset50x50.pull()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(train,test,train_labels,test_labels) = dset50x50.require_new(25,20)\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2500, input_dim=2500, init='normal', activation='relu'))\n",
    "model.add(Dense(74, init='normal', activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# training\n",
    "training = model.fit(train, train_labels, nb_epoch=500, batch_size=100, verbose=0)\n",
    "print (training.history['loss'][-1])\n",
    "\n",
    "scores = model.evaluate(test, test_labels, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1480 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btest [2.9385045219112085, 0.48851351351351352]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test data\n",
    "scores = model.evaluate(test, test_labels, verbose=1)\n",
    "print ('test', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1850 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btrain [0.22530647855352712, 0.98648648648648651]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on train data\n",
    "scores = model.evaluate(train, train_labels, verbose=1)\n",
    "print ('train', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150, 150)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x629176d8>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6abc0940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # za prikaz slika, grafika, itd.\n",
    "%matplotlib inline\n",
    "\n",
    "imgN = 1\n",
    "img = test[imgN]\n",
    "print (img.shape)\n",
    "img = img.reshape(150,150)\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1480/1480 [==============================] - 3s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "[ 0.01020689  0.01560746  0.01361043  0.01554592  0.01347764  0.01208425\n",
      "  0.01990515  0.01325988  0.00548291  0.01178867  0.00899314  0.01806451\n",
      "  0.01575935  0.00899526  0.0164725   0.01636611  0.01427393  0.01377496\n",
      "  0.00918025  0.01848772  0.00748999  0.00813467  0.01635604  0.00906577\n",
      "  0.00649881  0.01242959  0.01377408  0.01729573  0.0126344   0.01285324\n",
      "  0.01737186  0.00795613  0.01492221  0.01225537  0.0124846   0.01925874\n",
      "  0.01206441  0.01266124  0.01341078  0.01658423  0.01652396  0.01282774\n",
      "  0.00877672  0.01251855  0.01495915  0.01483633  0.01312443  0.00890781\n",
      "  0.01867377  0.01367426  0.0137087   0.01209495  0.01806654  0.00809883\n",
      "  0.01216273  0.01681804  0.01529296  0.01930566  0.0117807   0.01091604\n",
      "  0.01402602  0.00980539  0.0142187   0.01525008  0.01566504  0.01303838\n",
      "  0.01406621  0.0205777   0.01375543  0.01201292  0.01047396  0.01467988\n",
      "  0.01917647  0.01390084]\n"
     ]
    }
   ],
   "source": [
    "t = model.predict(d3_test, verbose=1, batch_size=10)\n",
    "print (t[imgN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 74 artists>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132543c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "    33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "    64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
    "plt.xticks(x)\n",
    "width = 1/1.5\n",
    "plt.bar(x, t[imgN], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "rez_t = t.argmax(axis=1)\n",
    "print (rez_t[imgN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya\n"
     ]
    }
   ],
   "source": [
    "print(dset.entries()[rez_t[imgN]].label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 98.67%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(d3_test, test_labels, verbose=0, batch_size=10)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyimagesearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-de7684d5fe76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpyimagesearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetworks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyimagesearch'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "data   = mnist.data / 255.0\n",
    "labels = mnist.target.astype('int')\n",
    "\n",
    "train_rank = 5000\n",
    "test_rank = 100\n",
    "#------- MNIST subset --------------------------\n",
    "train_subset = np.random.choice(data.shape[0], train_rank)\n",
    "test_subset = np.random.choice(data.shape[0], test_rank)\n",
    "\n",
    "# train dataset\n",
    "train_data = data[train_subset]\n",
    "train_labels = labels[train_subset]\n",
    "\n",
    "# test dataset\n",
    "test_data = data[test_subset]\n",
    "test_labels = labels[test_subset]\n",
    "\n",
    "def to_categorical(labels, n):\n",
    "    retVal = np.zeros((len(labels), n), dtype='int')\n",
    "    ll = np.array(list(enumerate(labels)))\n",
    "    retVal[ll[:,0],ll[:,1]] = 1\n",
    "    return retVal\n",
    "\n",
    "test = [3, 5, 9]\n",
    "print to_categorical(test, 10)\n",
    "\n",
    "# train and test to categorical\n",
    "train_out = to_categorical(train_labels, 10)\n",
    "test_out = to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from RNN import RNN_Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing updates ...\n",
      "Compiling functions ...\n",
      "Training ...\n",
      "Current epoch: 1 | Number of Epochs: 1500 | Train loss: 9.05759 | Validation loss: 9.037201881408691 | Accuracy: 0.0135135135135\n",
      "Current epoch: 2 | Number of Epochs: 1500 | Train loss: 9.01764 | Validation loss: 8.97143840789795 | Accuracy: 0.0202702702703\n",
      "Current epoch: 3 | Number of Epochs: 1500 | Train loss: 8.90958 | Validation loss: 8.775524139404297 | Accuracy: 0.0135135135135\n",
      "Current epoch: 4 | Number of Epochs: 1500 | Train loss: 8.46471 | Validation loss: 6.667167663574219 | Accuracy: 0.0135135135135\n",
      "Current epoch: 5 | Number of Epochs: 1500 | Train loss: 5.50451 | Validation loss: 4.7095537185668945 | Accuracy: 0.0135135135135\n",
      "Current epoch: 6 | Number of Epochs: 1500 | Train loss: 4.65721 | Validation loss: 4.450922966003418 | Accuracy: 0.0202702702703\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ef9584389c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN_Compute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Dusan\\PycharmProjects\\HiraganaRecognition\\RNN.py\u001b[0m in \u001b[0;36mRNN_Compute\u001b[0;34m(layer1, layer2, lrate, batch_size, epochs, train, test, train_labels, test_labels)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mm_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\theano\\scan_module\\scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    951\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RNN_Compute(50,100,0.001,100,1500,train,test,train_labels,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'neural_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac59f8b35c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'neural_net.lenet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mLeNet\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LeNet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'neural_net'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "module = importlib.import_module('neural_net.lenet')\n",
    "LeNet  = getattr(module, 'LeNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQlTKfwE1r2HJ0ykA4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x64f8fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt  # za prikaz slika, grafika, itd.\n",
    "%matplotlib inline\n",
    "size =2500\n",
    "folder='mi'\n",
    "file='mi_13.png'\n",
    "img = imread(rel_path + '/' + folder + '/' + file)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "plt.imshow(dst, cmap=\"Greys\")\n",
    "\n",
    "re_img = np.reshape(dst, size)\n",
    "flt_img = re_img / 65535.0\n",
    "\n",
    "testR =[]\n",
    "testR.append(dst)\n",
    "testRLab=[]\n",
    "testRLab.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_input_6 to have 2 dimensions, but got array with shape (1, 1, 50, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-bb3d9ee445da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtestRLab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestRLab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestR\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1253\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                                    check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_input_6 to have 2 dimensions, but got array with shape (1, 1, 50, 50)"
     ]
    }
   ],
   "source": [
    "testR=np.array(testR)\n",
    "testRLab=np.array(testRLab)\n",
    "testR = testR.reshape(testR.shape[0], 1, 50, 50).astype('float32')\n",
    "t = model.predict(testR, verbose=1, batch_size=90)\n",
    "\n",
    "img = testR[0]\n",
    "img = img.reshape(50,50)\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "print (t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 74 artists>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9B+cSHZAo93Z/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x68f3c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "    33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "    64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
    "plt.xticks(x)\n",
    "width = 1/1.5\n",
    "plt.bar(x, t[0], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "rez_t = t.argmax(axis=1)\n",
    "print (rez_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n"
     ]
    }
   ],
   "source": [
    "print(dset.entries()[rez_t[0]].label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2004041331be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "DEBUG: nvcc STDOUT nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "nvcc warning : nvcc support for Microsoft Visual Studio 2010 and earlier has been deprecated and is no longer being maintained\n",
      "mod.cu\n",
      "support for Microsoft Visual Studio 2010 has been deprecated!\n",
      "   Creating library C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmp93_ievmn/m91973e5c136ea49268a916ff971b7377.lib and object C:/Users/Dusan/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-3.5.2-64/tmp93_ievmn/m91973e5c136ea49268a916ff971b7377.exp\n",
      "\n",
      "Using gpu device 0: GeForce GTX 950 (CNMeM is disabled, cuDNN 5110)\n",
      "H:\\Anaconda3\\lib\\site-packages\\theano\\sandbox\\cuda\\__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3, None, None) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 64, None, None 1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 64, None, None 36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 64, None, None 0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 128, None, Non 73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 128, None, Non 147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 128, None, Non 0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 256, None, Non 295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 256, None, Non 590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 256, None, Non 590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 256, None, Non 0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 512, None, Non 1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 512, None, Non 2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 512, None, Non 2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 512, None, Non 0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 512, None, Non 2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 512, None, Non 2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 512, None, Non 2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)       (None, 512, None, Non 0           block5_conv3[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "image_input (InputLayer)         (None, 3, 150, 150)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "vgg16 (Model)                    multiple              14714688    image_input[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 8192)          0           vgg16[1][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 4096)          33558528    flatten[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 4096)          16781312    fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 74)            303178      fc2[0][0]                        \n",
      "====================================================================================================\n",
      "Total params: 65,357,706\n",
      "Trainable params: 65,357,706\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
    "model_vgg16_conv.summary()\n",
    "\n",
    "#Create your own input format (here 3x200x200)\n",
    "input = Input(shape=(3,150,150),name = 'image_input')\n",
    "\n",
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "x = Dense(74, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input=input, output=x)\n",
    "\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "my_model.summary()\n",
    "\n",
    "\n",
    "#Then training with your data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge = np.concatenate((test, test, test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=my_model.predict(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train = train.reshape(train.shape[0], 1, 150, 150).astype('float32')\n",
    "test = test.reshape(test.shape[0], 1, 150, 150).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyyyy\n"
     ]
    }
   ],
   "source": [
    "print('eyyyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 74 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn0PYFiaxR/XdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3f6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\n",
    "    33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
    "    64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
    "plt.xticks(x)\n",
    "width = 1/1.5\n",
    "plt.bar(x, t[123], color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rez_t = t.argmax(axis=1)\n",
    "print (rez_t[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n"
     ]
    }
   ],
   "source": [
    "print(dset.entries()[rez_t[0]].label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = testR[0]\n",
    "img = img.reshape(50,50)\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "print (t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hnn\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n",
    "train_data = train_data.reshape(train_data.shape[0], 1, 150, 150).astype('float32')\n",
    "test_data = test_data.reshape(test_data.shape[0], 1, 150, 150).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hnn.HNN at 0x2e7f6748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hnn.HNN('cnn','cnnModel',hnn.cnn_model(150,150,74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ej..\n"
     ]
    }
   ],
   "source": [
    "print('ej..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('models/' + 'ann' + '/' + 'annModel1' + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hanan=hnn.HNN('ann','knnModel1',hnn.ann_model(2500,74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 1850 arrays: [array([ 0.37555504,  0.37163347,  0.3677119 , ...,  0.39630732,\n        0.39630732,  0.40022889], dtype=float32), array([ 0.39124131,  0.38731974,  0.37947661, ...,  0.42767987,\n        0.4237583 ,  ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3435d7cc3fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhanan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Dusan\\PycharmProjects\\HiraganaRecognition\\hnn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, labels, epoch, batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__classifier\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'ann'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__classifier\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'cnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m   1030\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mH:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     63\u001b[0m                                  \u001b[1;34m'the following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                  \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                  '...')\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 arrays but instead got the following list of 1850 arrays: [array([ 0.37555504,  0.37163347,  0.3677119 , ...,  0.39630732,\n        0.39630732,  0.40022889], dtype=float32), array([ 0.39124131,  0.38731974,  0.37947661, ...,  0.42767987,\n        0.4237583 ,  ..."
     ]
    }
   ],
   "source": [
    "hnn.HNN.train(hanan,train_data,train_labels,300,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-741cb90f8828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'ann'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'annModel1'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save('models/' + 'ann' + '/' + 'annModel1' + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset50x50 = dataset.HiraSet('dset50x50', 2500)    \n",
    "dset50x50.pull()\n",
    "\n",
    "\n",
    "(train,test,train_labels,test_labels) = dset50x50.require_new(25,20)\n",
    "train=np.array(train)\n",
    "test=np.array(test)\n",
    "train_labels=np.array(train_labels)\n",
    "test_labels=np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 49.66%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2500, input_dim=2500, init='normal', activation='relu'))\n",
    "model.add(Dense(74, init='normal', activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# training\n",
    "training = model.fit(train, train_labels, nb_epoch=300, batch_size=100, verbose=0)\n",
    "scores = model.evaluate(test, test_labels, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
